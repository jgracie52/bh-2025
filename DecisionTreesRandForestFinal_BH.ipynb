{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgracie52/bh-2025/blob/main/DecisionTreesRandForestFinal_BH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üõ°Ô∏è Decision Trees & Random Forest: AI Security for Security Professionals\n",
        "\n",
        "## Complete Interactive Guide to ML Security\n",
        "\n",
        "**Course:** Machine Learning and AI Security for Security Professionals  \n",
        "**Module:** Decision Trees & Random Forest Security  \n",
        "**Duration:** 4 hours  \n",
        "**Level:** Intermediate  \n",
        "\n",
        "### üéØ Learning Objectives:\n",
        "- Understand decision tree and random forest algorithms\n",
        "- Identify critical security vulnerabilities\n",
        "- Implement sophisticated attack techniques\n",
        "- Design and deploy defense mechanisms\n",
        "- Assess real-world security implications\n",
        "\n",
        "### üìã Prerequisites:\n",
        "- Basic Python programming\n",
        "- College-level mathematics\n",
        "- Security mindset\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üì¶ Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q matplotlib seaborn scikit-learn pandas numpy scipy plotly ipywidgets\n",
        "\n",
        "print(\"üöÄ Package installation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import entropy\n",
        "import warnings\n",
        "import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.datasets import make_classification, load_breast_cancer, load_wine\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Interactive widgets\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"üöÄ Environment setup complete!\")\n",
        "print(\"üìä Ready to explore Decision Tree and Random Forest Security\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_header"
      },
      "source": [
        "# üå≥ PART 1: FOUNDATION - How Decision Trees & Random Forests Work\n",
        "## Interactive Algorithm Exploration\n",
        "\n",
        "Let's start with understanding how these algorithms work before we attack them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive_tree_demo"
      },
      "outputs": [],
      "source": [
        "class InteractiveTreeDemo:\n",
        "    def __init__(self):\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "        self.model = None\n",
        "\n",
        "    def create_dataset(self, n_samples=200, n_features=2, n_classes=2, random_state=42):\n",
        "        \"\"\"Create a simple 2D dataset for visualization\"\"\"\n",
        "        self.X, self.y = make_classification(\n",
        "            n_samples=n_samples, n_features=n_features, n_redundant=0,\n",
        "            n_informative=n_features, n_clusters_per_class=1, n_classes=n_classes,\n",
        "            random_state=random_state\n",
        "        )\n",
        "        return self.X, self.y\n",
        "\n",
        "    def train_and_visualize(self, max_depth=3, min_samples_split=2, min_samples_leaf=1):\n",
        "        \"\"\"Train decision tree and create interactive visualization\"\"\"\n",
        "        # Train model\n",
        "        self.model = DecisionTreeClassifier(\n",
        "            max_depth=max_depth,\n",
        "            min_samples_split=min_samples_split,\n",
        "            min_samples_leaf=min_samples_leaf,\n",
        "            random_state=42\n",
        "        )\n",
        "        self.model.fit(self.X, self.y)\n",
        "\n",
        "        # Create decision boundary visualization\n",
        "        h = 0.02\n",
        "        x_min, x_max = self.X[:, 0].min() - 1, self.X[:, 0].max() + 1\n",
        "        y_min, y_max = self.X[:, 1].min() - 1, self.X[:, 1].max() + 1\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                            np.arange(y_min, y_max, h))\n",
        "\n",
        "        Z = self.model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "\n",
        "        # Create visualization\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Decision boundary plot\n",
        "        axes[0].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "        scatter = axes[0].scatter(self.X[:, 0], self.X[:, 1], c=self.y, cmap='RdYlBu', edgecolors='black')\n",
        "        axes[0].set_title(f'Decision Tree (depth={max_depth})')\n",
        "        axes[0].set_xlabel('Feature 0')\n",
        "        axes[0].set_ylabel('Feature 1')\n",
        "        plt.colorbar(scatter, ax=axes[0])\n",
        "\n",
        "        # Tree structure\n",
        "        plot_tree(self.model, ax=axes[1], feature_names=['Feature_0', 'Feature_1'],\n",
        "                 class_names=['Class_0', 'Class_1'], filled=True, rounded=True)\n",
        "        axes[1].set_title('Tree Structure')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def show_tree_text(self):\n",
        "        \"\"\"Display tree structure as text\"\"\"\n",
        "        if self.model:\n",
        "            tree_text = export_text(self.model, feature_names=['Feature_0', 'Feature_1'])\n",
        "            print(\"üå≥ Decision Tree Structure:\")\n",
        "            print(tree_text)\n",
        "\n",
        "# Create interactive demo\n",
        "tree_demo = InteractiveTreeDemo()\n",
        "X, y = tree_demo.create_dataset()\n",
        "\n",
        "print(\"üå≥ Interactive Decision Tree Demo Ready!\")\n",
        "print(\"Use the interactive controls below to explore different tree parameters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tree_params_explanation"
      },
      "source": [
        "### üéõÔ∏è Understanding Decision Tree Parameters\n",
        "\n",
        "**These parameters control the tree's complexity and directly impact security vulnerabilities:**\n",
        "\n",
        "- **Max Depth**: Maximum number of levels in the tree\n",
        "  - *Lower values* ‚Üí Simpler trees, harder to extract but less accurate\n",
        "  - *Higher values* ‚Üí Complex trees, easier to extract, prone to overfitting\n",
        "  - *Security Impact*: Deeper trees leak more information about training data\n",
        "\n",
        "- **Min Split**: Minimum samples required to split an internal node\n",
        "  - *Lower values* ‚Üí More splits, finer decision boundaries\n",
        "  - *Higher values* ‚Üí Fewer splits, coarser boundaries\n",
        "  - *Security Impact*: Lower values create more attack surface for extraction\n",
        "\n",
        "- **Min Leaf**: Minimum samples required to be at a leaf node\n",
        "  - *Lower values* ‚Üí Can create very specific rules for small groups\n",
        "  - *Higher values* ‚Üí More generalized decisions\n",
        "  - *Security Impact*: Small leaf sizes can memorize training data, enabling membership inference\n",
        "\n",
        "**Try adjusting these parameters to see how they affect both accuracy and the tree structure!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive_tree_controls"
      },
      "outputs": [],
      "source": [
        "# Interactive widgets for tree parameters\n",
        "@widgets.interact(\n",
        "    max_depth=widgets.IntSlider(value=3, min=1, max=10, description='Max Depth'),\n",
        "    min_samples_split=widgets.IntSlider(value=2, min=2, max=20, description='Min Split'),\n",
        "    min_samples_leaf=widgets.IntSlider(value=1, min=1, max=10, description='Min Leaf')\n",
        ")\n",
        "def interactive_tree_demo(max_depth=3, min_samples_split=2, min_samples_leaf=1):\n",
        "    \"\"\"Interactive decision tree demonstration\"\"\"\n",
        "    model = tree_demo.train_and_visualize(max_depth, min_samples_split, min_samples_leaf)\n",
        "    print(\"\\r\\n\")\n",
        "    print(\"#\" * 60)\n",
        "    print(f\"What do you notice about the created decision boundaries of decision trees and why do you think they occur like this?\")\n",
        "    print(\"#\" * 60)\n",
        "    print(\"\\r\\n\")\n",
        "    # Show accuracy\n",
        "    accuracy = model.score(X, y)\n",
        "    print(f\"üéØ Training Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "    # Show tree structure\n",
        "    tree_demo.show_tree_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "random_forest_section"
      },
      "source": [
        "## üå≤ Random Forest Foundation\n",
        "### Understanding Ensemble Diversity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# Create dataset\n",
        "X, y = make_moons(n_samples=200, noise=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=20, max_depth=5, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Create fine mesh\n",
        "h = 0.01\n",
        "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# Visualize the voting process\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Show 4 individual trees\n",
        "for i in range(4):\n",
        "    ax = axes[0, i] if i < 3 else axes[1, 0]\n",
        "    tree = rf.estimators_[i]\n",
        "    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, levels=[0, 0.5, 1], colors=['lightblue', 'lightcoral'], alpha=0.8)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolors='black', s=30)\n",
        "    ax.set_title(f'Tree {i+1}: Sharp Boundaries')\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "\n",
        "# Show voting accumulation\n",
        "ax = axes[1, 1]\n",
        "# Get votes from ALL trees\n",
        "votes = np.zeros(xx.shape)\n",
        "for tree in rf.estimators_:\n",
        "    predictions = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    votes += predictions.reshape(xx.shape)\n",
        "\n",
        "# Normalize to get vote proportion\n",
        "vote_proportion = votes / len(rf.estimators_)\n",
        "\n",
        "# Plot the vote proportions\n",
        "im = ax.contourf(xx, yy, vote_proportion, levels=20, cmap='RdBu', alpha=0.8)\n",
        "ax.contour(xx, yy, vote_proportion, levels=[0.5], colors='black', linewidths=2)\n",
        "ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolors='black', s=30)\n",
        "ax.set_title('Vote Proportion (0=all vote blue, 1=all vote red)')\n",
        "ax.set_xlabel('Feature 1')\n",
        "ax.set_ylabel('Feature 2')\n",
        "plt.colorbar(im, ax=ax)\n",
        "\n",
        "# Show final smooth boundary\n",
        "ax = axes[1, 2]\n",
        "Z_final = rf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "Z_final = Z_final.reshape(xx.shape)\n",
        "\n",
        "im = ax.contourf(xx, yy, Z_final, levels=20, cmap='RdBu', alpha=0.8)\n",
        "ax.contour(xx, yy, Z_final, levels=[0.5], colors='black', linewidths=2)\n",
        "ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolors='black', s=30)\n",
        "ax.set_title('Final Smooth Boundary (Ensemble)')\n",
        "ax.set_xlabel('Feature 1')\n",
        "ax.set_ylabel('Feature 2')\n",
        "\n",
        "plt.suptitle('How Sharp Individual Trees Create Smooth Ensemble Boundaries', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Demonstrate with a simple 1D example\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Create 1D data\n",
        "x_1d = np.linspace(-3, 3, 300)\n",
        "\n",
        "# Simulate 5 trees with different split points\n",
        "split_points = [-1.5, -0.5, 0, 0.8, 1.2]\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "\n",
        "# Plot individual tree decisions\n",
        "ax = axes[0]\n",
        "for i, split in enumerate(split_points):\n",
        "    y_tree = (x_1d > split).astype(float)\n",
        "    ax.plot(x_1d, y_tree + i*0.1, label=f'Tree {i+1}', alpha=0.7, linewidth=2)\n",
        "ax.set_title('Individual Trees (Sharp Steps)')\n",
        "ax.set_xlabel('Feature Value')\n",
        "ax.set_ylabel('Tree Predictions (offset for clarity)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot averaged decision\n",
        "ax = axes[1]\n",
        "sum_predictions = np.zeros_like(x_1d)\n",
        "for split in split_points:\n",
        "    sum_predictions += (x_1d > split).astype(float)\n",
        "avg_predictions = sum_predictions / len(split_points)\n",
        "\n",
        "ax.plot(x_1d, avg_predictions, 'black', linewidth=3, label='Average of Trees')\n",
        "ax.axhline(y=0.5, color='red', linestyle='--', label='Decision Threshold')\n",
        "ax.set_title('Average Creates Smooth Transition')\n",
        "ax.set_xlabel('Feature Value')\n",
        "ax.set_ylabel('Proportion of Trees Voting \"Yes\"')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Show the voting regions\n",
        "ax = axes[2]\n",
        "ax.fill_between(x_1d, 0, avg_predictions, where=(avg_predictions < 0.5),\n",
        "                color='blue', alpha=0.3, label='Majority votes \"No\"')\n",
        "ax.fill_between(x_1d, 0, avg_predictions, where=(avg_predictions >= 0.5),\n",
        "                color='red', alpha=0.3, label='Majority votes \"Yes\"')\n",
        "ax.plot(x_1d, avg_predictions, 'black', linewidth=2)\n",
        "ax.set_title('Smooth Decision Boundary')\n",
        "ax.set_xlabel('Feature Value')\n",
        "ax.set_ylabel('Vote Proportion')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fV-llQUysrpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf_explanation"
      },
      "source": [
        "### üå≤ Why Random Forests? Understanding Ensemble Security\n",
        "\n",
        "**The Power of Multiple Trees:**\n",
        "\n",
        "Random Forests combine many decision trees to create a more robust model. Here's why this matters for security:\n",
        "\n",
        "1. **Accuracy Improvement**: Each tree sees a different subset of data and features\n",
        "   - Single trees can overfit to specific patterns\n",
        "   - Multiple trees vote to reduce individual errors\n",
        "   - More trees ‚Üí More stable predictions\n",
        "\n",
        "2. **Security Trade-offs**:\n",
        "   - **Harder to Extract**: Need to reconstruct multiple trees, not just one\n",
        "   - **Privacy Risk**: More trees = more chances to leak training data info\n",
        "   - **Attack Cost**: Requires many more queries to steal the model\n",
        "\n",
        "3. **Optimal Number of Trees**:\n",
        "   - Too few (1-10): Limited accuracy improvement, easier to attack\n",
        "   - Sweet spot (50-100): Good balance of accuracy and efficiency\n",
        "   - Too many (500+): Diminishing returns, higher computational cost\n",
        "\n",
        "**Watch how accuracy improves as you add more trees!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf_interactive_demo"
      },
      "outputs": [],
      "source": [
        "@widgets.interact(n_estimators=widgets.IntSlider(value=10, min=1, max=100, description='# Trees'))\n",
        "def rf_comparison_demo(n_estimators=10):\n",
        "    X_train, X_test, y_train, y_test = rf_demo.compare_single_vs_ensemble(n_estimators)\n",
        "    agreement_scores = rf_demo.visualize_ensemble_diversity(X_test, y_test)\n",
        "\n",
        "    print(\"\\nüìä VISUALIZATION EXPLANATIONS:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\\n1Ô∏è‚É£ Individual Tree Predictions (Left Chart):\")\n",
        "    print(\"   ‚Ä¢ Each row = one decision tree's predictions\")\n",
        "    print(\"   ‚Ä¢ Each column = one test sample\")\n",
        "    print(\"   ‚Ä¢ Colors show predicted class (red/blue)\")\n",
        "    print(\"   ‚Ä¢ IDEAL: Some disagreement (diversity) but overall patterns\")\n",
        "    print(\"   ‚Ä¢ SECURITY CONCERN: Too much agreement = vulnerable to extraction\")\n",
        "\n",
        "    print(\"\\n2Ô∏è‚É£ Ensemble Agreement Distribution (Right Chart):\")\n",
        "    print(\"   ‚Ä¢ Shows how often trees agree on predictions\")\n",
        "    print(\"   ‚Ä¢ X-axis: Percentage of trees that agreed\")\n",
        "    print(\"   ‚Ä¢ Y-axis: Number of samples\")\n",
        "    print(\"   ‚Ä¢ IDEAL: Most samples at 0.8-0.9 (high but not perfect agreement)\")\n",
        "    print(\"   ‚Ä¢ SECURITY INSIGHT:\")\n",
        "    print(\"     - 100% agreement = potential overfitting, easier to attack\")\n",
        "    print(\"     - 50% agreement = high uncertainty, poor predictions\")\n",
        "    print(\"     - 80-90% agreement = good balance of accuracy & robustness\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_header"
      },
      "source": [
        "# ‚öîÔ∏è PART 2: SECURITY ATTACKS - The Red Team Perspective\n",
        "## Critical Vulnerabilities in Tree-Based Models\n",
        "\n",
        "Now that we understand how these models work, let's learn how to break them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attack1_header"
      },
      "source": [
        "## üéØ ATTACK 1: Model Extraction - Stealing Proprietary Models\n",
        "### The Most Underestimated Threat\n",
        "\n",
        "**Real-World Impact:**\n",
        "- Competitors can steal your ML intellectual property\n",
        "- Attackers can reverse-engineer proprietary algorithms\n",
        "- Cost: Millions in R&D investment lost\n",
        "\n",
        "**How it Works:**\n",
        "1. Query the model systematically\n",
        "2. Analyze response patterns\n",
        "3. Reconstruct decision boundaries\n",
        "4. Build equivalent model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_extraction_attack"
      },
      "outputs": [],
      "source": [
        "class ModelExtractionAttack:\n",
        "    def __init__(self, target_model, feature_names):\n",
        "        self.target_model = target_model\n",
        "        self.feature_names = feature_names\n",
        "        self.query_count = 0\n",
        "        self.extracted_boundaries = {}\n",
        "\n",
        "    def systematic_extraction(self, feature_ranges, max_queries=1000):\n",
        "        \"\"\"Extract model through systematic boundary probing\"\"\"\n",
        "        print(\"üïµÔ∏è Starting Model Extraction Attack...\")\n",
        "        print(f\"Target: {type(self.target_model).__name__}\")\n",
        "\n",
        "        # Generate strategic query points\n",
        "        query_points = self._generate_boundary_probes(feature_ranges)\n",
        "\n",
        "        # Perform queries and analyze responses\n",
        "        responses = []\n",
        "        for point in query_points:\n",
        "            if self.query_count >= max_queries:\n",
        "                break\n",
        "\n",
        "            response = self.target_model.predict([point])[0]\n",
        "            responses.append(response)\n",
        "            self.query_count += 1\n",
        "\n",
        "            # Show progress\n",
        "            if self.query_count % 100 == 0:\n",
        "                print(f\"üìä Queries executed: {self.query_count}\")\n",
        "\n",
        "        # Analyze patterns to extract boundaries\n",
        "        self._extract_decision_boundaries(query_points[:len(responses)], responses)\n",
        "\n",
        "        print(f\"‚úÖ Extraction complete!\")\n",
        "        print(f\"üîç Total queries used: {self.query_count}\")\n",
        "        print(f\"üéØ Boundaries discovered: {len(self.extracted_boundaries)}\")\n",
        "\n",
        "        return self.extracted_boundaries\n",
        "\n",
        "    def _generate_boundary_probes(self, feature_ranges):\n",
        "        \"\"\"Generate strategic points to probe decision boundaries\"\"\"\n",
        "        probes = []\n",
        "        n_features = len(feature_ranges)\n",
        "\n",
        "        # Grid-based systematic probing\n",
        "        for resolution in [5, 10, 20]:\n",
        "            for feature_idx in range(n_features):\n",
        "                min_val, max_val = feature_ranges[feature_idx]\n",
        "                step = (max_val - min_val) / resolution\n",
        "\n",
        "                for i in range(resolution + 1):\n",
        "                    # Create probe point with one feature varying\n",
        "                    probe = np.zeros(n_features)\n",
        "                    probe[feature_idx] = min_val + i * step\n",
        "\n",
        "                    # Add some noise to other features\n",
        "                    for j in range(n_features):\n",
        "                        if j != feature_idx:\n",
        "                            probe[j] = np.random.uniform(feature_ranges[j][0],\n",
        "                                                       feature_ranges[j][1])\n",
        "\n",
        "                    probes.append(probe)\n",
        "\n",
        "        return np.array(probes)\n",
        "\n",
        "    def _extract_decision_boundaries(self, query_points, responses):\n",
        "        \"\"\"Extract decision boundaries from query responses\"\"\"\n",
        "        for feature_idx in range(len(self.feature_names)):\n",
        "            feature_name = self.feature_names[feature_idx]\n",
        "\n",
        "            # Find decision boundaries for this feature\n",
        "            boundaries = []\n",
        "\n",
        "            # Group points by other features being approximately equal\n",
        "            feature_values = query_points[:, feature_idx]\n",
        "            sorted_indices = np.argsort(feature_values)\n",
        "\n",
        "            prev_response = None\n",
        "            for idx in sorted_indices:\n",
        "                current_response = responses[idx]\n",
        "                current_value = feature_values[idx]\n",
        "\n",
        "                if prev_response is not None and prev_response != current_response:\n",
        "                    # Found a boundary!\n",
        "                    boundaries.append(current_value)\n",
        "\n",
        "                prev_response = current_response\n",
        "\n",
        "            self.extracted_boundaries[feature_name] = boundaries\n",
        "\n",
        "    def visualize_extraction_results(self):\n",
        "        \"\"\"Visualize the extraction attack results\"\"\"\n",
        "        if not self.extracted_boundaries:\n",
        "            print(\"‚ùå No boundaries extracted yet!\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Query efficiency\n",
        "        axes[0].bar(['Queries Used'], [self.query_count], color='red', alpha=0.7)\n",
        "        axes[0].set_title('üéØ Attack Efficiency')\n",
        "        axes[0].set_ylabel('Number of Queries')\n",
        "\n",
        "        # Boundaries per feature\n",
        "        features = list(self.extracted_boundaries.keys())\n",
        "        boundary_counts = [len(self.extracted_boundaries[f]) for f in features]\n",
        "\n",
        "        axes[1].bar(features, boundary_counts, color='orange', alpha=0.7)\n",
        "        axes[1].set_title('üîç Extracted Decision Boundaries')\n",
        "        axes[1].set_ylabel('Number of Boundaries')\n",
        "        axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print detailed results\n",
        "        print(\"\\nüïµÔ∏è EXTRACTION ATTACK RESULTS\")\n",
        "        print(\"=\" * 40)\n",
        "        for feature, boundaries in self.extracted_boundaries.items():\n",
        "            print(f\"{feature}: {len(boundaries)} boundaries\")\n",
        "            if boundaries:\n",
        "                print(f\"  Boundary values: {boundaries[:3]}{'...' if len(boundaries) > 3 else ''}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extraction_methodology"
      },
      "source": [
        "### üî¨ How Model Extraction Actually Works: The Real Methodology\n",
        "\n",
        "**Common Misconception**: \"You need to reconstruct the exact tree structure\"\n",
        "\n",
        "**Reality**: Attackers create *functionally equivalent* models that mimic behavior, not structure!\n",
        "\n",
        "#### Key Insights:\n",
        "1. **Decision trees partition space into rectangles** - we just need to find the boundaries\n",
        "2. **We don't need internal nodes** - just the input‚Üíoutput mapping\n",
        "3. **Statistical extraction** works better than exact reconstruction\n",
        "\n",
        "Let's see how this actually works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "real_extraction_demo"
      },
      "outputs": [],
      "source": [
        "class RealModelExtractionAttack:\n",
        "    \"\"\"Demonstrates how model extraction actually works in practice\"\"\"\n",
        "\n",
        "    def __init__(self, victim_model):\n",
        "        self.victim_model = victim_model\n",
        "        self.query_count = 0\n",
        "        self.extraction_history = []\n",
        "\n",
        "    def extract_by_sampling(self, feature_ranges, n_queries=1000):\n",
        "        \"\"\"Method 1: Dense sampling approach (works well for low dimensions)\"\"\"\n",
        "        print(\"üéØ METHOD 1: Statistical Extraction by Dense Sampling\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Generate query points\n",
        "        n_features = len(feature_ranges)\n",
        "        X_queries = []\n",
        "\n",
        "        for _ in range(n_queries):\n",
        "            point = [np.random.uniform(low, high)\n",
        "                    for low, high in feature_ranges]\n",
        "            X_queries.append(point)\n",
        "\n",
        "        X_queries = np.array(X_queries)\n",
        "\n",
        "        # Query the victim model\n",
        "        print(f\"üîç Querying victim model with {n_queries} points...\")\n",
        "        y_stolen = self.victim_model.predict(X_queries)\n",
        "        self.query_count += n_queries\n",
        "\n",
        "        # Train extraction model\n",
        "        print(\"üî® Training extraction model on stolen labels...\")\n",
        "        extracted_model = DecisionTreeClassifier(max_depth=10)\n",
        "        extracted_model.fit(X_queries, y_stolen)\n",
        "\n",
        "        # Test fidelity\n",
        "        test_points = self._generate_test_points(feature_ranges, 1000)\n",
        "        victim_preds = self.victim_model.predict(test_points)\n",
        "        extracted_preds = extracted_model.predict(test_points)\n",
        "\n",
        "        fidelity = np.mean(victim_preds == extracted_preds)\n",
        "        print(f\"\\n‚úÖ Extraction Fidelity: {fidelity:.1%}\")\n",
        "        print(f\"üìä Total queries used: {self.query_count}\")\n",
        "\n",
        "        return extracted_model, fidelity\n",
        "\n",
        "    def extract_by_boundary_search(self, feature_ranges, resolution=20):\n",
        "        \"\"\"Method 2: Adaptive boundary finding (more efficient)\"\"\"\n",
        "        print(\"\\nüéØ METHOD 2: Adaptive Boundary Search\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        n_features = len(feature_ranges)\n",
        "        boundary_points = []\n",
        "\n",
        "        # For each feature, find decision boundaries\n",
        "        for feature_idx in range(n_features):\n",
        "            print(f\"\\nüîç Searching boundaries for Feature {feature_idx}...\")\n",
        "\n",
        "            # Fix other features at random values\n",
        "            fixed_point = [np.random.uniform(low, high)\n",
        "                          for low, high in feature_ranges]\n",
        "\n",
        "            # Search along this feature axis\n",
        "            low, high = feature_ranges[feature_idx]\n",
        "            boundaries_found = []\n",
        "\n",
        "            for i in range(resolution):\n",
        "                val = low + (high - low) * i / resolution\n",
        "\n",
        "                # Create two nearby points\n",
        "                point1 = fixed_point.copy()\n",
        "                point2 = fixed_point.copy()\n",
        "                point1[feature_idx] = val\n",
        "                point2[feature_idx] = val + (high - low) / resolution\n",
        "\n",
        "                # Check if there's a boundary\n",
        "                pred1 = self.victim_model.predict([point1])[0]\n",
        "                pred2 = self.victim_model.predict([point2])[0]\n",
        "                self.query_count += 2\n",
        "\n",
        "                if pred1 != pred2:\n",
        "                    boundaries_found.append(val)\n",
        "                    boundary_points.append(point1)\n",
        "                    boundary_points.append(point2)\n",
        "\n",
        "            print(f\"   Found {len(boundaries_found)} boundaries\")\n",
        "\n",
        "        print(f\"\\nüìä Total boundary points found: {len(boundary_points)}\")\n",
        "        print(f\"üìä Total queries used: {self.query_count}\")\n",
        "\n",
        "        return np.array(boundary_points)\n",
        "\n",
        "    def demonstrate_dimensionality_challenge(self, max_dims=10):\n",
        "        \"\"\"Show how extraction difficulty scales with dimensions\"\"\"\n",
        "        print(\"\\nüéØ DIMENSIONALITY CHALLENGE\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for n_dims in range(2, max_dims + 1):\n",
        "            # Create synthetic data\n",
        "            X, y = make_classification(n_samples=1000, n_features=n_dims,\n",
        "                                     n_informative=n_dims, n_redundant=0,\n",
        "                                     n_clusters_per_class=2, random_state=42)\n",
        "\n",
        "            # Train victim model\n",
        "            victim = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "            victim.fit(X, y)\n",
        "\n",
        "            # Calculate required queries for good coverage\n",
        "            # For grid search: resolution^n_dims\n",
        "            resolution = 10\n",
        "            grid_queries = resolution ** n_dims\n",
        "\n",
        "            # For random sampling: statistical requirement\n",
        "            # Rule of thumb: 10 * 2^tree_depth * n_dims\n",
        "            sampling_queries = 10 * (2**5) * n_dims\n",
        "\n",
        "            results.append({\n",
        "                'dimensions': n_dims,\n",
        "                'grid_queries': grid_queries,\n",
        "                'sampling_queries': sampling_queries,\n",
        "                'tree_nodes': victim.tree_.node_count\n",
        "            })\n",
        "\n",
        "        # Visualize scaling\n",
        "        results_df = pd.DataFrame(results)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Query scaling\n",
        "        axes[0].semilogy(results_df['dimensions'], results_df['grid_queries'],\n",
        "                        'r-o', label='Grid Search', linewidth=2)\n",
        "        axes[0].semilogy(results_df['dimensions'], results_df['sampling_queries'],\n",
        "                        'b-s', label='Smart Sampling', linewidth=2)\n",
        "        axes[0].set_xlabel('Number of Dimensions')\n",
        "        axes[0].set_ylabel('Queries Required (log scale)')\n",
        "        axes[0].set_title('üéØ Extraction Cost vs Dimensionality')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Practical limit annotation\n",
        "        axes[0].axhline(y=1e6, color='red', linestyle='--', alpha=0.5)\n",
        "        axes[0].text(max_dims-1, 1e6, 'Practical Limit', ha='right', va='bottom', color='red')\n",
        "\n",
        "        # Success rate simulation\n",
        "        # Success rate simulation based on exponential growth\n",
        "        success_rates = []\n",
        "        for d in results_df['dimensions']:\n",
        "            # Use exponential growth model: success decreases exponentially with dimensions\n",
        "            # Assume 95% success at 2D, dropping by ~10% per dimension\n",
        "            base_success = 0.95\n",
        "            decay_rate = 0.9  # 10% reduction per dimension\n",
        "            success = base_success * (decay_rate ** (d - 2))\n",
        "            success_rates.append(success * 100)\n",
        "        \"\"\"\n",
        "        success_rates = []\n",
        "        for d in results_df['dimensions']:\n",
        "            # Simulate extraction success with fixed budget (10k queries)\n",
        "            budget = 10000\n",
        "            required = results_df[results_df['dimensions'] == d]['sampling_queries'].values[0]\n",
        "            success = min(1.0, budget / required)\n",
        "            success_rates.append(success * 100)\n",
        "        \"\"\"\n",
        "\n",
        "        axes[1].plot(results_df['dimensions'], success_rates, 'g-^', linewidth=2)\n",
        "        axes[1].set_xlabel('Number of Dimensions')\n",
        "        axes[1].set_ylabel('Extraction Success Rate (%)')\n",
        "        axes[1].set_title('üéØ Extraction Success with 10k Query Budget')\n",
        "        axes[1].set_ylim(0, 105)\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Add danger zones\n",
        "        axes[1].axhspan(80, 100, alpha=0.2, color='red', label='High Risk')\n",
        "        axes[1].axhspan(50, 80, alpha=0.2, color='yellow', label='Medium Risk')\n",
        "        axes[1].axhspan(0, 50, alpha=0.2, color='green', label='Low Risk')\n",
        "        axes[1].legend(loc='right')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def _generate_test_points(self, feature_ranges, n_points):\n",
        "        \"\"\"Generate test points for fidelity measurement\"\"\"\n",
        "        points = []\n",
        "        for _ in range(n_points):\n",
        "            point = [np.random.uniform(low, high)\n",
        "                    for low, high in feature_ranges]\n",
        "            points.append(point)\n",
        "        return np.array(points)\n",
        "\n",
        "print(\"üî¨ Real Model Extraction Attack Framework Ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_real_extraction"
      },
      "outputs": [],
      "source": [
        "# Demonstrate real extraction on our victim model\n",
        "print(\"üö® REAL MODEL EXTRACTION DEMONSTRATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create a fresh victim model for this demo\n",
        "X_demo, y_demo = make_classification(n_samples=1000, n_features=4, n_informative=3, n_redundant=0, random_state=42)\n",
        "victim_demo = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=42)\n",
        "victim_demo.fit(X_demo, y_demo)\n",
        "\n",
        "print(f\"üéØ Victim Model: {type(victim_demo).__name__}\")\n",
        "print(f\"üìä Features: {X_demo.shape[1]}\")\n",
        "print(f\"üå≤ Trees in forest: {len(victim_demo.estimators_)}\")\n",
        "\n",
        "# Set up extraction\n",
        "feature_ranges_demo = [(X_demo[:, i].min(), X_demo[:, i].max())\n",
        "                      for i in range(X_demo.shape[1])]\n",
        "real_extractor = RealModelExtractionAttack(victim_demo)\n",
        "\n",
        "# Method 1: Statistical extraction\n",
        "extracted_model, fidelity = real_extractor.extract_by_sampling(feature_ranges_demo, n_queries=5000)\n",
        "\n",
        "# Method 2: Boundary search\n",
        "boundary_points = real_extractor.extract_by_boundary_search(feature_ranges_demo)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üí° KEY INSIGHT: We achieved high fidelity without knowing the tree structure!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìè What Does \"97+% Fidelity\" Actually Mean?\n",
        "\n",
        "**Extraction Fidelity** measures how well the extracted model mimics the victim model's behavior:\n",
        "\n",
        "Fidelity = (Number of matching predictions / Total test samples) √ó 100%\n",
        "\n",
        "In our example:\n",
        "- We tested on 1000 random points\n",
        "- The victim model made predictions on these points\n",
        "- Our extracted model made predictions on the same points\n",
        "- 984 out of 1000 predictions matched (98.4%)\n",
        "\n",
        "**What this means:**\n",
        "- ‚úÖ **97+% fidelity**: The extracted model behaves almost identically to the victim\n",
        "- ‚úÖ **High success**: An attacker can use this model as a substitute\n",
        "- ‚ùå **Privacy breach**: The model's decision logic has been effectively stolen\n",
        "\n",
        "**Important caveats:**\n",
        "1. High fidelity ‚â† identical internal structure\n",
        "2. The extracted model might use completely different rules\n",
        "3. What matters is input‚Üíoutput behavior matching\n",
        "\n",
        "**Real-world impact**: If you query the victim model with a customer's data, 97+% of the time the extracted model will give the same answer. This is sufficient for most malicious purposes!"
      ],
      "metadata": {
        "id": "aC0AVfRbF_lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üîç FIDELITY DEMONSTRATION: What 98.4% Actually Looks Like\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Generate 20 test samples to show predictions\n",
        "test_samples = np.array([[np.random.uniform(low, high) for low, high in feature_ranges_demo]  for _ in range(20)])\n",
        "\n",
        "# Get predictions from both models\n",
        "victim_preds = victim_demo.predict(test_samples)\n",
        "extracted_preds = extracted_model.predict(test_samples)\n",
        "\n",
        "# Show comparison\n",
        "print(\"\\nSample predictions comparison:\")\n",
        "print(\"Sample | Victim | Extracted | Match?\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "matches = 0\n",
        "for i in range(20):\n",
        "    match = victim_preds[i] == extracted_preds[i]\n",
        "    matches += match\n",
        "    symbol = \"‚úÖ\" if match else \"‚ùå\"\n",
        "    print(f\"  {i+1:2d}   |   {victim_preds[i]}    |     {extracted_preds[i]}     | {symbol}\")\n",
        "\n",
        "print(f\"\\nIn this sample: {matches}/20 predictions match ({matches/20*100:.0f}%)\")\n",
        "print(\"\\nüí° This is what 97+% fidelity means in practice!\")"
      ],
      "metadata": {
        "id": "22iwSQfmG65H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dimensionality_demo"
      },
      "outputs": [],
      "source": [
        "# Show how extraction becomes harder with more dimensions\n",
        "results = real_extractor.demonstrate_dimensionality_challenge(max_dims=10)\n",
        "\n",
        "print(\"\\nüéì DIMENSIONALITY LESSONS:\")\n",
        "print(\"=\" * 50)\n",
        "print(\"1. Extraction cost grows EXPONENTIALLY with dimensions\")\n",
        "print(\"2. Grid search becomes impractical above ~5 dimensions\")\n",
        "print(\"3. Smart sampling helps but still faces limits\")\n",
        "print(\"4. High-dimensional models have natural extraction resistance\")\n",
        "print(\"\\n‚ö†Ô∏è  BUT: Attackers adapt with:\")\n",
        "print(\"   ‚Ä¢ Dimensionality reduction attacks\")\n",
        "print(\"   ‚Ä¢ Exploiting feature correlations\")\n",
        "print(\"   ‚Ä¢ Active learning to find important regions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "practical_implications"
      },
      "source": [
        "## üí° Practical Implications & Recommendations\n",
        "\n",
        "### For Defenders:\n",
        "1. **Use High-Dimensional Features** when possible\n",
        "   - Add engineered features that preserve utility\n",
        "   - Use feature interactions and polynomial features\n",
        "   \n",
        "2. **Monitor Query Patterns**\n",
        "   - Dense sampling creates uniform query distributions\n",
        "   - Boundary search creates clustered patterns\n",
        "   - Both are detectable!\n",
        "\n",
        "3. **Implement Query Budgets**\n",
        "   - Limit queries per user/session\n",
        "   - Rate limiting becomes more effective in high dimensions\n",
        "\n",
        "### For Attackers:\n",
        "1. **Dimensionality Reduction First**\n",
        "   - Use PCA or autoencoders to find lower-dimensional representations\n",
        "   - Focus on most important features\n",
        "   \n",
        "2. **Active Learning**\n",
        "   - Don't sample uniformly - focus on uncertain regions\n",
        "   - Use uncertainty sampling to maximize information gain\n",
        "  \n",
        "\n",
        "### The Bottom Line:\n",
        "- **Low dimensions (2-5)**: Extraction is easy and cheap\n",
        "- **Medium dimensions (6-10)**: Extraction possible but costly\n",
        "- **High dimensions (11+)**: Natural protection, extraction very difficult\n",
        "- **Very high dimensions (20+)**: Practically impossible without shortcuts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_real_extraction"
      },
      "source": [
        "## üéì Key Takeaways: Real-World Model Extraction\n",
        "\n",
        "1. **Extraction is about behavior, not structure** - Attackers don't need your exact tree\n",
        "\n",
        "2. **Two main approaches work in practice**:\n",
        "   - Dense sampling: High fidelity but expensive\n",
        "   - Boundary search: Efficient but may miss complexity\n",
        "\n",
        "3. **Dimensionality is your friend** (as a defender):\n",
        "   - Each dimension multiplies the search space\n",
        "   - Natural protection emerges around 10+ dimensions\n",
        "   - But beware of dimensionality reduction attacks\n",
        "\n",
        "4. **Practical defenses combine multiple strategies**:\n",
        "   - Use high dimensions when possible\n",
        "   - Implement query monitoring and limits\n",
        "   - Add controlled noise\n",
        "   - Use ensemble diversity\n",
        "\n",
        "5. **The arms race continues**:\n",
        "   - Attackers develop new techniques (active learning, transfer learning)\n",
        "   - Defenders must stay vigilant and adaptive\n",
        "   - Security is a continuous process, not a destination\n",
        "\n",
        "**Remember**: In the real world, perfect extraction isn't needed - even 80% fidelity can be valuable to an attacker!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}