{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgracie52/bh-2025/blob/main/EnhancedKnnLab_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNN_Security_Title"
      },
      "source": [
        "# üîí kNN Security Lab: Interactive Attacks and Defenses\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "By the end of this lab, you will:\n",
        "1. **Understand** why kNN's distance-based nature creates unique vulnerabilities\n",
        "2. **Implement** 5+ types of attacks against kNN models with visual demonstrations\n",
        "3. **Evaluate** security-accuracy trade-offs quantitatively\n",
        "4. **Design** robust kNN systems with appropriate defenses\n",
        "\n",
        "## ‚è±Ô∏è Lab Structure (60 minutes)\n",
        "- **Part 1**: Interactive kNN Fundamentals (10 min)\n",
        "- **Part 2**: Attack Demonstrations (30 min)\n",
        "- **Part 3**: Defense Mechanisms (15 min)\n",
        "- **Part 4**: Production Security (5 min)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U plotly\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = 'colab'"
      ],
      "metadata": {
        "id": "Pv_2KEAQlKoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_imports"
      },
      "outputs": [],
      "source": [
        "# üõ†Ô∏è Setup and Imports\n",
        "!pip install numpy matplotlib ipywidgets scikit-learn plotly -q\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import make_blobs, make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Custom color schemes for better visualization\n",
        "COLORS = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "ATTACK_COLOR = '#FF6B6B'\n",
        "DEFENSE_COLOR = '#4ECDC4'\n",
        "CLEAN_COLOR = '#45B7D1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_fundamentals"
      },
      "source": [
        "## Part 1: Interactive kNN Fundamentals üéì\n",
        "\n",
        "### Understanding kNN Through Interactive Visualization\n",
        "\n",
        "k-Nearest Neighbors is like asking your neighbors for advice:\n",
        "- You look at the **k closest** data points\n",
        "- They \"vote\" on what class the new point should be\n",
        "- Majority wins!\n",
        "\n",
        "**Security Insight**: This voting mechanism creates unique vulnerabilities - if an attacker can influence who your \"neighbors\" are, they control the vote!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Circle\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import make_blobs\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Generate data\n",
        "np.random.seed(42)\n",
        "X, y = make_blobs(n_samples=50, centers=2, n_features=2,\n",
        "                  center_box=(-4, 4), random_state=42)\n",
        "\n",
        "# Calculate bounds\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "\n",
        "def create_dynamic_knn_demo():\n",
        "    \"\"\"Create kNN demo with dynamic slider updates\"\"\"\n",
        "\n",
        "    # Create output widgets\n",
        "    plot_output = widgets.Output()\n",
        "    info_output = widgets.Output()\n",
        "\n",
        "    # Create control widgets\n",
        "    k_slider = widgets.IntSlider(\n",
        "        value=3, min=1, max=15, step=2,\n",
        "        description='k value:',\n",
        "        continuous_update=True,  # Enable continuous updates\n",
        "        layout=widgets.Layout(width='400px')\n",
        "    )\n",
        "\n",
        "    x_slider = widgets.FloatSlider(\n",
        "        value=0.0, min=x_min, max=x_max, step=0.1,\n",
        "        description='Test X:',\n",
        "        continuous_update=True,  # Enable continuous updates\n",
        "        layout=widgets.Layout(width='400px'),\n",
        "        readout_format='.1f'\n",
        "    )\n",
        "\n",
        "    y_slider = widgets.FloatSlider(\n",
        "        value=0.0, min=y_min, max=y_max, step=0.1,\n",
        "        description='Test Y:',\n",
        "        continuous_update=True,  # Enable continuous updates\n",
        "        layout=widgets.Layout(width='400px'),\n",
        "        readout_format='.1f'\n",
        "    )\n",
        "\n",
        "    show_neighbors = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description='Show Neighbors'\n",
        "    )\n",
        "\n",
        "    def update_plot(*args):\n",
        "        \"\"\"Update plot based on current slider values\"\"\"\n",
        "        with plot_output:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "            # Get current values\n",
        "            k = k_slider.value\n",
        "            test_x = x_slider.value\n",
        "            test_y = y_slider.value\n",
        "            show_n = show_neighbors.value\n",
        "\n",
        "            # Create figure\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "            # Train KNN\n",
        "            knn = KNeighborsClassifier(n_neighbors=k)\n",
        "            knn.fit(X, y)\n",
        "\n",
        "            # Create decision boundary\n",
        "            h = 0.1\n",
        "            xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                               np.arange(y_min, y_max, h))\n",
        "            Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "            # Plot decision boundaries\n",
        "            ax1.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu', levels=[0, 0.5, 1])\n",
        "            ax2.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu', levels=[0, 0.5, 1])\n",
        "\n",
        "            # Plot training points\n",
        "            for i in range(2):\n",
        "                mask = y == i\n",
        "                color = 'darkred' if i == 0 else 'darkblue'\n",
        "                ax1.scatter(X[mask, 0], X[mask, 1], c=color, s=50,\n",
        "                           edgecolor='black', label=f'Class {i}')\n",
        "                ax2.scatter(X[mask, 0], X[mask, 1], c=color, s=50,\n",
        "                           edgecolor='black')\n",
        "\n",
        "            # Make prediction\n",
        "            test_point = np.array([[test_x, test_y]])\n",
        "            prediction = knn.predict(test_point)[0]\n",
        "            distances, indices = knn.kneighbors(test_point)\n",
        "            neighbors = X[indices[0]]\n",
        "            neighbor_classes = y[indices[0]]\n",
        "\n",
        "            # Plot test point\n",
        "            test_color = 'red' if prediction == 0 else 'blue'\n",
        "            ax2.scatter(test_x, test_y, c='yellow', s=500, marker='*',\n",
        "                       edgecolor='black', linewidth=2, zorder=5)\n",
        "\n",
        "            # Add crosshairs for test point\n",
        "            ax2.axhline(y=test_y, color='gray', linestyle=':', alpha=0.3, linewidth=1)\n",
        "            ax2.axvline(x=test_x, color='gray', linestyle=':', alpha=0.3, linewidth=1)\n",
        "\n",
        "            # Show neighbors if enabled\n",
        "            if show_n:\n",
        "                # Draw lines to neighbors\n",
        "                for neighbor in neighbors:\n",
        "                    ax2.plot([test_x, neighbor[0]], [test_y, neighbor[1]],\n",
        "                            'green', alpha=0.6, linewidth=2, linestyle='--')\n",
        "\n",
        "                # Highlight neighbors\n",
        "                for i, (neighbor, nclass) in enumerate(zip(neighbors, neighbor_classes)):\n",
        "                    circle = Circle((neighbor[0], neighbor[1]), 0.25,\n",
        "                                  fill=False, edgecolor='green', linewidth=2.5)\n",
        "                    ax2.add_patch(circle)\n",
        "\n",
        "            # Set titles\n",
        "            ax1.set_title(f'Decision Boundary (k={k})', fontsize=14)\n",
        "            ax2.set_title(f'Test Point Classification: Class {prediction}', fontsize=14)\n",
        "\n",
        "            # Set labels and limits\n",
        "            for ax in [ax1, ax2]:\n",
        "                ax.set_xlabel('Feature 1')\n",
        "                ax.set_ylabel('Feature 2')\n",
        "                ax.set_xlim(x_min, x_max)\n",
        "                ax.set_ylim(y_min, y_max)\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "            ax1.legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        # Update info display\n",
        "        with info_output:\n",
        "            clear_output(wait=True)\n",
        "            votes = [np.sum(neighbor_classes == i) for i in range(2)]\n",
        "\n",
        "            print(f\"üìç Test Point: ({test_x:.2f}, {test_y:.2f})\")\n",
        "            print(f\"üéØ Predicted Class: {prediction}\")\n",
        "            print(f\"üó≥Ô∏è  Votes: Class 0 (Red) = {votes[0]}, Class 1 (Blue) = {votes[1]}\")\n",
        "            print(f\"üîê Security: Need {k//2 + 1} neighbors to flip prediction\")\n",
        "\n",
        "    # Connect sliders to update function\n",
        "    k_slider.observe(update_plot, names='value')\n",
        "    x_slider.observe(update_plot, names='value')\n",
        "    y_slider.observe(update_plot, names='value')\n",
        "    show_neighbors.observe(update_plot, names='value')\n",
        "\n",
        "    # Create layout\n",
        "    controls = widgets.VBox([\n",
        "        widgets.HTML(\"<h3>üéÆ k-Nearest Neighbors Interactive Demo</h3>\"),\n",
        "        k_slider,\n",
        "        x_slider,\n",
        "        y_slider,\n",
        "        show_neighbors\n",
        "    ])\n",
        "\n",
        "    # Display everything\n",
        "    display(widgets.VBox([controls, plot_output, info_output]))\n",
        "\n",
        "    # Initial plot\n",
        "    update_plot()\n",
        "\n",
        "# Run the demo\n",
        "create_dynamic_knn_demo()"
      ],
      "metadata": {
        "id": "2xuKxvPTcGlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "distance_metrics_section"
      },
      "source": [
        "### üìê Distance Metrics: The Heart of kNN Security\n",
        "\n",
        "kNN relies on distance to find neighbors. Different metrics create different vulnerabilities!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "distance_metrics_demo"
      },
      "outputs": [],
      "source": [
        "# üìä Interactive Distance Metric Comparison\n",
        "def visualize_distance_metrics():\n",
        "    \"\"\"Show how different distance metrics affect neighbor selection\"\"\"\n",
        "\n",
        "    # Generate data\n",
        "    X, y = make_blobs(n_samples=30, centers=2, random_state=42)\n",
        "    test_point = np.array([[0, 0]])\n",
        "\n",
        "    metrics = ['euclidean', 'manhattan', 'chebyshev']\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    for idx, (ax, metric) in enumerate(zip(axes, metrics)):\n",
        "        # Train kNN with different metric\n",
        "        knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
        "        knn.fit(X, y)\n",
        "\n",
        "        # Get neighbors\n",
        "        distances, indices = knn.kneighbors(test_point)\n",
        "        neighbors = X[indices[0]]\n",
        "\n",
        "        # Plot\n",
        "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', s=100, alpha=0.6)\n",
        "        ax.scatter(test_point[0, 0], test_point[0, 1],\n",
        "                  c='red', marker='*', s=500, edgecolor='black')\n",
        "\n",
        "        # Highlight neighbors\n",
        "        ax.scatter(neighbors[:, 0], neighbors[:, 1],\n",
        "                  s=200, facecolors='none', edgecolors='green', linewidth=3)\n",
        "\n",
        "        # Draw distance circles/squares\n",
        "        if metric == 'euclidean':\n",
        "            circle = plt.Circle(test_point[0], distances[0, -1],\n",
        "                              fill=False, color='gray', linestyle='--')\n",
        "            ax.add_patch(circle)\n",
        "        elif metric == 'manhattan':\n",
        "            # Draw diamond for Manhattan distance\n",
        "            d = distances[0, -1]\n",
        "            diamond = plt.Polygon([(test_point[0, 0], test_point[0, 1] + d),\n",
        "                                 (test_point[0, 0] + d, test_point[0, 1]),\n",
        "                                 (test_point[0, 0], test_point[0, 1] - d),\n",
        "                                 (test_point[0, 0] - d, test_point[0, 1])],\n",
        "                                fill=False, edgecolor='gray', linestyle='--')\n",
        "            ax.add_patch(diamond)\n",
        "        else:  # chebyshev\n",
        "            d = distances[0, -1]\n",
        "            square = plt.Rectangle((test_point[0, 0] - d, test_point[0, 1] - d),\n",
        "                                 2*d, 2*d, fill=False, color='gray', linestyle='--')\n",
        "            ax.add_patch(square)\n",
        "\n",
        "        ax.set_title(f'{metric.capitalize()} Distance', fontsize=14)\n",
        "        ax.set_aspect('equal')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.suptitle('üéØ How Distance Metrics Affect Neighbor Selection', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n‚ö†Ô∏è Security Note: Different ways of measuring distance have different weaknesses!\")\n",
        "    print(\"  Please note which traning points are used as the nearest neighbors when using different distance metrics.\")\n",
        "    print(\"- Euclidean (straight line): An attacker can make small changes in any direction to fool the system\")\n",
        "    print(\"- Manhattan (city blocks): An attacker gets better results by moving straight up/down or left/right\")\n",
        "    print(\"- Chebyshev (maximum): An attacker only needs to change ONE thing by a lot to trick the system\")\n",
        "\n",
        "visualize_distance_metrics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_attacks"
      },
      "source": [
        "## Part 2: Attack Demonstrations üó°Ô∏è\n",
        "\n",
        "Now let's see how attackers can exploit kNN's vulnerabilities. We'll demonstrate 5 major attack types with interactive visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attack1_label_flipping"
      },
      "source": [
        "### Attack 1: Strategic Label Flipping üîÑ\n",
        "\n",
        "Instead of randomly flipping labels, smart attackers target points near decision boundaries for maximum impact."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons, make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# üéØ Strategic Label Flipping Attack - Enhanced Version\n",
        "class StrategicLabelFlippingAttack:\n",
        "    \"\"\"\n",
        "    Label flipping attack that targets boundary points to maximize classifier confusion.\n",
        "    \"\"\"\n",
        "    def __init__(self, X_train, y_train, k=5):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.k = k\n",
        "        self.knn = KNeighborsClassifier(n_neighbors=k)\n",
        "        self.knn.fit(X_train, y_train)\n",
        "\n",
        "    def find_boundary_points(self, threshold=0.7):\n",
        "        \"\"\"\n",
        "        Find points near decision boundary (most vulnerable to flipping).\n",
        "        Points with ~50/50 neighbor split are on the boundary.\n",
        "        \"\"\"\n",
        "        boundary_scores = []\n",
        "\n",
        "        for i, x in enumerate(self.X_train):\n",
        "            # Get k+1 neighbors (including self)\n",
        "            distances, indices = self.knn.kneighbors(x.reshape(1, -1),\n",
        "                                                    n_neighbors=self.k+1)\n",
        "            neighbor_indices = indices[0][1:]  # Exclude self\n",
        "            neighbor_labels = self.y_train[neighbor_indices]\n",
        "\n",
        "            # Calculate proportion of neighbors with same label as point\n",
        "            same_label_ratio = np.mean(neighbor_labels == self.y_train[i])\n",
        "\n",
        "            # Boundary points have mixed neighborhoods (ratio close to 0.5)\n",
        "            boundary_score = 1 - abs(same_label_ratio - 0.5) * 2\n",
        "            boundary_scores.append(boundary_score)\n",
        "\n",
        "        return np.array(boundary_scores)\n",
        "\n",
        "    def select_attack_points(self, flip_ratio=0.1, strategy='boundary'):\n",
        "        \"\"\"\n",
        "        Select points to flip based on attack strategy.\n",
        "        \"\"\"\n",
        "        n_flip = int(len(self.y_train) * flip_ratio)\n",
        "\n",
        "        if strategy == 'boundary':\n",
        "            # Target boundary points (most effective)\n",
        "            scores = self.find_boundary_points()\n",
        "            flip_indices = np.argsort(scores)[-n_flip:]\n",
        "        elif strategy == 'random':\n",
        "            # Random selection (baseline)\n",
        "            flip_indices = np.random.choice(len(self.y_train), n_flip, replace=False)\n",
        "        elif strategy == 'cluster_center':\n",
        "            # Target points in center of clusters (most damaging)\n",
        "            scores = self.calculate_centrality_scores()\n",
        "            flip_indices = np.argsort(scores)[-n_flip:]\n",
        "\n",
        "        return flip_indices\n",
        "\n",
        "    def calculate_centrality_scores(self):\n",
        "        \"\"\"\n",
        "        Find points that are central to their class clusters.\n",
        "        \"\"\"\n",
        "        centrality_scores = []\n",
        "\n",
        "        for i, x in enumerate(self.X_train):\n",
        "            # Find same-class neighbors\n",
        "            same_class_mask = self.y_train == self.y_train[i]\n",
        "            same_class_points = self.X_train[same_class_mask]\n",
        "\n",
        "            # Calculate average distance to same-class points\n",
        "            if len(same_class_points) > 1:\n",
        "                distances = np.linalg.norm(same_class_points - x, axis=1)\n",
        "                avg_distance = np.mean(distances[distances > 0])  # Exclude self\n",
        "                centrality_scores.append(1 / (avg_distance + 1e-6))\n",
        "            else:\n",
        "                centrality_scores.append(0)\n",
        "\n",
        "        return np.array(centrality_scores)\n",
        "\n",
        "    def attack(self, flip_ratio=0.1, strategy='boundary'):\n",
        "        \"\"\"\n",
        "        Execute label flipping attack.\n",
        "        \"\"\"\n",
        "        flip_indices = self.select_attack_points(flip_ratio, strategy)\n",
        "\n",
        "        # Create poisoned labels\n",
        "        y_poisoned = self.y_train.copy()\n",
        "        y_poisoned[flip_indices] = 1 - y_poisoned[flip_indices]\n",
        "\n",
        "        return y_poisoned, flip_indices\n",
        "\n",
        "# Enhanced visualization function\n",
        "def visualize_boundary_explanation(ax, X_train, y_train, attacker):\n",
        "    \"\"\"\n",
        "    Show WHY certain points are boundary points by visualizing their neighborhoods.\n",
        "    \"\"\"\n",
        "    # Find the highest boundary score points\n",
        "    boundary_scores = attacker.find_boundary_points()\n",
        "    top_boundary_indices = np.argsort(boundary_scores)[-3:]  # Top 3 boundary points\n",
        "\n",
        "    # Plot all points with class colors\n",
        "    class_0_mask = y_train == 0\n",
        "    class_1_mask = y_train == 1\n",
        "\n",
        "    ax.scatter(X_train[class_0_mask, 0], X_train[class_0_mask, 1],\n",
        "              c='blue', s=30, alpha=0.3, marker='o', label='Class 0')\n",
        "    ax.scatter(X_train[class_1_mask, 0], X_train[class_1_mask, 1],\n",
        "              c='red', s=30, alpha=0.3, marker='s', label='Class 1')\n",
        "\n",
        "    # For each top boundary point, show its neighbors\n",
        "    for idx in top_boundary_indices:\n",
        "        point = X_train[idx]\n",
        "\n",
        "        # Get neighbors\n",
        "        distances, indices = attacker.knn.kneighbors(point.reshape(1, -1),\n",
        "                                                    n_neighbors=attacker.k+1)\n",
        "        neighbor_indices = indices[0][1:]  # Exclude self\n",
        "\n",
        "        # Draw connections to neighbors\n",
        "        for n_idx in neighbor_indices:\n",
        "            neighbor = X_train[n_idx]\n",
        "            ax.plot([point[0], neighbor[0]], [point[1], neighbor[1]],\n",
        "                   'gray', alpha=0.5, linewidth=1, zorder=1)\n",
        "\n",
        "        # Highlight the boundary point\n",
        "        marker = 'o' if y_train[idx] == 0 else 's'\n",
        "        color = 'blue' if y_train[idx] == 0 else 'red'\n",
        "        ax.scatter(point[0], point[1], c=color, s=300,\n",
        "                  marker=marker, edgecolor='black', linewidth=3, zorder=3)\n",
        "\n",
        "        # Count neighbor classes\n",
        "        neighbor_labels = y_train[neighbor_indices]\n",
        "        class_0_count = np.sum(neighbor_labels == 0)\n",
        "        class_1_count = np.sum(neighbor_labels == 1)\n",
        "\n",
        "        # Add text showing neighbor composition\n",
        "        ax.annotate(f'{class_0_count}|{class_1_count}',\n",
        "                   xy=(point[0], point[1]), xytext=(5, 5),\n",
        "                   textcoords='offset points', fontsize=10,\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
        "\n",
        "    ax.set_title('Why Boundary Points?\\n' +\n",
        "                'Boundary points have mixed neighbors (shown as #blue|#red)\\n' +\n",
        "                'Lines connect to k=5 nearest neighbors', fontsize=11)\n",
        "    ax.legend(loc='upper right', fontsize=9)\n",
        "\n",
        "def visualize_attack_process(X_train, y_train, y_poisoned, flip_indices,\n",
        "                           X_test, y_test, attacker):\n",
        "    \"\"\"\n",
        "    Create comprehensive visualization of the attack process.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(22, 17))\n",
        "\n",
        "    # Create grid layout\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # 1. Original data with boundary scores\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    boundary_scores = attacker.find_boundary_points()\n",
        "\n",
        "    # Plot class 0 points\n",
        "    class_0_mask = y_train == 0\n",
        "    class_1_mask = y_train == 1\n",
        "\n",
        "    # Use different markers for different classes\n",
        "    # Size represents boundary score\n",
        "    sizes = 50 + boundary_scores * 200  # Scale sizes based on boundary score\n",
        "\n",
        "    # Plot class 0 as circles\n",
        "    scatter1 = ax1.scatter(X_train[class_0_mask, 0], X_train[class_0_mask, 1],\n",
        "                          c=boundary_scores[class_0_mask], cmap='RdYlBu_r',\n",
        "                          s=sizes[class_0_mask], marker='o',\n",
        "                          edgecolor='blue', linewidth=2, alpha=0.7,\n",
        "                          label='Class 0')\n",
        "\n",
        "    # Plot class 1 as squares\n",
        "    scatter2 = ax1.scatter(X_train[class_1_mask, 0], X_train[class_1_mask, 1],\n",
        "                          c=boundary_scores[class_1_mask], cmap='RdYlBu_r',\n",
        "                          s=sizes[class_1_mask], marker='s',\n",
        "                          edgecolor='red', linewidth=2, alpha=0.7,\n",
        "                          label='Class 1')\n",
        "\n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(scatter1, ax=ax1, label='Boundary Score')\n",
        "\n",
        "    # Highlight high boundary score points\n",
        "    high_boundary = boundary_scores > 0.7\n",
        "    ax1.scatter(X_train[high_boundary, 0], X_train[high_boundary, 1],\n",
        "               s=300, facecolors='none', edgecolors='black',\n",
        "               linewidths=3, linestyles='dashed')\n",
        "\n",
        "    ax1.set_title('Step 1: Identify Boundary Points\\n' +\n",
        "                  'Size = Boundary Score | Circles = Class 0 | Squares = Class 1\\n' +\n",
        "                  'Dashed = High Risk Boundary Points', fontsize=11)\n",
        "    ax1.legend(loc='upper right', fontsize=9)\n",
        "\n",
        "    # 2. Attack targets\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    ax2.scatter(X_train[:, 0], X_train[:, 1], c=y_train,\n",
        "               cmap='RdBu', s=60, alpha=0.5)\n",
        "    ax2.scatter(X_train[flip_indices, 0], X_train[flip_indices, 1],\n",
        "               s=200, marker='X', c='black',\n",
        "               label=f'Targets ({len(flip_indices)} points)')\n",
        "    ax2.set_title('Step 2: Select Attack Targets', fontsize=12)\n",
        "    ax2.legend()\n",
        "\n",
        "    # 3. Before/After comparison\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    # Show original labels for flipped points\n",
        "    for idx in flip_indices:\n",
        "        ax3.annotate('', xy=(X_train[idx, 0], X_train[idx, 1] + 0.1),\n",
        "                    xytext=(X_train[idx, 0], X_train[idx, 1] - 0.1),\n",
        "                    arrowprops=dict(arrowstyle='<->', color='red', lw=2))\n",
        "    ax3.scatter(X_train[:, 0], X_train[:, 1], c=y_train,\n",
        "               cmap='RdBu', s=60, alpha=0.5, label='Original')\n",
        "    ax3.scatter(X_train[flip_indices, 0], X_train[flip_indices, 1],\n",
        "               c=y_poisoned[flip_indices], cmap='RdBu', s=200,\n",
        "               marker='D', edgecolor='red', linewidth=3,\n",
        "               label='Flipped Labels')\n",
        "    ax3.set_title('Step 3: Flip Selected Labels', fontsize=12)\n",
        "    ax3.legend()\n",
        "\n",
        "    # 4-5. Decision boundaries\n",
        "    knn_clean = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn_clean.fit(X_train, y_train)\n",
        "\n",
        "    knn_poisoned = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn_poisoned.fit(X_train, y_poisoned)\n",
        "\n",
        "    ax4 = fig.add_subplot(gs[1, 0])\n",
        "    plot_decision_boundary(ax4, knn_clean, X_train, y_train,\n",
        "                          X_test, y_test, 'Clean Model')\n",
        "\n",
        "    ax5 = fig.add_subplot(gs[1, 1])\n",
        "    plot_decision_boundary(ax5, knn_poisoned, X_train, y_poisoned,\n",
        "                          X_test, y_test, 'Poisoned Model',\n",
        "                          highlight_flipped=flip_indices)\n",
        "\n",
        "    # 6. Prediction differences\n",
        "    ax6 = fig.add_subplot(gs[1, 2])\n",
        "    clean_pred = knn_clean.predict(X_test)\n",
        "    poisoned_pred = knn_poisoned.predict(X_test)\n",
        "    mismatched = clean_pred != poisoned_pred\n",
        "\n",
        "    ax6.scatter(X_test[~mismatched, 0], X_test[~mismatched, 1],\n",
        "               c='gray', s=50, alpha=0.5, label='Same prediction')\n",
        "    ax6.scatter(X_test[mismatched, 0], X_test[mismatched, 1],\n",
        "               c='red', s=100, marker='X', label='Changed prediction')\n",
        "    ax6.set_title(f'Prediction Changes\\n({np.sum(mismatched)} points affected)',\n",
        "                 fontsize=12)\n",
        "    ax6.legend()\n",
        "\n",
        "    # 7-8. Confusion matrices\n",
        "    ax7 = fig.add_subplot(gs[2, 0])\n",
        "    cm_clean = confusion_matrix(y_test, clean_pred)\n",
        "    sns.heatmap(cm_clean, annot=True, fmt='d', cmap='Blues', ax=ax7)\n",
        "    ax7.set_title('Clean Model Confusion Matrix', fontsize=12)\n",
        "    ax7.set_xlabel('Predicted')\n",
        "    ax7.set_ylabel('True')\n",
        "\n",
        "    ax8 = fig.add_subplot(gs[2, 1])\n",
        "    cm_poisoned = confusion_matrix(y_test, poisoned_pred)\n",
        "    sns.heatmap(cm_poisoned, annot=True, fmt='d', cmap='Reds', ax=ax8)\n",
        "    ax8.set_title('Poisoned Model Confusion Matrix', fontsize=12)\n",
        "    ax8.set_xlabel('Predicted')\n",
        "    ax8.set_ylabel('True')\n",
        "\n",
        "    # 9. Attack summary\n",
        "    ax9 = fig.add_subplot(gs[2, 2])\n",
        "    ax9.axis('off')\n",
        "\n",
        "    acc_clean = accuracy_score(y_test, clean_pred)\n",
        "    acc_poisoned = accuracy_score(y_test, poisoned_pred)\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "    üéØ ATTACK SUMMARY\n",
        "\n",
        "    Training Set Size: {len(y_train)}\n",
        "    Points Flipped: {len(flip_indices)} ({len(flip_indices)/len(y_train)*100:.1f}%)\n",
        "\n",
        "    Clean Accuracy: {acc_clean*100:.1f}%\n",
        "    Poisoned Accuracy: {acc_poisoned*100:.1f}%\n",
        "    Accuracy Drop: {(acc_clean-acc_poisoned)*100:.1f}%\n",
        "\n",
        "    Predictions Changed: {np.sum(mismatched)}/{len(y_test)}\n",
        "    Attack Efficiency: {np.sum(mismatched)/len(flip_indices):.2f}x\n",
        "    \"\"\"\n",
        "\n",
        "    ax9.text(0.1, 0.5, summary_text, fontsize=14,\n",
        "            verticalalignment='center',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\"))\n",
        "\n",
        "    plt.suptitle('Label Flipping Attack Analysis', fontsize=16, y=0.98)\n",
        "\n",
        "    # Add overall color guide\n",
        "    fig.text(0.5, 0.96,\n",
        "            'Color Guide: Blue = Class 0 | Red = Class 1 | Background: Light Blue = Predicts 0, Pink = Predicts 1',\n",
        "            ha='center', fontsize=10,\n",
        "            bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"white\", edgecolor=\"black\", alpha=0.9))\n",
        "\n",
        "    # Add confusion matrix explanation\n",
        "    fig.text(0.5, 0.01,\n",
        "            'Confusion Matrix Guide: Rows = True labels, Columns = Predicted labels. ' +\n",
        "            'Diagonal = Correct predictions, Off-diagonal = Errors. ' +\n",
        "            'Perfect model has all counts on diagonal.',\n",
        "            ha='center', fontsize=10,\n",
        "            bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.8))\n",
        "    return fig\n",
        "\n",
        "def plot_decision_boundary(ax, model, X_train, y_train, X_test, y_test,\n",
        "                          title, highlight_flipped=None):\n",
        "    \"\"\"Enhanced decision boundary plotting with clear legends.\"\"\"\n",
        "    x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
        "    y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                         np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot decision regions with correct labels\n",
        "    contourf = ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu', levels=1)\n",
        "    ax.contour(xx, yy, Z, colors='black', linewidths=1.5, levels=[0.5])\n",
        "\n",
        "    # Determine which region is which by checking a few points\n",
        "    # Check left side of plot\n",
        "    left_prediction = model.predict([[x_min + 0.5, (y_min + y_max) / 2]])[0]\n",
        "    # Check right side of plot\n",
        "    right_prediction = model.predict([[x_max - 0.5, (y_min + y_max) / 2]])[0]\n",
        "\n",
        "    # Place labels correctly based on actual predictions\n",
        "    if left_prediction == 0:\n",
        "        ax.text(x_min + 0.1, y_max - 0.1, 'Light Blue Region\\n(Predicts Class 1)',\n",
        "               fontsize=9, bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\", alpha=0.7))\n",
        "        ax.text(x_max - 0.7, y_max - 0.1, 'Pink Region\\n(Predicts Class 1)',\n",
        "               fontsize=9, bbox=dict(boxstyle=\"round\", facecolor=\"lightcoral\", alpha=0.7))\n",
        "    else:\n",
        "        ax.text(x_min + 0.1, y_max - 0.1, 'Pink Region\\n(Predicts Class 0)',\n",
        "               fontsize=9, bbox=dict(boxstyle=\"round\", facecolor=\"lightcoral\", alpha=0.7))\n",
        "        ax.text(x_max - 0.7, y_max - 0.1, 'Light Blue Region\\n(Predicts Class 0)',\n",
        "               fontsize=9, bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\", alpha=0.7))\n",
        "\n",
        "    # Plot training points (circles and squares)\n",
        "    train_0 = ax.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1],\n",
        "                        c='blue', marker='o', s=50, alpha=0.7,\n",
        "                        edgecolor='darkblue', label='Train Class 0')\n",
        "    train_1 = ax.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1],\n",
        "                        c='red', marker='s', s=50, alpha=0.7,\n",
        "                        edgecolor='darkred', label='Train Class 1')\n",
        "\n",
        "    # Highlight flipped points with yellow X\n",
        "    if highlight_flipped is not None:\n",
        "        ax.scatter(X_train[highlight_flipped, 0], X_train[highlight_flipped, 1],\n",
        "                  s=200, marker='X', c='yellow', edgecolor='black',\n",
        "                  linewidth=2, label='Flipped Points', zorder=5)\n",
        "\n",
        "    # Plot test points (triangles)\n",
        "    test_0 = ax.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1],\n",
        "                       c='blue', marker='^', s=100, edgecolor='black',\n",
        "                       label='Test Class 0')\n",
        "    test_1 = ax.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1],\n",
        "                       c='red', marker='^', s=100, edgecolor='black',\n",
        "                       label='Test Class 1')\n",
        "\n",
        "    acc = accuracy_score(y_test, model.predict(X_test))\n",
        "    ax.set_title(f'{title}\\nTest Accuracy: {acc*100:.1f}%', fontsize=12)\n",
        "\n",
        "    # Add legend with clear descriptions\n",
        "    ax.legend(loc='upper left', fontsize=8, framealpha=0.9)\n",
        "\n",
        "    # Add shape legend\n",
        "    ax.text(0.02, 0.15, 'Shapes:\\n‚óã‚ñ° = Training\\n‚ñ≥ = Test',\n",
        "            transform=ax.transAxes, fontsize=9,\n",
        "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
        "            verticalalignment='top')\n",
        "\n",
        "# Main demonstration function\n",
        "def demonstrate_label_flipping_attack():\n",
        "    \"\"\"\n",
        "    Comprehensive demonstration of label flipping attacks.\n",
        "\n",
        "    WHAT IS A LABEL FLIPPING ATTACK?\n",
        "    ================================\n",
        "    1. Attacker gains access to training data (before model training)\n",
        "    2. Attacker identifies most impactful points to corrupt\n",
        "    3. Attacker manually changes the labels of these points (0‚Üí1 or 1‚Üí0)\n",
        "    4. Model is trained on this poisoned dataset\n",
        "    5. Result: Model learns incorrect patterns and performs poorly\n",
        "\n",
        "    This is a training-time attack, not a test-time attack!\n",
        "    \"\"\"\n",
        "    print(\"üî¨ LABEL FLIPPING ATTACK DEMONSTRATION\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Attack Type: Training Data Poisoning\")\n",
        "    print(\"Attack Goal: Degrade model performance by corrupting labels\")\n",
        "    print(\"Attack Method: Flip labels of carefully selected training points\")\n",
        "    print(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "    # Generate more complex data for better demonstration\n",
        "    X, y = make_moons(n_samples=150, noise=0.15, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create attacker\n",
        "    attacker = StrategicLabelFlippingAttack(X_train, y_train, k=5)\n",
        "\n",
        "    # Execute attack\n",
        "    y_poisoned, flip_indices = attacker.attack(flip_ratio=0.15, strategy='boundary')\n",
        "\n",
        "    # Create visualization\n",
        "    fig = visualize_attack_process(X_train, y_train, y_poisoned, flip_indices,\n",
        "                                 X_test, y_test, attacker)\n",
        "    plt.show()\n",
        "\n",
        "    # Compare different attack strategies\n",
        "    print(\"\\nüìä COMPARING ATTACK STRATEGIES\\n\")\n",
        "    print(\"Attack efficiency measures how many test predictions change per flipped training point.\\n\")\n",
        "    strategies = ['random', 'boundary', 'cluster_center']\n",
        "\n",
        "    for strategy in strategies:\n",
        "        y_poisoned, flip_indices = attacker.attack(flip_ratio=0.15, strategy=strategy)\n",
        "\n",
        "        knn_clean = KNeighborsClassifier(n_neighbors=5)\n",
        "        knn_clean.fit(X_train, y_train)\n",
        "\n",
        "        knn_poisoned = KNeighborsClassifier(n_neighbors=5)\n",
        "        knn_poisoned.fit(X_train, y_poisoned)\n",
        "\n",
        "        acc_clean = accuracy_score(y_test, knn_clean.predict(X_test))\n",
        "        acc_poisoned = accuracy_score(y_test, knn_poisoned.predict(X_test))\n",
        "\n",
        "        print(f\"{strategy.upper()} Strategy:\")\n",
        "        print(f\"  Clean accuracy: {acc_clean*100:.1f}%\")\n",
        "        print(f\"  Poisoned accuracy: {acc_poisoned*100:.1f}%\")\n",
        "        print(f\"  Accuracy drop: {(acc_clean-acc_poisoned)*100:.1f}%\")\n",
        "        print(f\"  Attack efficiency: {(acc_clean-acc_poisoned)/len(flip_indices)*1000:.1f}% per point\\n\")\n",
        "\n",
        "# Security implications explanation\n",
        "def explain_security_risks():\n",
        "    \"\"\"\n",
        "    Explain the security risks and real-world implications of label flipping attacks.\n",
        "    \"\"\"\n",
        "    print(\"\"\"\n",
        "    üö® SECURITY RISKS OF LABEL FLIPPING ATTACKS\n",
        "    ==========================================\n",
        "\n",
        "    HOW THE ATTACK WORKS:\n",
        "    --------------------\n",
        "    1. GAIN ACCESS: Attacker gets access to training data BEFORE model training\n",
        "       - Insider with database access\n",
        "       - Compromised data annotation service\n",
        "       - Malicious crowdworker\n",
        "\n",
        "    2. IDENTIFY TARGETS: Find most impactful points to corrupt\n",
        "       - Boundary points (mixed neighborhoods)\n",
        "       - High-influence samples\n",
        "       - Critical decision regions\n",
        "\n",
        "    3. FLIP LABELS: Manually change labels (0‚Üí1, Yes‚ÜíNo, Malware‚ÜíBenign)\n",
        "       - No need to modify features, just labels\n",
        "       - Small percentage can cause big impact\n",
        "       - Hard to detect without auditing\n",
        "\n",
        "    4. POISON TRAINING: Model learns from corrupted data\n",
        "       - Learns wrong patterns\n",
        "       - Decision boundaries shift\n",
        "       - Performance degrades\n",
        "\n",
        "    REAL-WORLD ATTACK SCENARIOS:\n",
        "    ---------------------------\n",
        "\n",
        "    a) Spam Filter Poisoning\n",
        "       - Flip spam emails to \"not spam\"\n",
        "       - Result: Spam gets through to users\n",
        "       - Impact: Phishing attacks succeed\n",
        "\n",
        "    b) Malware Detection Evasion\n",
        "       - Flip malware samples to \"benign\"\n",
        "       - Result: Malware bypasses detection\n",
        "       - Impact: System compromise\n",
        "\n",
        "    c) Medical Diagnosis Corruption\n",
        "       - Flip disease labels in training data\n",
        "       - Result: Misdiagnosis of patients\n",
        "       - Impact: Life-threatening errors\n",
        "\n",
        "    d) Financial Fraud Detection\n",
        "       - Flip fraudulent transactions to \"legitimate\"\n",
        "       - Result: Fraud goes undetected\n",
        "       - Impact: Financial losses\n",
        "\n",
        "    e) Content Moderation Bypass\n",
        "       - Flip harmful content to \"safe\"\n",
        "       - Result: Toxic content spreads\n",
        "       - Impact: Platform safety compromised\n",
        "\n",
        "    WHY ATTACKERS WANT THIS:\n",
        "    -----------------------\n",
        "\n",
        "    1. TARGETED ATTACKS\n",
        "       - Make specific malicious samples bypass detection\n",
        "       - Create blind spots in model's decision making\n",
        "       - Enable future attacks to succeed\n",
        "\n",
        "    2. ECONOMIC DAMAGE\n",
        "       - Force expensive model retraining\n",
        "       - Reduce customer trust\n",
        "       - Create liability issues\n",
        "\n",
        "    3. COMPETITIVE SABOTAGE\n",
        "       - Degrade competitor's AI performance\n",
        "       - Make their products unreliable\n",
        "       - Damage their reputation\n",
        "\n",
        "    DEFENSE STRATEGIES:\n",
        "    ------------------\n",
        "\n",
        "    1. DATA VALIDATION\n",
        "       ‚úì Multiple independent labelers\n",
        "       ‚úì Anomaly detection on labels\n",
        "       ‚úì Cross-validation checks\n",
        "       ‚úì Trusted data sources only\n",
        "\n",
        "    2. ROBUST TRAINING\n",
        "       ‚úì Algorithms resistant to label noise\n",
        "       ‚úì Ensemble methods for verification\n",
        "       ‚úì Regular performance monitoring\n",
        "       ‚úì Outlier detection in training\n",
        "\n",
        "    3. ACCESS CONTROL\n",
        "       ‚úì Limit who can modify training data\n",
        "       ‚úì Audit trail for all changes\n",
        "       ‚úì Version control on datasets\n",
        "       ‚úì Integrity checks\n",
        "\n",
        "    4. DETECTION METHODS\n",
        "       ‚úì Statistical analysis of label distribution\n",
        "       ‚úì Monitor model confidence scores\n",
        "       ‚úì Track performance metrics over time\n",
        "       ‚úì Compare with baseline models\n",
        "\n",
        "    KNN SPECIFIC VULNERABILITIES:\n",
        "    ----------------------------\n",
        "    ‚Ä¢ No model parameters = can't filter noise during training\n",
        "    ‚Ä¢ Direct memorization of labels = every flip matters\n",
        "    ‚Ä¢ Local decisions = easier to create targeted blind spots\n",
        "    ‚Ä¢ Boundary sensitivity = small flips cause big changes\n",
        "\n",
        "    ‚ö†Ô∏è REMEMBER: This is why ML systems need security-first design!\n",
        "    Never assume training data is trustworthy in production systems.\n",
        "    \"\"\")\n",
        "\n",
        "# Run the demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    demonstrate_label_flipping_attack()\n",
        "    explain_security_risks()"
      ],
      "metadata": {
        "id": "7v-nb_PwDaLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDxlsxEADZ2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "label_flipping_demo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons, make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# üéØ Strategic Label Flipping Attack - Enhanced Version\n",
        "class StrategicLabelFlippingAttack:\n",
        "    \"\"\"\n",
        "    Label flipping attack that targets boundary points to maximize classifier confusion.\n",
        "    \"\"\"\n",
        "    def __init__(self, X_train, y_train, k=5):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.k = k\n",
        "        self.knn = KNeighborsClassifier(n_neighbors=k)\n",
        "        self.knn.fit(X_train, y_train)\n",
        "\n",
        "    def find_boundary_points(self, threshold=0.7):\n",
        "        \"\"\"\n",
        "        Find points near decision boundary (most vulnerable to flipping).\n",
        "        Points with ~50/50 neighbor split are on the boundary.\n",
        "        \"\"\"\n",
        "        boundary_scores = []\n",
        "\n",
        "        for i, x in enumerate(self.X_train):\n",
        "            # Get k+1 neighbors (including self)\n",
        "            distances, indices = self.knn.kneighbors(x.reshape(1, -1),\n",
        "                                                    n_neighbors=self.k+1)\n",
        "            neighbor_indices = indices[0][1:]  # Exclude self\n",
        "            neighbor_labels = self.y_train[neighbor_indices]\n",
        "\n",
        "            # Calculate proportion of neighbors with same label as point\n",
        "            same_label_ratio = np.mean(neighbor_labels == self.y_train[i])\n",
        "\n",
        "            # Boundary points have mixed neighborhoods (ratio close to 0.5)\n",
        "            boundary_score = 1 - abs(same_label_ratio - 0.5) * 2\n",
        "            boundary_scores.append(boundary_score)\n",
        "\n",
        "        return np.array(boundary_scores)\n",
        "\n",
        "    def select_attack_points(self, flip_ratio=0.1, strategy='boundary'):\n",
        "        \"\"\"\n",
        "        Select points to flip based on attack strategy.\n",
        "        \"\"\"\n",
        "        n_flip = int(len(self.y_train) * flip_ratio)\n",
        "\n",
        "        if strategy == 'boundary':\n",
        "            # Target boundary points (most effective)\n",
        "            scores = self.find_boundary_points()\n",
        "            flip_indices = np.argsort(scores)[-n_flip:]\n",
        "        elif strategy == 'random':\n",
        "            # Random selection (baseline)\n",
        "            flip_indices = np.random.choice(len(self.y_train), n_flip, replace=False)\n",
        "        elif strategy == 'cluster_center':\n",
        "            # Target points in center of clusters (most damaging)\n",
        "            scores = self.calculate_centrality_scores()\n",
        "            flip_indices = np.argsort(scores)[-n_flip:]\n",
        "\n",
        "        return flip_indices\n",
        "\n",
        "    def calculate_centrality_scores(self):\n",
        "        \"\"\"\n",
        "        Find points that are central to their class clusters.\n",
        "        \"\"\"\n",
        "        centrality_scores = []\n",
        "\n",
        "        for i, x in enumerate(self.X_train):\n",
        "            # Find same-class neighbors\n",
        "            same_class_mask = self.y_train == self.y_train[i]\n",
        "            same_class_points = self.X_train[same_class_mask]\n",
        "\n",
        "            # Calculate average distance to same-class points\n",
        "            if len(same_class_points) > 1:\n",
        "                distances = np.linalg.norm(same_class_points - x, axis=1)\n",
        "                avg_distance = np.mean(distances[distances > 0])  # Exclude self\n",
        "                centrality_scores.append(1 / (avg_distance + 1e-6))\n",
        "            else:\n",
        "                centrality_scores.append(0)\n",
        "\n",
        "        return np.array(centrality_scores)\n",
        "\n",
        "    def attack(self, flip_ratio=0.1, strategy='boundary'):\n",
        "        \"\"\"\n",
        "        Execute label flipping attack.\n",
        "        \"\"\"\n",
        "        flip_indices = self.select_attack_points(flip_ratio, strategy)\n",
        "\n",
        "        # Create poisoned labels\n",
        "        y_poisoned = self.y_train.copy()\n",
        "        y_poisoned[flip_indices] = 1 - y_poisoned[flip_indices]\n",
        "\n",
        "        return y_poisoned, flip_indices\n",
        "\n",
        "# Enhanced visualization function\n",
        "def visualize_attack_process(X_train, y_train, y_poisoned, flip_indices,\n",
        "                           X_test, y_test, attacker):\n",
        "    \"\"\"\n",
        "    Create comprehensive visualization of the attack process.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "    # Create grid layout\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # 1. Original data with boundary scores\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    boundary_scores = attacker.find_boundary_points()\n",
        "    scatter = ax1.scatter(X_train[:, 0], X_train[:, 1],\n",
        "                         c=boundary_scores, cmap='RdYlBu_r',\n",
        "                         s=100, edgecolor='black', linewidth=0.5)\n",
        "    plt.colorbar(scatter, ax=ax1, label='Boundary Score')\n",
        "    ax1.set_title('Step 1: Identify Boundary Points\\n(Red = Near Boundary)', fontsize=12)\n",
        "\n",
        "    # 2. Attack targets\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    ax2.scatter(X_train[:, 0], X_train[:, 1], c=y_train,\n",
        "               cmap='RdBu', s=60, alpha=0.5)\n",
        "    ax2.scatter(X_train[flip_indices, 0], X_train[flip_indices, 1],\n",
        "               s=200, marker='X', c='black',\n",
        "               label=f'Targets ({len(flip_indices)} points)')\n",
        "    ax2.set_title('Step 2: Select Attack Targets', fontsize=12)\n",
        "    ax2.legend()\n",
        "\n",
        "    # 3. Before/After comparison\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    # Show original labels for flipped points\n",
        "    for idx in flip_indices:\n",
        "        ax3.annotate('', xy=(X_train[idx, 0], X_train[idx, 1] + 0.1),\n",
        "                    xytext=(X_train[idx, 0], X_train[idx, 1] - 0.1),\n",
        "                    arrowprops=dict(arrowstyle='<->', color='red', lw=2))\n",
        "    ax3.scatter(X_train[:, 0], X_train[:, 1], c=y_train,\n",
        "               cmap='RdBu', s=60, alpha=0.5, label='Original')\n",
        "    ax3.scatter(X_train[flip_indices, 0], X_train[flip_indices, 1],\n",
        "               c=y_poisoned[flip_indices], cmap='RdBu', s=200,\n",
        "               marker='D', edgecolor='red', linewidth=3,\n",
        "               label='Flipped Labels')\n",
        "    ax3.set_title('Step 3: Flip Selected Labels', fontsize=12)\n",
        "    ax3.legend()\n",
        "\n",
        "    # 4-5. Decision boundaries\n",
        "    knn_clean = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn_clean.fit(X_train, y_train)\n",
        "\n",
        "    knn_poisoned = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn_poisoned.fit(X_train, y_poisoned)\n",
        "\n",
        "    ax4 = fig.add_subplot(gs[1, 0])\n",
        "    plot_decision_boundary(ax4, knn_clean, X_train, y_train,\n",
        "                          X_test, y_test, 'Clean Model')\n",
        "\n",
        "    ax5 = fig.add_subplot(gs[1, 1])\n",
        "    plot_decision_boundary(ax5, knn_poisoned, X_train, y_poisoned,\n",
        "                          X_test, y_test, 'Poisoned Model',\n",
        "                          highlight_flipped=flip_indices)\n",
        "\n",
        "    # 6. Prediction differences\n",
        "    ax6 = fig.add_subplot(gs[1, 2])\n",
        "    clean_pred = knn_clean.predict(X_test)\n",
        "    poisoned_pred = knn_poisoned.predict(X_test)\n",
        "    mismatched = clean_pred != poisoned_pred\n",
        "\n",
        "    ax6.scatter(X_test[~mismatched, 0], X_test[~mismatched, 1],\n",
        "               c='gray', s=50, alpha=0.5, label='Same prediction')\n",
        "    ax6.scatter(X_test[mismatched, 0], X_test[mismatched, 1],\n",
        "               c='red', s=100, marker='X', label='Changed prediction')\n",
        "    ax6.set_title(f'Prediction Changes\\n({np.sum(mismatched)} points affected)',\n",
        "                 fontsize=12)\n",
        "    ax6.legend()\n",
        "\n",
        "    # 7-8. Confusion matrices\n",
        "    ax7 = fig.add_subplot(gs[2, 0])\n",
        "    cm_clean = confusion_matrix(y_test, clean_pred)\n",
        "    sns.heatmap(cm_clean, annot=True, fmt='d', cmap='Blues', ax=ax7)\n",
        "    ax7.set_title('Clean Model Confusion Matrix', fontsize=12)\n",
        "    ax7.set_xlabel('Predicted')\n",
        "    ax7.set_ylabel('True')\n",
        "\n",
        "    ax8 = fig.add_subplot(gs[2, 1])\n",
        "    cm_poisoned = confusion_matrix(y_test, poisoned_pred)\n",
        "    sns.heatmap(cm_poisoned, annot=True, fmt='d', cmap='Reds', ax=ax8)\n",
        "    ax8.set_title('Poisoned Model Confusion Matrix', fontsize=12)\n",
        "    ax8.set_xlabel('Predicted')\n",
        "    ax8.set_ylabel('True')\n",
        "\n",
        "    # 9. Attack summary\n",
        "    ax9 = fig.add_subplot(gs[2, 2])\n",
        "    ax9.axis('off')\n",
        "\n",
        "    acc_clean = accuracy_score(y_test, clean_pred)\n",
        "    acc_poisoned = accuracy_score(y_test, poisoned_pred)\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "    üéØ ATTACK SUMMARY\n",
        "\n",
        "    Training Set Size: {len(y_train)}\n",
        "    Points Flipped: {len(flip_indices)} ({len(flip_indices)/len(y_train)*100:.1f}%)\n",
        "\n",
        "    Clean Accuracy: {acc_clean*100:.1f}%\n",
        "    Poisoned Accuracy: {acc_poisoned*100:.1f}%\n",
        "    Accuracy Drop: {(acc_clean-acc_poisoned)*100:.1f}%\n",
        "\n",
        "    Predictions Changed: {np.sum(mismatched)}/{len(y_test)}\n",
        "    Attack Efficiency: {np.sum(mismatched)/len(flip_indices):.2f}x\n",
        "    \"\"\"\n",
        "\n",
        "    ax9.text(0.1, 0.5, summary_text, fontsize=14,\n",
        "            verticalalignment='center',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\"))\n",
        "\n",
        "    plt.suptitle('Label Flipping Attack Analysis', fontsize=16, y=0.98)\n",
        "    return fig\n",
        "\n",
        "def plot_decision_boundary(ax, model, X_train, y_train, X_test, y_test,\n",
        "                          title, highlight_flipped=None):\n",
        "    \"\"\"Enhanced decision boundary plotting.\"\"\"\n",
        "    x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
        "    y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                         np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
        "    ax.contour(xx, yy, Z, colors='black', linewidths=0.5, alpha=0.3)\n",
        "\n",
        "    # Plot training points\n",
        "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train,\n",
        "              cmap='RdBu', edgecolor='black', s=50, alpha=0.7)\n",
        "\n",
        "    # Highlight flipped points\n",
        "    if highlight_flipped is not None:\n",
        "        ax.scatter(X_train[highlight_flipped, 0], X_train[highlight_flipped, 1],\n",
        "                  s=200, marker='X', edgecolor='yellow',\n",
        "                  linewidth=3, facecolor='none')\n",
        "\n",
        "    # Plot test points\n",
        "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test,\n",
        "              cmap='RdBu', marker='^', s=100, edgecolor='black')\n",
        "\n",
        "    acc = accuracy_score(y_test, model.predict(X_test))\n",
        "    ax.set_title(f'{title}\\nTest Accuracy: {acc*100:.1f}%', fontsize=12)\n",
        "\n",
        "# Main demonstration function\n",
        "def demonstrate_label_flipping_attack():\n",
        "    \"\"\"\n",
        "    Comprehensive demonstration of label flipping attacks.\n",
        "    \"\"\"\n",
        "    # Generate more complex data for better demonstration\n",
        "    X, y = make_moons(n_samples=150, noise=0.15, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create attacker\n",
        "    attacker = StrategicLabelFlippingAttack(X_train, y_train, k=5)\n",
        "\n",
        "    # Execute attack\n",
        "    y_poisoned, flip_indices = attacker.attack(flip_ratio=0.15, strategy='boundary')\n",
        "\n",
        "    # Create visualization\n",
        "    fig = visualize_attack_process(X_train, y_train, y_poisoned, flip_indices,\n",
        "                                 X_test, y_test, attacker)\n",
        "    plt.show()\n",
        "\n",
        "    # Compare different attack strategies\n",
        "    print(\"\\nüìä COMPARING ATTACK STRATEGIES\\n\")\n",
        "    strategies = ['random', 'boundary', 'cluster_center']\n",
        "\n",
        "    for strategy in strategies:\n",
        "        y_poisoned, flip_indices = attacker.attack(flip_ratio=0.15, strategy=strategy)\n",
        "\n",
        "        knn_clean = KNeighborsClassifier(n_neighbors=5)\n",
        "        knn_clean.fit(X_train, y_train)\n",
        "\n",
        "        knn_poisoned = KNeighborsClassifier(n_neighbors=5)\n",
        "        knn_poisoned.fit(X_train, y_poisoned)\n",
        "\n",
        "        acc_clean = accuracy_score(y_test, knn_clean.predict(X_test))\n",
        "        acc_poisoned = accuracy_score(y_test, knn_poisoned.predict(X_test))\n",
        "\n",
        "        print(f\"{strategy.upper()} Strategy:\")\n",
        "        print(f\"  Clean accuracy: {acc_clean*100:.1f}%\")\n",
        "        print(f\"  Poisoned accuracy: {acc_poisoned*100:.1f}%\")\n",
        "        print(f\"  Accuracy drop: {(acc_clean-acc_poisoned)*100:.1f}%\")\n",
        "        print(f\"  Attack efficiency: {(acc_clean-acc_poisoned)/len(flip_indices)*1000:.1f}% per point\\n\")\n",
        "\n",
        "# Security implications explanation\n",
        "def explain_security_risks():\n",
        "    \"\"\"\n",
        "    Explain the security risks and real-world implications of label flipping attacks.\n",
        "    \"\"\"\n",
        "    print(\"\"\"\n",
        "    üö® SECURITY RISKS OF LABEL FLIPPING ATTACKS\n",
        "    ==========================================\n",
        "\n",
        "    1. TRAINING DATA POISONING\n",
        "       - Attackers with access to training data can corrupt model behavior\n",
        "       - Even small amounts of poisoning (10-15%) can significantly degrade performance\n",
        "       - Hard to detect without careful data auditing\n",
        "\n",
        "    2. REAL-WORLD ATTACK SCENARIOS\n",
        "\n",
        "       a) Crowdsourced Labeling Attacks\n",
        "          - Many ML systems use crowdsourced labels (e.g., CAPTCHA, content moderation)\n",
        "          - Malicious workers can intentionally mislabel data\n",
        "          - Example: Flipping spam/not-spam labels to let spam through\n",
        "\n",
        "       b) Supply Chain Attacks\n",
        "          - Poisoning public datasets that others use for training\n",
        "          - Compromising data annotation services\n",
        "          - Example: Medical image datasets with flipped disease labels\n",
        "\n",
        "       c) Insider Threats\n",
        "          - Malicious employees with database access\n",
        "          - Disgruntled data annotators\n",
        "          - Example: Financial fraud detection systems\n",
        "\n",
        "    3. WHY ATTACKERS WANT THIS\n",
        "\n",
        "       a) Targeted Misclassification\n",
        "          - Make specific malware appear benign\n",
        "          - Bypass content filters\n",
        "          - Evade fraud detection\n",
        "\n",
        "       b) Model Degradation\n",
        "          - Reduce overall system reliability\n",
        "          - Force expensive retraining\n",
        "          - Damage company reputation\n",
        "\n",
        "       c) Backdoor Creation\n",
        "          - Strategic flipping can create hidden behaviors\n",
        "          - Model works normally except on specific inputs\n",
        "\n",
        "    4. DEFENSE STRATEGIES\n",
        "\n",
        "       a) Data Validation\n",
        "          - Cross-validation with multiple annotators\n",
        "          - Outlier detection in label space\n",
        "          - Trusted data sources\n",
        "\n",
        "       b) Robust Training\n",
        "          - Use algorithms resistant to label noise\n",
        "          - Ensemble methods\n",
        "          - Certified defenses\n",
        "\n",
        "       c) Monitoring\n",
        "          - Track model performance over time\n",
        "          - Detect distribution shifts\n",
        "          - Audit training data regularly\n",
        "\n",
        "    5. KNN SPECIFIC VULNERABILITIES\n",
        "       - KNN is particularly vulnerable because:\n",
        "         * No training phase to filter noise\n",
        "         * Direct dependence on training labels\n",
        "         * Local decision making (easier to target specific regions)\n",
        "         * Boundary points have high influence\n",
        "\n",
        "    Remember: In production systems, assume your training data\n",
        "    might be compromised and plan defenses accordingly!\n",
        "    \"\"\")\n",
        "\n",
        "# Run the demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üî¨ LABEL FLIPPING ATTACK DEMONSTRATION\\n\")\n",
        "    demonstrate_label_flipping_attack()\n",
        "    explain_security_risks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attack3_model_extraction"
      },
      "source": [
        "### Attack 2: Model Extraction Attack üìã\n",
        "\n",
        "Can we steal the entire model by querying it? With kNN, we can often reconstruct the training data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_extraction_demo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import make_moons\n",
        "import time\n",
        "\n",
        "# üîì Model Extraction Attack\n",
        "class KNNModelExtraction:\n",
        "    def __init__(self, target_model, k=5):\n",
        "        self.target_model = target_model\n",
        "        self.k = k\n",
        "        self.extracted_points = []\n",
        "        self.extracted_labels = []\n",
        "\n",
        "    def extract_boundary_points(self, n_queries=1000, bounds=None):\n",
        "        \"\"\"Extract training data by probing decision boundaries\"\"\"\n",
        "        if bounds is None:\n",
        "            bounds = [(-5, 5), (-5, 5)]\n",
        "\n",
        "        print(\"\\nüìä PHASE 1 PROCESS:\")\n",
        "        print(\"‚îÄ\" * 60)\n",
        "        print(\"1. Generating random query points across the feature space...\")\n",
        "\n",
        "        # Generate random query points\n",
        "        query_points = np.random.uniform(\n",
        "            low=[b[0] for b in bounds],\n",
        "            high=[b[1] for b in bounds],\n",
        "            size=(n_queries, 2)\n",
        "        )\n",
        "\n",
        "        print(f\"   Generated {n_queries} random points in bounds {bounds}\")\n",
        "\n",
        "        print(\"\\n2. Querying the target model for predictions...\")\n",
        "        # Query the model\n",
        "        predictions = self.target_model.predict(query_points)\n",
        "\n",
        "        print(\"\\n3. Finding decision boundary candidates...\")\n",
        "        print(\"   For each query point:\")\n",
        "        print(\"   - Create 10 slightly perturbed neighbors (¬±0.1 noise)\")\n",
        "        print(\"   - Check if neighbors have different class predictions\")\n",
        "        print(\"   - If yes ‚Üí point is near a decision boundary!\")\n",
        "\n",
        "        # Find points near decision boundaries\n",
        "        boundary_points = []\n",
        "        for i, point in enumerate(query_points):\n",
        "            # Check local neighborhood\n",
        "            local_points = point + np.random.normal(0, 0.1, (10, 2))\n",
        "            local_preds = self.target_model.predict(local_points)\n",
        "\n",
        "            # If predictions vary, we're near a boundary\n",
        "            if len(np.unique(local_preds)) > 1:\n",
        "                boundary_points.append((point, predictions[i]))\n",
        "\n",
        "            # Progress update\n",
        "            if (i + 1) % 500 == 0:\n",
        "                print(f\"   Processed {i + 1}/{n_queries} points... Found {len(boundary_points)} boundary candidates\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Phase 1 Complete: Found {len(boundary_points)} boundary points\")\n",
        "        print(\"   These points have neighbors with different class predictions,\")\n",
        "        print(\"   indicating they're near the decision boundary!\")\n",
        "\n",
        "        return boundary_points\n",
        "\n",
        "    def refine_extraction(self, boundary_points, n_iterations=5):\n",
        "        \"\"\"Refine extraction by focusing on boundary regions\"\"\"\n",
        "        print(\"\\nüìä PHASE 2 PROCESS:\")\n",
        "        print(\"‚îÄ\" * 60)\n",
        "        print(\"Moving boundary candidates closer to the actual decision boundary...\")\n",
        "        print(f\"Using gradient descent with {n_iterations} iterations per point\")\n",
        "        print(\"\\nRefinement algorithm:\")\n",
        "        print(\"1. For each boundary candidate:\")\n",
        "        print(\"2. Estimate the gradient direction (which way leads to class change)\")\n",
        "        print(\"3. Move the point slightly TOWARDS the boundary\")\n",
        "        print(\"4. Repeat to get as close as possible to the true boundary\")\n",
        "\n",
        "        refined_points = []\n",
        "\n",
        "        for idx, (point, label) in enumerate(boundary_points[:50]):  # Limit for visualization\n",
        "            current_point = point.copy()\n",
        "\n",
        "            # Gradient descent towards decision boundary\n",
        "            for iter in range(n_iterations):\n",
        "                # Estimate gradient\n",
        "                epsilon = 0.01\n",
        "                gradient = np.zeros(2)\n",
        "\n",
        "                for dim in range(2):\n",
        "                    perturb = np.zeros(2)\n",
        "                    perturb[dim] = epsilon\n",
        "\n",
        "                    pred_plus = self.target_model.predict((current_point + perturb).reshape(1, -1))[0]\n",
        "                    pred_minus = self.target_model.predict((current_point - perturb).reshape(1, -1))[0]\n",
        "\n",
        "                    if pred_plus != pred_minus:\n",
        "                        gradient[dim] = 1 if pred_plus != label else -1\n",
        "\n",
        "                # Move towards boundary\n",
        "                current_point -= 0.1 * gradient\n",
        "\n",
        "            refined_points.append((current_point, label))\n",
        "\n",
        "            if (idx + 1) % 10 == 0:\n",
        "                print(f\"   Refined {idx + 1}/50 points...\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Phase 2 Complete: Refined {len(refined_points)} boundary points\")\n",
        "        print(\"   These points are now much closer to the actual decision boundary!\")\n",
        "\n",
        "        return refined_points\n",
        "\n",
        "# Demonstrate model extraction\n",
        "def demonstrate_model_extraction():\n",
        "    print(\"üéØ MODEL EXTRACTION ATTACK DEMONSTRATION\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Goal: Steal a machine learning model by only querying its predictions\")\n",
        "    print(\"Target: k-Nearest Neighbors (kNN) classifier\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Generate training data\n",
        "    print(\"\\nüîí Setting up the victim model...\")\n",
        "    X_train, y_train = make_moons(n_samples=50, noise=0.15, random_state=42)\n",
        "    X_train = X_train * 2  # Scale up for better visualization\n",
        "\n",
        "    # Train target model\n",
        "    target_model = KNeighborsClassifier(n_neighbors=5)\n",
        "    target_model.fit(X_train, y_train)\n",
        "    print(f\"   Trained kNN model with {len(X_train)} private training points\")\n",
        "    print(\"   The attacker CANNOT see these training points!\")\n",
        "\n",
        "    # Create attacker\n",
        "    attacker = KNNModelExtraction(target_model)\n",
        "\n",
        "    # Extract boundary points\n",
        "    print(\"\\nüîç ATTACK PHASE 1: Probing decision boundaries...\")\n",
        "    boundary_points = attacker.extract_boundary_points(n_queries=2000,\n",
        "                                                      bounds=[(-3, 3), (-2, 2)])\n",
        "\n",
        "    # Refine extraction\n",
        "    print(\"\\nüéØ ATTACK PHASE 2: Refining extraction...\")\n",
        "    refined_points = attacker.refine_extraction(boundary_points)\n",
        "\n",
        "    # Visualize the attack\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
        "    fig.suptitle('Model Extraction Attack on k-Nearest Neighbors', fontsize=16, y=0.98)\n",
        "\n",
        "    # Create mesh for decision boundaries\n",
        "    x_min, x_max = -3, 3\n",
        "    y_min, y_max = -2, 2\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                         np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "    # Plot 1: Original model\n",
        "    ax = axes[0, 0]\n",
        "    Z = target_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
        "    scatter1 = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train,\n",
        "              cmap='RdYlBu', s=100, edgecolor='black', linewidth=2)\n",
        "    ax.set_title('Original Model (Private Training Data)\\n‚ö†Ô∏è Attacker CANNOT see these points!', fontsize=14)\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "    # Add legend\n",
        "    handles, labels = scatter1.legend_elements()\n",
        "    ax.legend(handles, ['Class 0 (Blue)', 'Class 1 (Red)'], loc='upper right')\n",
        "\n",
        "    # Plot 2: Query points\n",
        "    ax = axes[0, 1]\n",
        "    query_x = [p[0][0] for p in boundary_points]\n",
        "    query_y = [p[0][1] for p in boundary_points]\n",
        "    query_c = [p[1] for p in boundary_points]\n",
        "    scatter2 = ax.scatter(query_x, query_y, c=query_c, cmap='RdYlBu',\n",
        "              s=20, alpha=0.6)\n",
        "    ax.set_title(f'Attack Phase 1: {len(boundary_points)} Boundary Queries\\n' +\n",
        "                 'üî¥ Red dots: Points predicted as Class 1\\n' +\n",
        "                 'üîµ Blue dots: Points predicted as Class 0\\n' +\n",
        "                 'Points clustered near decision boundaries', fontsize=12)\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "\n",
        "    # Plot 3: Refined extraction\n",
        "    ax = axes[1, 0]\n",
        "    ax.contourf(xx, yy, Z, alpha=0.2, cmap='RdYlBu')\n",
        "\n",
        "    # Plot refined points\n",
        "    extract_x = [p[0][0] for p in refined_points]\n",
        "    extract_y = [p[0][1] for p in refined_points]\n",
        "    extract_c = [p[1] for p in refined_points]\n",
        "    ax.scatter(extract_x, extract_y, c=extract_c, cmap='RdYlBu',\n",
        "              s=200, marker='D', edgecolor='darkred', linewidth=3,\n",
        "              label='Refined boundary points')\n",
        "\n",
        "    # Show original points for comparison (faded)\n",
        "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train,\n",
        "              cmap='RdYlBu', s=50, alpha=0.2, label='Original (hidden)')\n",
        "    ax.set_title('Attack Phase 2: Refined Boundary Points\\n' +\n",
        "                 'üíé Red/Blue diamonds: Points moved to decision boundary\\n' +\n",
        "                 'These points approximate the original training data locations!', fontsize=12)\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "    ax.legend(loc='upper right')\n",
        "\n",
        "    # Plot 4: Build stolen model\n",
        "    ax = axes[1, 1]\n",
        "\n",
        "    # Filter points with two coordinates before creating arrays\n",
        "    valid_refined_points = [(p, l) for p, l in refined_points if len(p) == 2]\n",
        "\n",
        "    # Train a new model with extracted points\n",
        "    if len(valid_refined_points) > 5:\n",
        "        stolen_X = np.array([p for p, l in valid_refined_points])\n",
        "        stolen_y = np.array([l for p, l in valid_refined_points])\n",
        "\n",
        "        print(\"\\nüèóÔ∏è BUILDING STOLEN MODEL...\")\n",
        "        print(f\"   Training new kNN model using {len(stolen_X)} extracted boundary points\")\n",
        "\n",
        "        stolen_model = KNeighborsClassifier(n_neighbors=3)\n",
        "        stolen_model.fit(stolen_X, stolen_y)\n",
        "\n",
        "        Z_stolen = stolen_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z_stolen = Z_stolen.reshape(xx.shape)\n",
        "\n",
        "        ax.contourf(xx, yy, Z_stolen, alpha=0.4, cmap='RdYlBu')\n",
        "        ax.scatter(stolen_X[:, 0], stolen_X[:, 1], c=stolen_y, cmap='RdYlBu',\n",
        "                  s=100, edgecolor='darkred', linewidth=2)\n",
        "\n",
        "        # Calculate similarity\n",
        "        agreement = np.mean(Z == Z_stolen)\n",
        "        disagreement = 1 - agreement\n",
        "\n",
        "        print(f\"\\nüìä MODEL COMPARISON:\")\n",
        "        print(f\"   Decision boundary agreement: {agreement:.1%}\")\n",
        "        print(f\"   Decision boundary disagreement: {disagreement:.1%}\")\n",
        "        print(f\"   ‚Üí The stolen model makes the SAME predictions as the original\")\n",
        "        print(f\"     on {agreement:.1%} of the feature space!\")\n",
        "\n",
        "        ax.set_title(f'Stolen Model (Trained on Extracted Points)\\n' +\n",
        "                    f'{disagreement:.1%} disagreement means {agreement:.1%} of predictions match!\\n' +\n",
        "                    'The attacker successfully replicated the model behavior', fontsize=12)\n",
        "\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚ö†Ô∏è  MODEL EXTRACTION ATTACK SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nüéØ What happened:\")\n",
        "    print(\"1. The attacker queried the model 2000 times with random points\")\n",
        "    print(\"2. Found points where nearby queries gave different predictions\")\n",
        "    print(\"3. Used gradient descent to move these points to the exact boundary\")\n",
        "    print(\"4. Trained a new model using only these boundary points\")\n",
        "    print(f\"5. The stolen model replicates {agreement:.1%} of the original's behavior!\")\n",
        "\n",
        "    print(\"\\nüí° Why it works for kNN:\")\n",
        "    print(\"‚Ä¢ kNN's decision boundaries directly reveal training data locations\")\n",
        "    print(\"‚Ä¢ Points on the boundary are equidistant from different classes\")\n",
        "    print(\"‚Ä¢ The boundary shape encodes the training data distribution\")\n",
        "    print(\"‚Ä¢ With enough boundary points, we can reconstruct the model!\")\n",
        "\n",
        "    print(\"\\nüõ°Ô∏è Defense implications:\")\n",
        "    print(\"‚Ä¢ Limit the number of queries allowed per user\")\n",
        "    print(\"‚Ä¢ Add noise to predictions near boundaries\")\n",
        "    print(\"‚Ä¢ Monitor for systematic boundary probing patterns\")\n",
        "    print(\"‚Ä¢ Consider using models less vulnerable to extraction\")\n",
        "\n",
        "demonstrate_model_extraction()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def demonstrate_uncertainty_sampling():\n",
        "    \"\"\"Show how uncertainty-based active learning works for model extraction\"\"\"\n",
        "\n",
        "    print(\"üéØ 1. UNCERTAINTY-BASED QUERY SELECTION\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nWhat does 'intelligently select queries based on uncertainty' mean?\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Create a target model\n",
        "    X, y = make_classification(n_samples=200, n_features=2, n_informative=2,\n",
        "                              n_redundant=0, n_clusters_per_class=2,\n",
        "                              flip_y=0.1, random_state=42)\n",
        "\n",
        "    target_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    target_model.fit(X, y)\n",
        "\n",
        "    print(\"\\nüìä STRATEGY: Query where the attacker's model is most uncertain\")\n",
        "    print(\"\\nStep-by-step process:\")\n",
        "    print(\"1. Start with a few random queries to build initial attacker model\")\n",
        "    print(\"2. For each potential query point, measure uncertainty:\")\n",
        "    print(\"   - If attacker predicts 51% class A, 49% class B ‚Üí HIGH uncertainty and likely close.boundary point\")\n",
        "    print(\"   - If attacker predicts 99% class A, 1% class B ‚Üí LOW uncertainty\")\n",
        "    print(\"3. Query points with highest uncertainty\")\n",
        "    print(\"4. Retrain attacker model and repeat\")\n",
        "\n",
        "    # Initial random queries\n",
        "    n_initial = 50\n",
        "    query_pool = np.random.uniform(-3, 3, size=(1000, 2))\n",
        "    initial_indices = np.random.choice(len(query_pool), n_initial, replace=False)\n",
        "\n",
        "    X_queries = query_pool[initial_indices]\n",
        "    y_queries = target_model.predict(X_queries)\n",
        "\n",
        "    # Remove initial queries from the pool\n",
        "    remaining_mask = np.ones(len(query_pool), dtype=bool)\n",
        "    remaining_mask[initial_indices] = False\n",
        "    # No need to create current_query_pool explicitly here if we use mask consistently\n",
        "\n",
        "    # Train initial attacker model\n",
        "    attacker_model = MLPClassifier(hidden_layer_sizes=(50, 50), random_state=42)\n",
        "    attacker_model.fit(X_queries, y_queries)\n",
        "\n",
        "    # Demonstrate uncertainty sampling\n",
        "    n_rounds = 5\n",
        "    queries_per_round = 20\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    # Plot initial state (Round 0)\n",
        "    ax = axes[0]\n",
        "    xx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
        "    Z = attacker_model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
        "    ax.scatter(X_queries[:, 0], X_queries[:, 1],\n",
        "              c=y_queries, cmap='RdBu', s=50,\n",
        "              edgecolor='black', label='Initial random')\n",
        "    ax.set_title(f'Round 0: {len(X_queries)} total queries')\n",
        "    ax.legend()\n",
        "\n",
        "\n",
        "    for round_idx in range(n_rounds):\n",
        "        ax = axes[round_idx + 1]\n",
        "\n",
        "        # Identify remaining points in the original query pool\n",
        "        remaining_indices = np.where(remaining_mask)[0]\n",
        "\n",
        "\n",
        "        if len(remaining_indices) > 0:\n",
        "            # Get prediction probabilities for remaining points\n",
        "            probs = attacker_model.predict_proba(query_pool[remaining_indices])\n",
        "\n",
        "            # Uncertainty = entropy of prediction\n",
        "            # High when probabilities are close to 50/50\n",
        "            uncertainty = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n",
        "\n",
        "            # Select most uncertain points from remaining indices\n",
        "            n_select = min(queries_per_round, len(remaining_indices))\n",
        "            uncertain_indices_in_remaining = np.argsort(uncertainty)[-n_select:]\n",
        "            uncertain_indices_in_pool = remaining_indices[uncertain_indices_in_remaining]\n",
        "\n",
        "\n",
        "            # Query these points\n",
        "            new_X = query_pool[uncertain_indices_in_pool]\n",
        "            new_y = target_model.predict(new_X)\n",
        "\n",
        "            # Add to training set\n",
        "            X_queries = np.vstack([X_queries, new_X])\n",
        "            y_queries = np.hstack([y_queries, new_y])\n",
        "\n",
        "            # Update the remaining_mask to exclude the just-queried points\n",
        "            remaining_mask[uncertain_indices_in_pool] = False\n",
        "\n",
        "\n",
        "            # Retrain attacker model\n",
        "            attacker_model.fit(X_queries, y_queries)\n",
        "\n",
        "        # Visualize the current state\n",
        "        xx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
        "        Z = attacker_model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "        ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
        "\n",
        "        # Plot all queried points up to this round\n",
        "        # Separate initial points and uncertainty points for distinct markers\n",
        "        ax.scatter(X_queries[:n_initial, 0], X_queries[:n_initial, 1],\n",
        "                  c=y_queries[:n_initial], cmap='RdBu', s=50,\n",
        "                  edgecolor='black', label='Initial random')\n",
        "\n",
        "        # Find indices of uncertainty points\n",
        "        uncertainty_indices = np.where(~remaining_mask)[0] # Indices in the original query_pool that are NOT remaining\n",
        "        uncertainty_X = query_pool[uncertainty_indices]\n",
        "        uncertainty_y = target_model.predict(uncertainty_X) # Re-predict labels just for plotting consistency\n",
        "\n",
        "\n",
        "        ax.scatter(uncertainty_X[n_initial:, 0], uncertainty_X[n_initial:, 1],\n",
        "                  c=uncertainty_y[n_initial:], cmap='RdBu', s=100,\n",
        "                  marker='*', edgecolor='yellow', linewidth=2,\n",
        "                  label='Uncertainty queries')\n",
        "\n",
        "\n",
        "        ax.set_title(f'Round {round_idx + 1}: {len(X_queries)} total queries')\n",
        "        ax.legend()\n",
        "\n",
        "    plt.suptitle('Uncertainty-Based Active Learning for Model Extraction', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n‚úÖ RESULT: Uncertainty sampling finds decision boundaries faster!\")\n",
        "    print(\"   Notice how the star markers (uncertainty queries) cluster near boundaries\")\n",
        "\n",
        "def demonstrate_confidence_exploitation():\n",
        "    \"\"\"Show how confidence scores reveal information\"\"\"\n",
        "\n",
        "    print(\"\\n\\nüéØ 2. EXPLOITING CONFIDENCE SCORES\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nHow do confidence scores help extract models?\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Create target model\n",
        "    X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n",
        "                              n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "    target_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    target_model.fit(X, y)\n",
        "\n",
        "    print(\"\\nüìä CONFIDENCE SCORES REVEAL:\")\n",
        "    print(\"1. Distance to decision boundary (middle confidence (50%) = near boundary)\")\n",
        "    print(\"2. Regions of training data density (high confidence = many training points)\")\n",
        "    print(\"3. Model uncertainty patterns\")\n",
        "\n",
        "    # Generate query grid\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50),\n",
        "                         np.linspace(y_min, y_max, 50))\n",
        "\n",
        "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "    # Get predictions and confidence\n",
        "    predictions = target_model.predict(grid_points)\n",
        "    confidences = target_model.predict_proba(grid_points).max(axis=1)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Plot 1: True model\n",
        "    ax = axes[0]\n",
        "    Z = predictions.reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolor='black', s=100)\n",
        "    ax.set_title('Target Model (Hidden Training Data)')\n",
        "\n",
        "    # Plot 2: Confidence scores\n",
        "    ax = axes[1]\n",
        "    C = confidences.reshape(xx.shape)\n",
        "    contour = ax.contourf(xx, yy, C, levels=20, cmap='viridis')\n",
        "    plt.colorbar(contour, ax=ax, label='Confidence')\n",
        "    ax.set_title('Confidence Scores Reveal Structure')\n",
        "\n",
        "    # Plot 3: Extracted information\n",
        "    ax = axes[2]\n",
        "\n",
        "    # Find low confidence regions (boundaries)\n",
        "    boundary_mask = confidences < 0.7\n",
        "    boundary_points = grid_points[boundary_mask]\n",
        "\n",
        "    # Find high confidence regions (likely training data nearby)\n",
        "    high_conf_mask = confidences > 0.95\n",
        "    high_conf_points = grid_points[high_conf_mask]\n",
        "\n",
        "    ax.scatter(boundary_points[:, 0], boundary_points[:, 1],\n",
        "              c='red', s=10, alpha=0.5, label='Low conf (boundaries)')\n",
        "    ax.scatter(high_conf_points[:, 0], high_conf_points[:, 1],\n",
        "              c='green', s=10, alpha=0.3, label='High conf (data regions)')\n",
        "    ax.set_title('Extracted Information from Confidence')\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nüí° EXTRACTION STRATEGY:\")\n",
        "    print(\"1. Query many points and collect confidence scores\")\n",
        "    print(\"2. Low confidence (~50%) -> Near decision boundary\")\n",
        "    print(\"3. Use this to guide where to sample more densely\")\n",
        "    print(\"4. Build attacker model that matches these confidence patterns\")\n",
        "\n",
        "def explain_distillation_correctly():\n",
        "\n",
        "    print(\"\\n1. MODEL DISTILLATION for extraction:\")\n",
        "    print(\"   - Query target model with your own unlabeled data\")\n",
        "    print(\"   - Use target's predictions as labels\")\n",
        "    print(\"   - Train a 'student' model on (your_data, target_predictions)\")\n",
        "    print(\"   - No access to original training data needed!\")\n",
        "\n",
        "    # Simple distillation example\n",
        "    print(\"\\n\\nüìä DISTILLATION EXAMPLE:\")\n",
        "\n",
        "    # Original model - FIX: Added n_informative and n_redundant parameters\n",
        "    X_orig, y_orig = make_classification(n_samples=200, n_features=2, n_informative=2,\n",
        "                                       n_redundant=0, random_state=42)\n",
        "    teacher = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    teacher.fit(X_orig, y_orig)\n",
        "\n",
        "    # Attacker's data (different distribution!)\n",
        "    X_attack = np.random.uniform(-3, 3, size=(500, 2))\n",
        "    y_distilled = teacher.predict(X_attack)\n",
        "\n",
        "    # Train student\n",
        "    student = MLPClassifier(hidden_layer_sizes=(50,), random_state=42)\n",
        "    student.fit(X_attack, y_distilled)\n",
        "\n",
        "    # Compare\n",
        "    test_points = np.random.uniform(-3, 3, size=(1000, 2))\n",
        "    teacher_pred = teacher.predict(test_points)\n",
        "    student_pred = student.predict(test_points)\n",
        "\n",
        "    agreement = np.mean(teacher_pred == student_pred)\n",
        "    print(f\"\\nDistillation Result: {agreement:.1%} prediction agreement\")\n",
        "    print(\"Without seeing ANY original training data!\")\n",
        "\n",
        "def show_practical_defenses():\n",
        "    \"\"\"Show realistic defenses against these attacks\"\"\"\n",
        "\n",
        "    print(\"\\n\\nüõ°Ô∏è 4. PRACTICAL DEFENSES\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(\"\\n1. RATE LIMITING:\")\n",
        "    print(\"   - Limit queries per user/IP\")\n",
        "    print(\"   - Exponential backoff for suspicious patterns\")\n",
        "    print(\"   - Example: 1000 queries/day, 100/hour\")\n",
        "\n",
        "    print(\"\\n2. CONFIDENCE SCORE DEFENSES:\")\n",
        "    print(\"   - Add noise to confidence scores\")\n",
        "    print(\"   - Quantize to coarse levels (e.g., 'low', 'medium', 'high')\")\n",
        "    print(\"   - - Or don't return confidence at all!\")\n",
        "\n",
        "    print(\"\\n3. PREDICTION DEFENSES:\")\n",
        "    print(\"   - Add small random noise to predictions near boundaries\")\n",
        "    print(\"   - Temperature scaling to smooth probabilities\")\n",
        "    print(\"   - Return only top-k classes, not full distribution\")\n",
        "\n",
        "    print(\"\\n4. MONITORING:\")\n",
        "    print(\"   - Detect systematic boundary probing\")\n",
        "    print(\"   - Flag users querying with random-looking data\")\n",
        "    print(\"   - Alert on high-volume, low-entropy query patterns\")\n",
        "\n",
        "    # Show confidence perturbation\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    # Original confidence\n",
        "    x = np.linspace(0, 1, 100)\n",
        "    conf_orig = x\n",
        "\n",
        "    ax = axes[0]\n",
        "    ax.plot(x, conf_orig, 'b-', linewidth=2)\n",
        "    ax.set_xlabel('True Confidence')\n",
        "    ax.set_ylabel('Returned Confidence')\n",
        "    ax.set_title('Undefended: Exact Confidence')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Defended confidence\n",
        "    ax = axes[1]\n",
        "\n",
        "    # Add noise\n",
        "    conf_noisy = np.clip(conf_orig + np.random.normal(0, 0.05, 100), 0, 1)\n",
        "\n",
        "    # Quantize\n",
        "    conf_quantized = np.round(conf_orig * 3) / 3\n",
        "\n",
        "    ax.plot(x, conf_noisy, 'r-', alpha=0.5, label='With noise')\n",
        "    ax.plot(x, conf_quantized, 'g-', linewidth=2, label='Quantized')\n",
        "    ax.set_xlabel('True Confidence')\n",
        "    ax.set_ylabel('Returned Confidence')\n",
        "    ax.set_title('Defended: Perturbed Confidence')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run all demonstrations\n",
        "if __name__ == \"__main__\":\n",
        "    demonstrate_uncertainty_sampling()\n",
        "    demonstrate_confidence_exploitation()\n",
        "    explain_distillation_correctly()\n",
        "    show_practical_defenses()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üí° KEY TAKEAWAYS\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\n1. Uncertainty sampling: Query where your model is least confident\")\n",
        "    print(\"2. Confidence exploitation: Use scores to find boundaries & data regions\")\n",
        "    print(\"3. Distillation: Train on your data + target's predictions\")\n",
        "    print(\"4. Defenses: Rate limits + confidence perturbation + monitoring\")"
      ],
      "metadata": {
        "id": "bCyenB9X5Z0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attack4_distance_manipulation"
      },
      "source": [
        "### Attack 4: Distance Manipulation Attack üìè\n",
        "\n",
        "By carefully crafting inputs, attackers can exploit how kNN measures distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "distance_manipulation_demo"
      },
      "outputs": [],
      "source": [
        "# üìê Distance Manipulation Attack\n",
        "def demonstrate_distance_manipulation():\n",
        "    # Import needed for legend\n",
        "    from matplotlib.lines import Line2D\n",
        "\n",
        "    print(\"üéì LEARNING OBJECTIVE: Discover how different distance metrics create different vulnerabilities\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"You'll learn that the SAME perturbation can have DIFFERENT effects depending on the metric!\")\n",
        "    print(\"\\nüìä DATA PATTERN: Spread-out clusters with axis-aligned and diagonal points\")\n",
        "    print(\"This layout helps reveal how each metric 'sees' distance differently!\")\n",
        "    print(\"\\nüîç VISUALIZATION GUIDE:\")\n",
        "    print(\"‚Ä¢ Green dashed shapes show the neighborhood radius to the 5th nearest neighbor\")\n",
        "    print(\"‚Ä¢ Circle (Euclidean), Diamond (Manhattan), Square (Chebyshev)\")\n",
        "    print(\"‚Ä¢ Orange circles show the actual 5 nearest neighbors for each metric\")\n",
        "    print(\"‚Ä¢ Selected metric shows neighbor numbers (1=closest, 5=farthest)\")\n",
        "    print(\"‚Ä¢ Notice how different metrics select DIFFERENT neighbors from the same data!\")\n",
        "    print(\"‚Ä¢ Gray background highlights your currently selected metric\\n\")\n",
        "\n",
        "    # Generate training data with clear patterns that show metric differences\n",
        "    from sklearn.datasets import make_blobs\n",
        "\n",
        "    # Create two well-separated clusters with some spread\n",
        "    X_train, y_train = make_blobs(n_samples=100, centers=[[-2.5, 0], [2.5, 0]],\n",
        "                                 cluster_std=1.5, random_state=42)\n",
        "\n",
        "    # Add some points along axes and diagonals to show metric differences better\n",
        "    extra_points_class0 = np.array([\n",
        "        [-4, 0], [-3, 0], [0, -3], [0, -2],  # axis-aligned\n",
        "        [-3, -3], [-2, -2],  # diagonal\n",
        "    ])\n",
        "    extra_points_class1 = np.array([\n",
        "        [4, 0], [3, 0], [0, 3], [0, 2],  # axis-aligned\n",
        "        [3, 3], [2, 2],  # diagonal\n",
        "    ])\n",
        "\n",
        "    X_train = np.vstack([X_train, extra_points_class0, extra_points_class1])\n",
        "    y_train = np.hstack([y_train, np.zeros(len(extra_points_class0)),\n",
        "                        np.ones(len(extra_points_class1))]).astype(int)\n",
        "\n",
        "    # Create models with different metrics\n",
        "    metrics = ['euclidean', 'manhattan', 'chebyshev']\n",
        "    models = {metric: KNeighborsClassifier(n_neighbors=5, metric=metric).fit(X_train, y_train)\n",
        "              for metric in metrics}\n",
        "\n",
        "    # Target point for attack - placed in the middle where metrics differ most\n",
        "    target_point = np.array([[0.0, 0.0]])\n",
        "\n",
        "    # Interactive attack demonstration\n",
        "    def update_attack(perturbation_x, perturbation_y, metric_choice):\n",
        "        clear_output(wait=True)\n",
        "        plt.close('all')  # Add this line to properly close previous plots\n",
        "\n",
        "        # Create adversarial example\n",
        "        adversarial_point = target_point + np.array([[perturbation_x, perturbation_y]])\n",
        "\n",
        "        # Setup figure\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "        for idx, (ax, metric) in enumerate(zip(axes, metrics)):\n",
        "            model = models[metric]\n",
        "\n",
        "            # Create decision boundary with wider view\n",
        "            x_min, x_max = -5, 5\n",
        "            y_min, y_max = -5, 5\n",
        "            xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                                np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "            Z = Z.reshape(xx.shape)\n",
        "\n",
        "            # Plot\n",
        "            ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "            ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train,\n",
        "                      cmap='RdYlBu', s=50, alpha=0.6)\n",
        "\n",
        "            # Original prediction\n",
        "            orig_pred = model.predict(target_point)[0]\n",
        "            ax.scatter(target_point[0, 0], target_point[0, 1],\n",
        "                      c='green', s=200, marker='o', edgecolor='black',\n",
        "                      linewidth=2, label=f'Original (Class {orig_pred})')\n",
        "\n",
        "            # Adversarial prediction\n",
        "            adv_pred = model.predict(adversarial_point)[0]\n",
        "            ax.scatter(adversarial_point[0, 0], adversarial_point[0, 1],\n",
        "                      c='red', s=200, marker='*', edgecolor='black',\n",
        "                      linewidth=2, label=f'Adversarial (Class {adv_pred})')\n",
        "\n",
        "            # Draw perturbation arrow\n",
        "            if perturbation_x != 0 or perturbation_y != 0:  # Only draw arrow if there's a perturbation\n",
        "                ax.arrow(target_point[0, 0], target_point[0, 1],\n",
        "                        perturbation_x, perturbation_y,\n",
        "                        head_width=0.1, head_length=0.05,\n",
        "                        fc='black', ec='black', alpha=0.7)\n",
        "\n",
        "            # Always show neighbors for all metrics (not just selected one)\n",
        "            distances, indices = model.kneighbors(adversarial_point, n_neighbors=5)\n",
        "            neighbors = X_train[indices[0]]\n",
        "\n",
        "            # Highlight if metric matches selection\n",
        "            if metric == metric_choice:\n",
        "                ax.set_facecolor('#f0f0f0')\n",
        "                # Use thicker orange circles for selected metric\n",
        "                ax.scatter(neighbors[:, 0], neighbors[:, 1],\n",
        "                          s=150, facecolors='none', edgecolors='orange', linewidth=3,\n",
        "                          label=f'k=5 neighbors')\n",
        "                # Add numbers to neighbors in selected metric\n",
        "                for i, (nx, ny) in enumerate(neighbors):\n",
        "                    ax.annotate(str(i+1), (nx, ny), fontsize=8, ha='center', va='center',\n",
        "                               weight='bold', color='darkorange')\n",
        "            else:\n",
        "                # Use thinner orange circles for non-selected metrics\n",
        "                ax.scatter(neighbors[:, 0], neighbors[:, 1],\n",
        "                          s=150, facecolors='none', edgecolors='orange', linewidth=1.5,\n",
        "                          alpha=0.6, label=f'k=5 neighbors')\n",
        "\n",
        "            # Draw the distance neighborhood shape around the adversarial point\n",
        "            # Get distance to kth neighbor (k=5 in this case)\n",
        "            distances, _ = model.kneighbors(adversarial_point, n_neighbors=5)\n",
        "            radius = distances[0][-1]  # Distance to 5th nearest neighbor\n",
        "\n",
        "            center_x, center_y = adversarial_point[0, 0], adversarial_point[0, 1]\n",
        "\n",
        "            if metric == 'euclidean':\n",
        "                # Draw circle\n",
        "                circle = plt.Circle((center_x, center_y), radius,\n",
        "                                  fill=False, edgecolor='darkgreen', linewidth=2,\n",
        "                                  linestyle='--', alpha=0.7)\n",
        "                ax.add_patch(circle)\n",
        "\n",
        "            elif metric == 'manhattan':\n",
        "                # Draw diamond (rotated square)\n",
        "                diamond = plt.Polygon([\n",
        "                    (center_x + radius, center_y),      # Right\n",
        "                    (center_x, center_y + radius),      # Top\n",
        "                    (center_x - radius, center_y),      # Left\n",
        "                    (center_x, center_y - radius)       # Bottom\n",
        "                ], fill=False, edgecolor='darkgreen', linewidth=2,\n",
        "                   linestyle='--', alpha=0.7)\n",
        "                ax.add_patch(diamond)\n",
        "\n",
        "            else:  # chebyshev\n",
        "                # Draw square\n",
        "                square = plt.Rectangle((center_x - radius, center_y - radius),\n",
        "                                     2 * radius, 2 * radius,\n",
        "                                     fill=False, edgecolor='darkgreen', linewidth=2,\n",
        "                                     linestyle='--', alpha=0.7)\n",
        "                ax.add_patch(square)\n",
        "\n",
        "            # Calculate perturbation size\n",
        "            if metric == 'euclidean':\n",
        "                pert_size = np.sqrt(perturbation_x**2 + perturbation_y**2)\n",
        "            elif metric == 'manhattan':\n",
        "                pert_size = abs(perturbation_x) + abs(perturbation_y)\n",
        "            else:  # chebyshev\n",
        "                pert_size = max(abs(perturbation_x), abs(perturbation_y))\n",
        "\n",
        "            ax.set_title(f'{metric.capitalize()} Distance\\nPerturbation: {pert_size:.2f}',\n",
        "                        fontsize=14)\n",
        "\n",
        "            # Update legend to include shape explanation\n",
        "            handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "            # Add custom legend entry for the shape\n",
        "            if metric == 'euclidean':\n",
        "                shape_label = 'Circle (radius to 5th neighbor)'\n",
        "            elif metric == 'manhattan':\n",
        "                shape_label = 'Diamond (radius to 5th neighbor)'\n",
        "            else:\n",
        "                shape_label = 'Square (radius to 5th neighbor)'\n",
        "\n",
        "            shape_line = Line2D([0], [0], color='darkgreen', linewidth=2,\n",
        "                              linestyle='--', label=shape_label)\n",
        "\n",
        "            # Only add shape line if not already in legend\n",
        "            if shape_label not in labels:\n",
        "                handles.append(shape_line)\n",
        "                labels.append(shape_label)\n",
        "\n",
        "            ax.legend(handles, labels, loc='upper right')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            ax.set_aspect('equal')\n",
        "            ax.set_xlim(x_min, x_max)\n",
        "            ax.set_ylim(y_min, y_max)\n",
        "\n",
        "        plt.suptitle('üéØ Distance Manipulation Attack: Same perturbation, different impacts!',\n",
        "                    fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Analysis\n",
        "        print(\"\\nüìä Attack Analysis:\")\n",
        "        print(f\"Perturbation vector: ({perturbation_x:.2f}, {perturbation_y:.2f})\")\n",
        "        print(\"\\nImpact by metric:\")\n",
        "        for metric in metrics:\n",
        "            orig = models[metric].predict(target_point)[0]\n",
        "            adv = models[metric].predict(adversarial_point)[0]\n",
        "            changed = orig != adv\n",
        "\n",
        "            # Calculate perturbation size for this metric\n",
        "            if metric == 'euclidean':\n",
        "                pert_size = np.sqrt(perturbation_x**2 + perturbation_y**2)\n",
        "            elif metric == 'manhattan':\n",
        "                pert_size = abs(perturbation_x) + abs(perturbation_y)\n",
        "            else:  # chebyshev\n",
        "                pert_size = max(abs(perturbation_x), abs(perturbation_y))\n",
        "\n",
        "            status = '‚úì Changed!' if changed else '‚úó No change'\n",
        "            print(f\"  {metric:10s}: Class {orig} ‚Üí Class {adv} {status} (distance: {pert_size:.2f})\")\n",
        "\n",
        "        # Provide insights based on the current perturbation\n",
        "        print(\"\\nüí° Insight:\")\n",
        "        if abs(perturbation_x) > 0 and abs(perturbation_y) == 0:\n",
        "            print(\"‚Üí You're doing a horizontal attack! Notice Manhattan distance = |X|\")\n",
        "        elif abs(perturbation_x) == 0 and abs(perturbation_y) > 0:\n",
        "            print(\"‚Üí You're doing a vertical attack! Notice Manhattan distance = |Y|\")\n",
        "        elif abs(perturbation_x) == abs(perturbation_y) and perturbation_x != 0:\n",
        "            print(\"‚Üí Perfect diagonal! Compare Euclidean vs Manhattan distances\")\n",
        "        elif max(abs(perturbation_x), abs(perturbation_y)) > 0:\n",
        "            if abs(perturbation_x) == max(abs(perturbation_x), abs(perturbation_y)):\n",
        "                print(f\"‚Üí Chebyshev distance = {max(abs(perturbation_x), abs(perturbation_y)):.2f} (determined by X)\")\n",
        "            else:\n",
        "                print(f\"‚Üí Chebyshev distance = {max(abs(perturbation_x), abs(perturbation_y)):.2f} (determined by Y)\")\n",
        "\n",
        "        # Check for successful minimal attacks\n",
        "        successful_attacks = []\n",
        "        for metric in metrics:\n",
        "            orig = models[metric].predict(target_point)[0]\n",
        "            adv = models[metric].predict(adversarial_point)[0]\n",
        "            if orig != adv:\n",
        "                successful_attacks.append(metric)\n",
        "\n",
        "        if len(successful_attacks) == 1:\n",
        "            print(f\"\\nüéØ Nice! You found an attack that ONLY works on {successful_attacks[0]}!\")\n",
        "        elif len(successful_attacks) > 1:\n",
        "            print(f\"\\nüéØ This attack works on: {', '.join(successful_attacks)}\")\n",
        "\n",
        "        # Add neighbor comparison insight\n",
        "        print(\"\\nüîç Compare the orange-circled neighbors across metrics:\")\n",
        "        print(\"   ‚Ä¢ Are they the same 5 points in each metric? (Usually not!)\")\n",
        "        print(\"   ‚Ä¢ Count red vs blue neighbors - this determines the classification\")\n",
        "        print(\"   ‚Ä¢ The green shape shows WHY different neighbors are selected\")\n",
        "\n",
        "    # Create interactive controls\n",
        "    perturbation_x_slider = widgets.FloatSlider(\n",
        "        value=0.0, min=-3.0, max=3.0, step=0.1,\n",
        "        description='Perturb X:', continuous_update=False\n",
        "    )\n",
        "    perturbation_y_slider = widgets.FloatSlider(\n",
        "        value=0.0, min=-3.0, max=3.0, step=0.1,\n",
        "        description='Perturb Y:', continuous_update=False\n",
        "    )\n",
        "    metric_dropdown = widgets.Dropdown(\n",
        "        options=metrics, value='euclidean',\n",
        "        description='Focus metric:'\n",
        "    )\n",
        "\n",
        "    print(\"üéÆ Use the sliders to create adversarial perturbations!\")\n",
        "    print(\"Notice how the same perturbation affects different distance metrics differently.\\n\")\n",
        "\n",
        "    print(\"üìö EXPERIMENT GUIDE - Try these to discover key insights:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(\"1. üéØ Axis-aligned attacks (Manhattan's weakness):\")\n",
        "    print(\"   ‚Ä¢ Set X=2.0, Y=0.0 (horizontal only)\")\n",
        "    print(\"   ‚Ä¢ Set X=0.0, Y=2.0 (vertical only)\")\n",
        "    print(\"   ‚Ä¢ Look at which training points get selected as neighbors\")\n",
        "    print(\"   ‚Üí Discovery: Manhattan favors axis-aligned points!\\n\")\n",
        "\n",
        "    print(\"2. üéØ Diagonal attacks (Compare all three):\")\n",
        "    print(\"   ‚Ä¢ Set X=1.5, Y=1.5 (perfect diagonal)\")\n",
        "    print(\"   ‚Ä¢ Watch how each metric selects different diagonal vs axis-aligned points\")\n",
        "    print(\"   ‚Üí Discovery: Euclidean treats diagonals naturally, Manhattan 'thinks' in blocks!\\n\")\n",
        "\n",
        "    print(\"3. üéØ Chebyshev's square behavior:\")\n",
        "    print(\"   ‚Ä¢ Try X=1.0, Y=0.5 then X=1.0, Y=1.0\")\n",
        "    print(\"   ‚Ä¢ Notice when Chebyshev distance changes (only when max coordinate exceeds 1.0)\")\n",
        "    print(\"   ‚Üí Discovery: Chebyshev creates square neighborhoods - great for chess kings!\\n\")\n",
        "\n",
        "    print(\"4. üéØ Find metric-specific vulnerabilities:\")\n",
        "    print(\"   ‚Ä¢ Try X=0.7, Y=0.7 vs X=1.0, Y=0.0\")\n",
        "    print(\"   ‚Ä¢ These have same Manhattan distance (1.4) but different Euclidean!\")\n",
        "    print(\"   ‚Üí Discovery: Same 'cost' in one metric ‚â† same cost in another!\\n\")\n",
        "\n",
        "    print(\"üí° CHALLENGE: Can you change the classification with minimal perturbation for each metric?\")\n",
        "    print(\"\\nüìå KEY OBSERVATION: Look at the orange circles - they show which 5 training points vote!\")\n",
        "    print(\"   If a metric selects 3 blue + 2 red neighbors ‚Üí Blue wins\")\n",
        "    print(\"   If a metric selects 2 blue + 3 red neighbors ‚Üí Red wins\")\n",
        "    print(\"   Different shapes = different neighbors = different votes!\")\n",
        "\n",
        "    interact = widgets.interactive(update_attack,\n",
        "                                  perturbation_x=perturbation_x_slider,\n",
        "                                  perturbation_y=perturbation_y_slider,\n",
        "                                  metric_choice=metric_dropdown)\n",
        "    display(interact)\n",
        "\n",
        "demonstrate_distance_manipulation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "defense2_ensemble"
      },
      "source": [
        "### Defense 1: Ensemble Methods ü§ù"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ensemble_defense_demo"
      },
      "outputs": [],
      "source": [
        "# ü§ù Ensemble Defense\n",
        "class EnsembleKNN:\n",
        "    def __init__(self, k_values=[3, 5, 7, 9], aggregation='weighted'):\n",
        "        self.k_values = k_values\n",
        "        self.aggregation = aggregation\n",
        "        self.models = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for k in self.k_values:\n",
        "            self.models[k] = KNeighborsClassifier(n_neighbors=k)\n",
        "            self.models[k].fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        weights = []\n",
        "\n",
        "        for k, model in self.models.items():\n",
        "            pred = model.predict(X)\n",
        "            predictions.append(pred)\n",
        "\n",
        "            # Weight by k (larger k = more stable)\n",
        "            weights.append(np.sqrt(k))\n",
        "\n",
        "        predictions = np.array(predictions)\n",
        "        weights = np.array(weights) / np.sum(weights)\n",
        "\n",
        "        if self.aggregation == 'weighted':\n",
        "            # Weighted voting\n",
        "            weighted_pred = np.average(predictions, axis=0, weights=weights)\n",
        "            return np.round(weighted_pred).astype(int)\n",
        "        else:\n",
        "            # Simple majority\n",
        "            return np.round(np.mean(predictions, axis=0)).astype(int)\n",
        "\n",
        "# Interactive ensemble demonstration\n",
        "def demonstrate_ensemble_defense():\n",
        "    # Generate data with outliers\n",
        "    X, y = make_moons(n_samples=150, noise=0.2, random_state=42)\n",
        "\n",
        "    # Add some outliers\n",
        "    n_outliers = 10\n",
        "    outlier_X = np.random.uniform(-3, 3, (n_outliers, 2))\n",
        "    outlier_y = np.random.randint(0, 2, n_outliers)\n",
        "\n",
        "    X = np.vstack([X, outlier_X])\n",
        "    y = np.hstack([y, outlier_y])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "    # Train models\n",
        "    single_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    single_knn.fit(X_train, y_train)\n",
        "\n",
        "    ensemble_knn = EnsembleKNN(k_values=[3, 5, 7, 9, 11])\n",
        "    ensemble_knn.fit(X_train, y_train)\n",
        "\n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Single model\n",
        "    ax = axes[0]\n",
        "    visualize_model_robustness(ax, single_knn, X_train, y_train, X_test, y_test,\n",
        "                              \"Single kNN (k=5)\")\n",
        "\n",
        "    # Ensemble model\n",
        "    ax = axes[1]\n",
        "    visualize_model_robustness(ax, ensemble_knn, X_train, y_train, X_test, y_test,\n",
        "                              \"Ensemble kNN (k=[3,5,7,9,11])\")\n",
        "\n",
        "    plt.suptitle('ü§ù Ensemble Defense: Improved Robustness', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nüõ°Ô∏è Ensemble Benefits:\")\n",
        "    print(\"- Reduces impact of outliers\")\n",
        "    print(\"- More stable decision boundaries\")\n",
        "    print(\"- Harder to attack (need to fool multiple models)\")\n",
        "\n",
        "def visualize_model_robustness(ax, model, X_train, y_train, X_test, y_test, title):\n",
        "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
        "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                         np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
        "\n",
        "    # Identify outliers (points far from others)\n",
        "    from sklearn.neighbors import LocalOutlierFactor\n",
        "    lof = LocalOutlierFactor(n_neighbors=10)\n",
        "    outlier_labels = lof.fit_predict(X_train)\n",
        "\n",
        "    # Plot normal points\n",
        "    normal_mask = outlier_labels == 1\n",
        "    ax.scatter(X_train[normal_mask, 0], X_train[normal_mask, 1],\n",
        "              c=y_train[normal_mask], cmap='RdYlBu', s=50, alpha=0.6)\n",
        "\n",
        "    # Highlight outliers\n",
        "    outlier_mask = outlier_labels == -1\n",
        "    ax.scatter(X_train[outlier_mask, 0], X_train[outlier_mask, 1],\n",
        "              c=y_train[outlier_mask], cmap='RdYlBu', s=100,\n",
        "              marker='D', edgecolor='red', linewidth=2)\n",
        "\n",
        "    acc = accuracy_score(y_test, model.predict(X_test))\n",
        "    ax.set_title(f'{title}\\nTest Accuracy: {acc:.2%}', fontsize=14)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "demonstrate_ensemble_defense()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "defense3_outlier_detection"
      },
      "source": [
        "### Defense 2: Outlier Detection and Data Sanitization üßπ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "outlier_detection_demo"
      },
      "outputs": [],
      "source": [
        "# üßπ Data Sanitization Defense\n",
        "def demonstrate_outlier_detection():\n",
        "    # Generate clean data\n",
        "    X_clean, y_clean = make_moons(n_samples=100, noise=0.1, random_state=42)\n",
        "\n",
        "    # Add poisoned data\n",
        "    n_poison = 20\n",
        "    X_poison = np.random.uniform(-2, 2, (n_poison, 2))\n",
        "    y_poison = np.random.randint(0, 2, n_poison)\n",
        "\n",
        "    # Combine\n",
        "    X_all = np.vstack([X_clean, X_poison])\n",
        "    y_all = np.hstack([y_clean, y_poison])\n",
        "    is_poison = np.hstack([np.zeros(len(X_clean)), np.ones(len(X_poison))])\n",
        "\n",
        "    # Outlier detection methods\n",
        "    from sklearn.ensemble import IsolationForest\n",
        "    from sklearn.neighbors import LocalOutlierFactor\n",
        "    from sklearn.covariance import EllipticEnvelope\n",
        "\n",
        "    detectors = [\n",
        "        ('Local Outlier Factor', LocalOutlierFactor(contamination=0.15)),\n",
        "        ('Isolation Forest', IsolationForest(contamination=0.15, random_state=42)),\n",
        "        ('Elliptic Envelope', EllipticEnvelope(contamination=0.15, random_state=42))\n",
        "    ]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Original poisoned data\n",
        "    ax = axes[0]\n",
        "    ax.scatter(X_clean[:, 0], X_clean[:, 1], c=y_clean,\n",
        "              cmap='RdYlBu', s=50, alpha=0.6, label='Clean')\n",
        "    ax.scatter(X_poison[:, 0], X_poison[:, 1], c=y_poison,\n",
        "              cmap='RdYlBu', s=100, marker='X',\n",
        "              edgecolor='red', linewidth=2, label='Poison')\n",
        "    ax.set_title('Original Data (with Poison)', fontsize=14)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Apply detectors\n",
        "    for idx, (name, detector) in enumerate(detectors):\n",
        "        ax = axes[idx + 1]\n",
        "\n",
        "        # Detect outliers\n",
        "        if name == 'Local Outlier Factor':\n",
        "            outlier_pred = detector.fit_predict(X_all)\n",
        "        else:\n",
        "            outlier_pred = detector.fit_predict(X_all)\n",
        "\n",
        "        # Calculate performance\n",
        "        detected_outliers = outlier_pred == -1\n",
        "        true_positives = np.sum(detected_outliers & (is_poison == 1))\n",
        "        false_positives = np.sum(detected_outliers & (is_poison == 0))\n",
        "\n",
        "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "        recall = true_positives / n_poison\n",
        "\n",
        "        # Visualize\n",
        "        clean_mask = outlier_pred == 1\n",
        "        outlier_mask = outlier_pred == -1\n",
        "\n",
        "        ax.scatter(X_all[clean_mask, 0], X_all[clean_mask, 1],\n",
        "                  c=y_all[clean_mask], cmap='RdYlBu', s=50, alpha=0.6)\n",
        "        ax.scatter(X_all[outlier_mask, 0], X_all[outlier_mask, 1],\n",
        "                  c='gray', s=100, marker='x', linewidth=2)\n",
        "\n",
        "        ax.set_title(f'{name}\\nPrecision: {precision:.2%}, Recall: {recall:.2%}',\n",
        "                    fontsize=14)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.suptitle('üßπ Outlier Detection for Data Sanitization', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nüõ°Ô∏è Data Sanitization Strategy:\")\n",
        "    print(\"1. Use multiple outlier detection methods\")\n",
        "    print(\"2. Combine their outputs (e.g., majority vote)\")\n",
        "    print(\"3. Remove detected outliers before training\")\n",
        "    print(\"4. Monitor for concept drift in production\")\n",
        "\n",
        "demonstrate_outlier_detection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part4_production"
      },
      "source": [
        "## Part 3: Production Security Considerations üè≠\n",
        "\n",
        "### Key Security Measures for Production kNN Systems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "production_security"
      },
      "outputs": [],
      "source": [
        "# üè≠ Production Security Checklist\n",
        "def production_security_summary():\n",
        "    checklist = [\n",
        "        {\"category\": \"Input Validation\",\n",
        "         \"measures\": [\n",
        "             \"Validate input dimensions and types\",\n",
        "             \"Check for out-of-range values\",\n",
        "             \"Implement rate limiting\",\n",
        "             \"Detect anomalous query patterns\"\n",
        "         ]},\n",
        "        {\"category\": \"Model Protection\",\n",
        "         \"measures\": [\n",
        "             \"Use differential privacy (Œµ < 1 for sensitive data)\",\n",
        "             \"Implement ensemble methods\",\n",
        "             \"Regular model retraining with sanitized data\",\n",
        "             \"Version control and rollback capabilities\"\n",
        "         ]},\n",
        "        {\"category\": \"Monitoring\",\n",
        "         \"measures\": [\n",
        "             \"Track prediction confidence distributions\",\n",
        "             \"Monitor for distribution drift\",\n",
        "             \"Log suspicious queries\",\n",
        "             \"Alert on accuracy degradation\"\n",
        "         ]},\n",
        "        {\"category\": \"Access Control\",\n",
        "         \"measures\": [\n",
        "             \"API authentication (OAuth 2.0)\",\n",
        "             \"Query rate limiting per user\",\n",
        "             \"Audit logging of all predictions\",\n",
        "             \"Encrypt model storage\"\n",
        "         ]}\n",
        "    ]\n",
        "\n",
        "    print(\"üè≠ Production Security Checklist for kNN Systems\\n\")\n",
        "\n",
        "    for item in checklist:\n",
        "        print(f\"\\nüìã {item['category']}:\")\n",
        "        for measure in item['measures']:\n",
        "            print(f\"   ‚úì {measure}\")\n",
        "\n",
        "    print(\"\\n\\nüí° Remember: Security is a continuous process, not a one-time setup!\")\n",
        "    print(\"\\nüîç Key Metrics to Monitor:\")\n",
        "    print(\"   ‚Ä¢ Query volume trends\")\n",
        "    print(\"   ‚Ä¢ Prediction confidence distribution\")\n",
        "    print(\"   ‚Ä¢ Feature distribution shifts\")\n",
        "    print(\"   ‚Ä¢ Model accuracy over time\")\n",
        "\n",
        "    # Sample monitoring dashboard\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Metric 1: Query volume\n",
        "    ax = axes[0, 0]\n",
        "    hours = np.arange(24)\n",
        "    normal_queries = 100 + 50 * np.sin(hours * np.pi / 12) + np.random.normal(0, 10, 24)\n",
        "    attack_queries = normal_queries.copy()\n",
        "    attack_queries[18:22] *= 3  # Spike during attack\n",
        "\n",
        "    ax.plot(hours, normal_queries, 'g-', label='Normal', linewidth=2)\n",
        "    ax.plot(hours, attack_queries, 'r--', label='Under Attack', linewidth=2)\n",
        "    ax.fill_between(hours[18:22], 0, attack_queries[18:22], alpha=0.3, color='red')\n",
        "    ax.set_xlabel('Hour of Day')\n",
        "    ax.set_ylabel('Queries per Hour')\n",
        "    ax.set_title('Query Volume Monitoring', fontsize=14)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Metric 2: Confidence distribution\n",
        "    ax = axes[0, 1]\n",
        "    normal_conf = np.random.beta(5, 2, 1000)\n",
        "    attack_conf = np.random.beta(2, 5, 1000)\n",
        "\n",
        "    ax.hist(normal_conf, bins=30, alpha=0.6, label='Normal', color='green')\n",
        "    ax.hist(attack_conf, bins=30, alpha=0.6, label='Under Attack', color='red')\n",
        "    ax.set_xlabel('Prediction Confidence')\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_title('Confidence Distribution', fontsize=14)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Metric 3: Feature drift - IMPROVED VERSION\n",
        "    ax = axes[1, 0]\n",
        "    days = np.arange(30)\n",
        "    feature_mean = 0.5 + 0.01 * days + 0.1 * np.sin(days * np.pi / 7)\n",
        "    feature_mean[20:] += 0.3  # Drift\n",
        "\n",
        "    # Plot with proper labels\n",
        "    ax.plot(days, feature_mean, 'b-', linewidth=2, label='Observed Feature Mean')\n",
        "    ax.axhline(y=0.5, color='gray', linestyle='--', label='Expected Mean (0.5)')\n",
        "    ax.fill_between(days[20:], 0.5, feature_mean[20:], alpha=0.3, color='red',\n",
        "                    label='Distribution Drift')\n",
        "\n",
        "    # Add vertical line to mark drift start\n",
        "    ax.axvline(x=20, color='red', linestyle=':', alpha=0.5, label='Drift Start (Day 20)')\n",
        "\n",
        "    # Add annotations\n",
        "    ax.annotate('Normal behavior', xy=(10, 0.52), xytext=(10, 0.35),\n",
        "                arrowprops=dict(arrowstyle='->', alpha=0.5),\n",
        "                fontsize=9, ha='center')\n",
        "\n",
        "    ax.annotate('Drift detected!', xy=(25, 0.85), xytext=(25, 1.0),\n",
        "                arrowprops=dict(arrowstyle='->', color='red', alpha=0.7),\n",
        "                fontsize=9, ha='center', color='red')\n",
        "\n",
        "    ax.set_xlabel('Days')\n",
        "    ax.set_ylabel('Feature Mean')\n",
        "    ax.set_title('Feature Distribution Drift', fontsize=14)\n",
        "    ax.legend(loc='upper left', fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Metric 4: Model accuracy\n",
        "    ax = axes[1, 1]\n",
        "    days = np.arange(30)\n",
        "    accuracy = 0.92 + np.random.normal(0, 0.01, 30)\n",
        "    accuracy[15:20] -= 0.05  # Attack period\n",
        "    accuracy[25:] -= 0.03  # Degradation\n",
        "\n",
        "    ax.plot(days, accuracy, 'ko-', linewidth=2, markersize=5)\n",
        "    ax.axhline(y=0.9, color='orange', linestyle='--', label='Warning Threshold')\n",
        "    ax.axhline(y=0.85, color='red', linestyle='--', label='Critical Threshold')\n",
        "    ax.fill_between(days, 0.85, accuracy, where=(accuracy < 0.9),\n",
        "                   alpha=0.3, color='orange')\n",
        "    ax.set_xlabel('Days')\n",
        "    ax.set_ylabel('Test Accuracy')\n",
        "    ax.set_title('Model Performance Tracking', fontsize=14)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_ylim(0.8, 0.95)\n",
        "\n",
        "    plt.suptitle('üìä Sample Production Monitoring Dashboard', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "production_security_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## üéì Lab Summary\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **kNN's Unique Vulnerabilities**:\n",
        "   - Stores all training data (privacy risk)\n",
        "   - Distance-based decisions (manipulation attacks)\n",
        "   - No model parameters (harder to detect tampering)\n",
        "\n",
        "2. **Attack Landscape**:\n",
        "   - **Label Flipping**: Target boundary points for maximum impact\n",
        "   - **Membership Inference**: Exploit distance to training data\n",
        "   - **Model Extraction**: Reconstruct training data through queries\n",
        "   - **Distance Manipulation**: Exploit different metrics\n",
        "   - **Byzantine Attacks**: Poison distributed systems\n",
        "\n",
        "3. **Defense Strategies**:\n",
        "   - **Differential Privacy**: Add noise for privacy (Œµ ‚âà 1)\n",
        "   - **Ensemble Methods**: Multiple k values for robustness\n",
        "   - **Data Sanitization**: Detect and remove outliers\n",
        "   - **Robust Distances**: Learn better metrics\n",
        "\n",
        "4. **Production Considerations**:\n",
        "   - Input validation and rate limiting\n",
        "   - Continuous monitoring for anomalies\n",
        "   - Regular retraining with clean data\n",
        "   - Comprehensive logging and alerting\n",
        "\n",
        "### üöÄ Next Steps\n",
        "\n",
        "1. **Experiment**: Try combining multiple defenses\n",
        "2. **Implement**: Add these defenses to your kNN systems\n",
        "3. **Monitor**: Set up production monitoring dashboards\n",
        "4. **Stay Updated**: Follow latest research on kNN security\n",
        "\n",
        "### üìö Further Reading\n",
        "\n",
        "- \"ASK: Adversarial Soft k-Nearest Neighbor Attack and Defense\" (NeurIPS 2022)\n",
        "- \"Certified Robustness of Nearest Neighbors\" (AAAI 2021)\n",
        "- \"Privacy-Preserving k-NN for Small and Large Data Sets\" (ICDM 2019)\n",
        "\n",
        "---\n",
        "\n",
        "**Remember**: Security is not about perfect protection, but about making attacks more expensive than they're worth! üõ°Ô∏è"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}