{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgracie52/bh-2025/blob/main/NeuralNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks and AI Security: A Comprehensive Lab\n",
        "\n",
        "## Welcome to the Neural Network Security Lab!\n",
        "\n",
        "This lab is designed to teach you both how neural networks work AND how they can be attacked and defended. We'll start with the basics and progressively build up to advanced security concepts.\n",
        "\n",
        "### What You'll Learn:\n",
        "1. **Neural Network Fundamentals** - How they actually work\n",
        "2. **Classic Attacks** - FGSM and basic adversarial examples\n",
        "3. **Advanced Attacks** - State-of-the-art techniques from 2024-2025\n",
        "4. **Privacy Attacks** - How models leak training data\n",
        "5. **Defense Mechanisms** - How to protect your models\n",
        "6. **Privacy-Preserving Techniques** - Differential privacy and more\n",
        "7. **Advanced Security Topics** - Backdoor detection and model watermarking\n",
        "\n",
        "### Prerequisites:\n",
        "- Basic Python knowledge\n",
        "- Understanding of basic ML concepts (we'll review the rest!)\n",
        "\n",
        "Let's get started! ðŸš€"
      ],
      "metadata": {
        "id": "intro-section"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0: Environment Setup\n",
        "\n",
        "First, let's install all the packages we'll need. This might take a few minutes.\n",
        "\n",
        "**What we're installing:**\n",
        "- `torch` & `torchvision`: PyTorch for building neural networks\n",
        "- `adversarial-robustness-toolbox`: IBM's toolkit for adversarial attacks\n",
        "- `matplotlib` & `seaborn`: For visualizations\n",
        "- Additional security-focused libraries"
      ],
      "metadata": {
        "id": "setup-section"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-dependencies"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio --quiet\n",
        "!pip install adversarial-robustness-toolbox --quiet\n",
        "!pip install matplotlib seaborn numpy scikit-learn --quiet\n",
        "!pip install ipywidgets --quiet\n",
        "!pip install opencv-python scikit-image scipy --quiet\n",
        "\n",
        "print(\"âœ… All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.datasets import make_classification, make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"âœ… Environment ready!\")"
      ],
      "metadata": {
        "id": "import-libraries"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Understanding Neural Networks\n",
        "\n",
        "### 1.1 What is a Neural Network?\n",
        "\n",
        "Think of a neural network as a **decision-making machine** that learns from examples. Just like how you learned to recognize cats vs. dogs by seeing many examples, neural networks learn patterns from data.\n",
        "\n",
        "**Key Components:**\n",
        "1. **Neurons**: Basic units that receive inputs and produce outputs\n",
        "2. **Weights**: The \"importance\" given to each connection\n",
        "3. **Activation Functions**: Add non-linearity (like ReLU, Sigmoid)\n",
        "4. **Layers**: Groups of neurons working together\n",
        "\n",
        "Let's build our first neural network!"
      ],
      "metadata": {
        "id": "nn-basics"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a simple neural network from scratch!\n",
        "\n",
        "class SimpleNeuralNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple 3-layer neural network for binary classification.\n",
        "\n",
        "    Architecture:\n",
        "    - Input layer: 2 features (x, y coordinates)\n",
        "    - Hidden layer: 10 neurons with ReLU activation\n",
        "    - Output layer: 2 neurons (for 2 classes)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size=2, hidden_size=10, output_size=2):\n",
        "        super(SimpleNeuralNetwork, self).__init__()\n",
        "        # Define layers\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)  # 2 inputs -> 10 hidden neurons\n",
        "        self.output = nn.Linear(hidden_size, output_size)  # 10 hidden -> 2 outputs\n",
        "        self.activation = nn.ReLU()  # ReLU activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass: how data flows through the network\n",
        "        \"\"\"\n",
        "        # Step 1: Input -> Hidden layer\n",
        "        x = self.hidden(x)\n",
        "\n",
        "        # Step 2: Apply activation function (ReLU)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        # Step 3: Hidden -> Output layer\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create an instance of our network\n",
        "model = SimpleNeuralNetwork()\n",
        "print(\"Neural Network Architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
      ],
      "metadata": {
        "id": "simple-nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Creating a Dataset\n",
        "\n",
        "Let's create a simple 2D dataset that our network will learn to classify. We'll use the \"two moons\" dataset - it creates two crescent moon shapes that are interleaved."
      ],
      "metadata": {
        "id": "dataset-creation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the \"two moons\" dataset\n",
        "X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "# Visualize the dataset\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], alpha=0.6, label='Class 0', c='blue')\n",
        "plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], alpha=0.6, label='Class 1', c='red')\n",
        "plt.title('Two Moons Dataset')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")"
      ],
      "metadata": {
        "id": "create-dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Training the Neural Network\n",
        "\n",
        "Now let's train our network! Training involves:\n",
        "1. **Forward Pass**: Feed data through the network\n",
        "2. **Calculate Loss**: Measure how wrong our predictions are\n",
        "3. **Backward Pass**: Calculate gradients (how to improve)\n",
        "4. **Update Weights**: Make the network a bit better\n",
        "\n",
        "We repeat this process many times (epochs) until the network learns!"
      ],
      "metadata": {
        "id": "training-explanation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function with detailed explanations\n",
        "def train_model(model, X_train, y_train, epochs=100, learning_rate=0.01):\n",
        "    \"\"\"\n",
        "    Train a neural network model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The neural network\n",
        "    - X_train, y_train: Training data\n",
        "    - epochs: Number of training iterations\n",
        "    - learning_rate: How big steps to take when updating weights\n",
        "    \"\"\"\n",
        "    # Define loss function (CrossEntropyLoss for classification)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Define optimizer (Adam is a popular choice)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Track losses for visualization\n",
        "    losses = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        outputs = model(X_train)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        loss.backward()        # Calculate gradients\n",
        "        optimizer.step()       # Update weights\n",
        "\n",
        "        # Store loss\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Print progress every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return losses\n",
        "\n",
        "# Train our model\n",
        "print(\"Training the neural network...\")\n",
        "losses = train_model(model, X_train_tensor, y_train_tensor)\n",
        "\n",
        "# Plot the training loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses)\n",
        "plt.title('Training Loss Over Time')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Training complete!\")"
      ],
      "metadata": {
        "id": "train-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Visualizing Decision Boundaries\n",
        "\n",
        "One of the coolest things about neural networks is that they learn complex decision boundaries. Let's visualize what our network learned!"
      ],
      "metadata": {
        "id": "decision-boundaries"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
        "    \"\"\"\n",
        "    Visualize the decision boundary learned by the neural network.\n",
        "\n",
        "    The background colors show what the network predicts for each point in space.\n",
        "    \"\"\"\n",
        "    # Create a mesh grid\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "\n",
        "    # Get predictions for every point in the mesh\n",
        "    grid_points = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])\n",
        "    with torch.no_grad():\n",
        "        Z = model(grid_points).argmax(dim=1).numpy()\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')\n",
        "    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', edgecolor='black', alpha=0.6)\n",
        "    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', edgecolor='black', alpha=0.6)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(title)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# Visualize what our network learned\n",
        "plot_decision_boundary(model, X_train, y_train, \"Neural Network Decision Boundary\")\n",
        "\n",
        "# Evaluate accuracy\n",
        "with torch.no_grad():\n",
        "    predictions = model(X_test_tensor).argmax(dim=1)\n",
        "    accuracy = (predictions == y_test_tensor).float().mean()\n",
        "    print(f\"Test Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "visualize-boundaries"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Understanding Backpropagation\n",
        "\n",
        "### How Neural Networks Learn: The Backpropagation Algorithm\n",
        "\n",
        "Backpropagation is like teaching a student by showing them their mistakes and how to fix them. Here's the process:\n",
        "\n",
        "1. **Forward Pass**: Make a prediction\n",
        "2. **Calculate Error**: See how wrong we were\n",
        "3. **Backward Pass**: Figure out which weights contributed to the error\n",
        "4. **Update Weights**: Adjust weights to reduce error next time\n",
        "\n",
        "Let's implement this from scratch to really understand it!"
      ],
      "metadata": {
        "id": "backprop-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual implementation of backpropagation for educational purposes\n",
        "class ManualNeuralNetwork:\n",
        "    \"\"\"\n",
        "    A neural network implemented from scratch to demonstrate backpropagation.\n",
        "    This helps understand what PyTorch does automatically!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size=2, hidden_size=3, output_size=1):\n",
        "        # Initialize weights with small random values\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "        print(\"ðŸ§  Neural Network Architecture:\")\n",
        "        print(f\"Input Layer: {input_size} neurons\")\n",
        "        print(f\"Hidden Layer: {hidden_size} neurons\")\n",
        "        print(f\"Output Layer: {output_size} neurons\")\n",
        "        print(\"\\nInitial Weights:\")\n",
        "        print(f\"W1 shape: {self.W1.shape}\")\n",
        "        print(f\"W2 shape: {self.W2.shape}\")\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation function: squashes values between 0 and 1\"\"\"\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        \"\"\"Derivative of sigmoid: tells us how to adjust weights\"\"\"\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass: compute predictions\"\"\"\n",
        "        # Input -> Hidden\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "\n",
        "        # Hidden -> Output\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate=0.1):\n",
        "        \"\"\"Backward pass: calculate gradients and update weights\"\"\"\n",
        "        m = X.shape[0]  # Number of examples\n",
        "\n",
        "        # Calculate gradients for output layer\n",
        "        self.dz2 = output - y  # Error at output\n",
        "        self.dW2 = (1/m) * np.dot(self.a1.T, self.dz2)\n",
        "        self.db2 = (1/m) * np.sum(self.dz2, axis=0, keepdims=True)\n",
        "\n",
        "        # Calculate gradients for hidden layer\n",
        "        self.da1 = np.dot(self.dz2, self.W2.T)\n",
        "        self.dz1 = self.da1 * self.sigmoid_derivative(self.a1)\n",
        "        self.dW1 = (1/m) * np.dot(X.T, self.dz1)\n",
        "        self.db1 = (1/m) * np.sum(self.dz1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.W2 -= learning_rate * self.dW2\n",
        "        self.b2 -= learning_rate * self.db2\n",
        "        self.W1 -= learning_rate * self.dW1\n",
        "        self.b1 -= learning_rate * self.db1\n",
        "\n",
        "# Demonstrate backpropagation on a simple XOR problem\n",
        "print(\"ðŸŽ¯ Let's learn the XOR function!\")\n",
        "print(\"XOR Truth Table:\")\n",
        "print(\"0 XOR 0 = 0\")\n",
        "print(\"0 XOR 1 = 1\")\n",
        "print(\"1 XOR 0 = 1\")\n",
        "print(\"1 XOR 1 = 0\")\n",
        "print(\"\\nThis is impossible for a linear model but easy for a neural network!\\n\")\n",
        "\n",
        "# XOR dataset\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Create and train network\n",
        "nn = ManualNeuralNetwork(2, 4, 1)\n",
        "\n",
        "# Training loop with visualization\n",
        "losses = []\n",
        "for epoch in range(5000):\n",
        "    # Forward pass\n",
        "    output = nn.forward(X_xor)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = np.mean((output - y_xor)**2)\n",
        "    losses.append(loss)\n",
        "\n",
        "    # Backward pass\n",
        "    nn.backward(X_xor, y_xor, output)\n",
        "\n",
        "    # Print progress\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Test the trained network\n",
        "print(\"\\nðŸŽ‰ Testing our trained network:\")\n",
        "for i in range(len(X_xor)):\n",
        "    prediction = nn.forward(X_xor[i:i+1])\n",
        "    print(f\"Input: {X_xor[i]} â†’ Prediction: {prediction[0,0]:.4f} â†’ Rounded: {int(prediction[0,0] > 0.5)}\")\n",
        "\n",
        "# Plot learning curve\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses)\n",
        "plt.title('Learning the XOR Function')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "manual-backprop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Introduction to Adversarial Examples\n",
        "\n",
        "### ðŸš¨ Security Alert: Neural Networks Can Be Fooled!\n",
        "\n",
        "Adversarial examples are inputs designed to trick neural networks. It's like optical illusions for AI - small, often invisible changes that completely fool the model.\n",
        "\n",
        "**Why This Matters:**\n",
        "- Self-driving cars could misread stop signs\n",
        "- Medical diagnosis systems could be manipulated\n",
        "- Security systems could be bypassed\n",
        "\n",
        "Let's see how this works!"
      ],
      "metadata": {
        "id": "adversarial-intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset for adversarial examples\n",
        "print(\"Loading MNIST dataset...\")\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load training data\n",
        "trainset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Load test data\n",
        "testset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "testloader = DataLoader(testset, batch_size=1, shuffle=True)\n",
        "\n",
        "print(\"âœ… MNIST dataset loaded!\")\n",
        "print(f\"Training samples: {len(trainset)}\")\n",
        "print(f\"Test samples: {len(testset)}\")\n",
        "\n",
        "# Show some examples\n",
        "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
        "for i, (image, label) in enumerate(trainloader):\n",
        "    if i >= 5:\n",
        "        break\n",
        "    axes[i].imshow(image[0].squeeze(), cmap='gray')\n",
        "    axes[i].set_title(f'Label: {label[0].item()}')\n",
        "    axes[i].axis('off')\n",
        "plt.suptitle('Sample MNIST Images')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "load-mnist"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Build a CNN for MNIST\n",
        "class MNISTClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A Convolutional Neural Network for classifying handwritten digits.\n",
        "\n",
        "    Architecture:\n",
        "    - Conv Layer 1: 32 filters, 3x3 kernel\n",
        "    - Conv Layer 2: 64 filters, 3x3 kernel\n",
        "    - Fully Connected: 128 neurons\n",
        "    - Output: 10 classes (digits 0-9)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MNISTClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create and train the model (simplified training for speed)\n",
        "mnist_model = MNISTClassifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mnist_model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Training MNIST classifier...\")\n",
        "mnist_model.train()\n",
        "for epoch in range(2):  # Just 2 epochs for demonstration\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(trainloader):\n",
        "        if i > 100:  # Limit batches for speed\n",
        "            break\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mnist_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {running_loss/100:.3f}')\n",
        "\n",
        "print(\"âœ… Model trained!\")\n",
        "\n",
        "# Test accuracy\n",
        "mnist_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels) in enumerate(testloader):\n",
        "        if i > 100:\n",
        "            break\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = mnist_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "id": "mnist-cnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 FGSM Attack - Fast Gradient Sign Method\n",
        "\n",
        "FGSM is one of the first and simplest adversarial attacks. Here's how it works:\n",
        "\n",
        "1. Calculate how to change the input to increase the loss\n",
        "2. Take a small step in that direction\n",
        "3. The result looks almost identical but fools the model!\n",
        "\n",
        "**Mathematical Formula:**\n",
        "```\n",
        "adversarial_image = original_image + Îµ Ã— sign(gradient)\n",
        "```\n",
        "\n",
        "Where:\n",
        "- Îµ (epsilon) = how much to change the image\n",
        "- gradient = direction that increases the loss"
      ],
      "metadata": {
        "id": "fgsm-explanation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    \"\"\"\n",
        "    FGSM Attack: Create adversarial example\n",
        "\n",
        "    Args:\n",
        "        image: Original input image\n",
        "        epsilon: Maximum perturbation allowed\n",
        "        data_grad: Gradient of loss w.r.t input\n",
        "\n",
        "    Returns:\n",
        "        Perturbed image that fools the model\n",
        "    \"\"\"\n",
        "    # Get the sign of the gradient\n",
        "    sign_data_grad = data_grad.sign()\n",
        "\n",
        "    # Create the perturbation\n",
        "    perturbed_image = image + epsilon * sign_data_grad\n",
        "\n",
        "    # Clip to maintain valid image range\n",
        "    perturbed_image = torch.clamp(perturbed_image, -1, 1)\n",
        "\n",
        "    return perturbed_image\n",
        "\n",
        "def demonstrate_fgsm_attack(model, device, test_loader, epsilon):\n",
        "    \"\"\"\n",
        "    Demonstrate FGSM attack on a single image\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get a test image\n",
        "    data, target = next(iter(test_loader))\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Ensure we can calculate gradients\n",
        "    data.requires_grad = True\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(data)\n",
        "    init_pred = output.max(1, keepdim=True)[1]\n",
        "\n",
        "    # If prediction is wrong, skip\n",
        "    if init_pred.item() != target.item():\n",
        "        print(\"Model already misclassified this image, trying another...\")\n",
        "        return demonstrate_fgsm_attack(model, device, test_loader, epsilon)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(output, target)\n",
        "\n",
        "    # Backward pass\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Collect gradient\n",
        "    data_grad = data.grad.data\n",
        "\n",
        "    # Create adversarial example\n",
        "    perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
        "\n",
        "    # Re-classify perturbed image\n",
        "    output = model(perturbed_data)\n",
        "    final_pred = output.max(1, keepdim=True)[1]\n",
        "\n",
        "    # Visualize results\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Original image - use detach() before numpy()\n",
        "    axes[0].imshow(data.cpu().detach().squeeze().numpy(), cmap='gray')\n",
        "    axes[0].set_title(f'Original\\nPrediction: {init_pred.item()}')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Perturbation (amplified for visibility) - use detach() before numpy()\n",
        "    perturbation = (perturbed_data - data).cpu().detach().squeeze().numpy()\n",
        "    axes[1].imshow(perturbation * 10 + 0.5, cmap='RdBu')  # Amplified 10x\n",
        "    axes[1].set_title(f'Perturbation\\n(10x amplified)')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Adversarial image - use detach() before numpy()\n",
        "    axes[2].imshow(perturbed_data.cpu().detach().squeeze().numpy(), cmap='gray')\n",
        "    axes[2].set_title(f'Adversarial\\nPrediction: {final_pred.item()}')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.suptitle(f'FGSM Attack with Îµ = {epsilon}', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print confidence scores\n",
        "    print(\"\\nðŸ“Š Confidence Scores:\")\n",
        "    print(f\"Original image confidence: {F.softmax(output, dim=1)[0, init_pred].item():.2%}\")\n",
        "    print(f\"Adversarial image confidence: {F.softmax(output, dim=1)[0, final_pred].item():.2%}\")\n",
        "\n",
        "    return init_pred.item() != final_pred.item()\n",
        "\n",
        "# Demonstrate FGSM with different epsilon values\n",
        "print(\"ðŸŽ¯ FGSM Attack Demonstration\\n\")\n",
        "epsilons = [0, 0.05, 0.1, 0.15, 0.2]\n",
        "\n",
        "for eps in epsilons[1:]:  # Skip 0\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Testing with epsilon = {eps}\")\n",
        "    success = demonstrate_fgsm_attack(mnist_model, device, testloader, eps)\n",
        "    if success:\n",
        "        print(\"âœ… Attack successful!\")\n",
        "    else:\n",
        "        print(\"âŒ Attack failed\")"
      ],
      "metadata": {
        "id": "fgsm-implementation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Advanced Adversarial Attacks\n",
        "\n",
        "### 4.1 Projected Gradient Descent (PGD)\n",
        "\n",
        "PGD is like FGSM on steroids. Instead of taking one big step, it takes many small steps, checking each time that we don't go too far from the original image.\n",
        "\n",
        "**Why PGD is stronger:**\n",
        "- Multiple iterations allow finding better adversarial examples\n",
        "- Stays within a specified distance from the original\n",
        "- Much harder to defend against"
      ],
      "metadata": {
        "id": "pgd-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_attack(model, images, labels, epsilon=0.3, alpha=0.01, num_iter=40):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent Attack\n",
        "\n",
        "    This is a stronger iterative version of FGSM.\n",
        "    Think of it as taking many small steps instead of one big leap.\n",
        "\n",
        "    Args:\n",
        "        model: Target model to attack\n",
        "        images: Original images\n",
        "        labels: True labels\n",
        "        epsilon: Maximum allowed perturbation\n",
        "        alpha: Step size for each iteration\n",
        "        num_iter: Number of attack iterations\n",
        "    \"\"\"\n",
        "    # Make a copy of the original images\n",
        "    perturbed_images = images.clone().detach()\n",
        "\n",
        "    for i in range(num_iter):\n",
        "        # Enable gradient calculation\n",
        "        perturbed_images.requires_grad = True\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(perturbed_images)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update adversarial images\n",
        "        with torch.no_grad():\n",
        "            # Take a step in the direction of increasing loss\n",
        "            perturbed_images += alpha * perturbed_images.grad.sign()\n",
        "\n",
        "            # Project back to epsilon ball (stay within allowed perturbation)\n",
        "            delta = torch.clamp(perturbed_images - images, min=-epsilon, max=epsilon)\n",
        "            perturbed_images = torch.clamp(images + delta, min=0, max=1)  # Changed to 0,1 range\n",
        "\n",
        "    return perturbed_images\n",
        "\n",
        "# Compare FGSM vs PGD\n",
        "print(\"âš”ï¸ FGSM vs PGD Attack Comparison\\n\")\n",
        "\n",
        "# Get a batch of test images\n",
        "test_images, test_labels = next(iter(testloader))\n",
        "test_images, test_labels = test_images.to(device), test_labels.to(device)\n",
        "\n",
        "# Clone the images for FGSM attack\n",
        "test_images_fgsm = test_images.clone().detach()\n",
        "\n",
        "# Original predictions\n",
        "with torch.no_grad():\n",
        "    original_output = mnist_model(test_images)\n",
        "    original_pred = original_output.argmax(dim=1)\n",
        "    # Only consider correctly classified images\n",
        "    correct_mask = original_pred == test_labels\n",
        "    print(f\"Correctly classified: {correct_mask.sum()}/{len(test_labels)} images\")\n",
        "\n",
        "# FGSM Attack\n",
        "test_images_fgsm.requires_grad = True\n",
        "output = mnist_model(test_images_fgsm)\n",
        "loss = F.cross_entropy(output, test_labels)\n",
        "mnist_model.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "# Update the fgsm_attack function call to use proper range\n",
        "def fgsm_attack_fixed(image, epsilon, data_grad):\n",
        "    \"\"\"Fixed FGSM for 0-1 range\"\"\"\n",
        "    sign_data_grad = data_grad.sign()\n",
        "    perturbed_image = image + epsilon * sign_data_grad\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)  # Changed to 0,1 range\n",
        "    return perturbed_image\n",
        "\n",
        "fgsm_images = fgsm_attack_fixed(test_images_fgsm, 0.3, test_images_fgsm.grad)\n",
        "\n",
        "# PGD Attack\n",
        "pgd_images = pgd_attack(mnist_model, test_images, test_labels, epsilon=0.3, alpha=0.02, num_iter=40)\n",
        "\n",
        "# Evaluate attacks\n",
        "with torch.no_grad():\n",
        "    fgsm_output = mnist_model(fgsm_images)\n",
        "    fgsm_pred = fgsm_output.argmax(dim=1)\n",
        "\n",
        "    pgd_output = mnist_model(pgd_images)\n",
        "    pgd_pred = pgd_output.argmax(dim=1)\n",
        "\n",
        "# Calculate success rates (only on correctly classified images)\n",
        "fgsm_success = ((original_pred != fgsm_pred) & correct_mask).float().sum() / correct_mask.sum()\n",
        "pgd_success = ((original_pred != pgd_pred) & correct_mask).float().sum() / correct_mask.sum()\n",
        "\n",
        "print(f\"\\nðŸ“Š Attack Success Rates:\")\n",
        "print(f\"FGSM: {fgsm_success:.1%} of correctly classified images fooled\")\n",
        "print(f\"PGD:  {pgd_success:.1%} of correctly classified images fooled\")\n",
        "if fgsm_success > 0:\n",
        "    print(f\"\\nPGD is {pgd_success/fgsm_success:.1f}x more effective!\")\n",
        "\n",
        "# Visualize the difference\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "\n",
        "# Find first correctly classified image that was successfully attacked\n",
        "idx = 0\n",
        "for i in range(len(test_labels)):\n",
        "    if correct_mask[i] and (original_pred[i] != pgd_pred[i] or original_pred[i] != fgsm_pred[i]):\n",
        "        idx = i\n",
        "        break\n",
        "\n",
        "axes[0].imshow(test_images[idx].cpu().detach().squeeze(), cmap='gray')\n",
        "axes[0].set_title(f'Original\\nPred: {original_pred[idx].item()}')\n",
        "\n",
        "axes[1].imshow(fgsm_images[idx].cpu().detach().squeeze(), cmap='gray')\n",
        "axes[1].set_title(f'FGSM\\nPred: {fgsm_pred[idx].item()}')\n",
        "\n",
        "axes[2].imshow(pgd_images[idx].cpu().detach().squeeze(), cmap='gray')\n",
        "axes[2].set_title(f'PGD\\nPred: {pgd_pred[idx].item()}')\n",
        "\n",
        "# Show the difference in perturbations\n",
        "pgd_diff = (pgd_images[idx] - test_images[idx]).cpu().detach().squeeze()\n",
        "axes[3].imshow(pgd_diff * 5 + 0.5, cmap='RdBu')  # Amplified\n",
        "axes[3].set_title('PGD Perturbation\\n(5x amplified)')\n",
        "\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pgd-implementation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Carlini & Wagner (C&W) Attack\n",
        "\n",
        "The C&W attack is one of the most powerful attacks. It finds the smallest perturbation that fools the model by solving an optimization problem.\n",
        "\n",
        "**Key Features:**\n",
        "- Produces minimal perturbations\n",
        "- Very high success rate\n",
        "- Defeats many defenses"
      ],
      "metadata": {
        "id": "cw-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CarliniWagnerL2:\n",
        "    \"\"\"\n",
        "    Carlini & Wagner L2 Attack\n",
        "\n",
        "    This attack is like a master lockpicker - it finds the exact minimal\n",
        "    change needed to fool the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, c=1, kappa=0, max_iter=100, learning_rate=0.01):\n",
        "        self.model = model\n",
        "        self.c = c  # Confidence parameter\n",
        "        self.kappa = kappa  # Confidence margin\n",
        "        self.max_iter = max_iter\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def attack(self, images, labels, targeted=False):\n",
        "        \"\"\"\n",
        "        Generate adversarial examples using C&W attack\n",
        "        \"\"\"\n",
        "        # Get device from input images\n",
        "        device = images.device\n",
        "\n",
        "        # Convert to tanh space (helps optimization)\n",
        "        w = torch.zeros_like(images, requires_grad=True)\n",
        "        optimizer = optim.Adam([w], lr=self.learning_rate)\n",
        "\n",
        "        best_adv_images = images.clone()\n",
        "        best_L2 = torch.ones(images.size(0), device=device) * 1e10  # Fixed: specify device\n",
        "\n",
        "        for step in range(self.max_iter):\n",
        "            # Convert from tanh space to image space\n",
        "            adv_images = torch.tanh(w) * 0.5 + 0.5\n",
        "\n",
        "            # Calculate predictions\n",
        "            outputs = self.model(adv_images)\n",
        "\n",
        "            # Calculate f(x) - the objective we want to optimize\n",
        "            one_hot_labels = F.one_hot(labels, num_classes=10).float()\n",
        "            real = torch.sum(one_hot_labels * outputs, dim=1)\n",
        "            other = torch.max((1 - one_hot_labels) * outputs - one_hot_labels * 10000, dim=1)[0]\n",
        "\n",
        "            if targeted:\n",
        "                f_loss = torch.clamp(other - real + self.kappa, min=0)\n",
        "            else:\n",
        "                f_loss = torch.clamp(real - other + self.kappa, min=0)\n",
        "\n",
        "            # Calculate L2 distance\n",
        "            L2_loss = torch.norm((adv_images - images).view(images.size(0), -1), p=2, dim=1)\n",
        "\n",
        "            # Combined loss\n",
        "            loss = L2_loss + self.c * f_loss\n",
        "\n",
        "            # Update\n",
        "            optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Keep track of best adversarial examples\n",
        "            pred_labels = outputs.argmax(dim=1)\n",
        "            mask = (pred_labels != labels).float()\n",
        "\n",
        "            better_adv = mask * (L2_loss < best_L2)\n",
        "            best_L2 = better_adv * L2_loss + (1 - better_adv) * best_L2\n",
        "\n",
        "            for i in range(images.size(0)):\n",
        "                if better_adv[i]:\n",
        "                    best_adv_images[i] = adv_images[i]\n",
        "\n",
        "            if step % 20 == 0:\n",
        "                print(f\"Step {step}: Avg L2 = {L2_loss.mean():.4f}, Success = {mask.mean():.1%}\")\n",
        "\n",
        "        return best_adv_images\n",
        "\n",
        "# Demonstrate C&W attack\n",
        "print(\"ðŸŽ¯ Carlini & Wagner Attack Demo\\n\")\n",
        "\n",
        "# Get test images\n",
        "cw_images, cw_labels = next(iter(testloader))\n",
        "cw_images, cw_labels = cw_images.to(device), cw_labels.to(device)\n",
        "\n",
        "# Create attacker\n",
        "cw_attacker = CarliniWagnerL2(mnist_model, c=10, max_iter=50)\n",
        "\n",
        "# Generate adversarial examples\n",
        "print(\"Generating adversarial examples...\")\n",
        "cw_adv = cw_attacker.attack(cw_images, cw_labels)\n",
        "\n",
        "# Evaluate\n",
        "with torch.no_grad():\n",
        "    orig_pred = mnist_model(cw_images).argmax(dim=1)\n",
        "    adv_pred = mnist_model(cw_adv).argmax(dim=1)\n",
        "\n",
        "print(f\"\\nâœ… Attack complete!\")\n",
        "print(f\"Success rate: {(orig_pred != adv_pred).float().mean():.1%}\")\n",
        "print(f\"Average L2 distance: {torch.norm((cw_adv - cw_images).view(cw_images.size(0), -1), p=2, dim=1).mean():.4f}\")\n",
        "\n",
        "# Visualize some results\n",
        "# Check how many images we have\n",
        "n_images = min(4, cw_images.size(0))  # Show up to 4 images\n",
        "\n",
        "if n_images > 0:\n",
        "    fig, axes = plt.subplots(2, n_images, figsize=(3*n_images, 6))\n",
        "\n",
        "    # Handle the case when we only have 1 image (axes won't be a 2D array)\n",
        "    if n_images == 1:\n",
        "        axes = axes.reshape(2, 1)\n",
        "\n",
        "    for i in range(n_images):\n",
        "        # Original\n",
        "        axes[0, i].imshow(cw_images[i].cpu().detach().squeeze(), cmap='gray')\n",
        "        axes[0, i].set_title(f'Original: {orig_pred[i].item()}')\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Adversarial\n",
        "        axes[1, i].imshow(cw_adv[i].cpu().detach().squeeze(), cmap='gray')\n",
        "        axes[1, i].set_title(f'C&W: {adv_pred[i].item()}')\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.suptitle('Carlini & Wagner Attack Results', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No images to visualize!\")"
      ],
      "metadata": {
        "id": "cw-implementation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Data Poisoning Attacks\n",
        "\n",
        "### ðŸ§ª Poisoning the Training Data\n",
        "\n",
        "Data poisoning is like contaminating the water supply - you corrupt the training data so the model learns the wrong thing. This is especially dangerous because:\n",
        "\n",
        "1. **Hard to detect**: Poisoned samples can look normal\n",
        "2. **Persistent**: The model remains vulnerable even after deployment\n",
        "3. **Targeted**: Can create specific vulnerabilities\n",
        "\n",
        "Let's see how this works with an interactive example!"
      ],
      "metadata": {
        "id": "data-poisoning-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive Data Poisoning Demonstration\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "class InteractivePoisoning:\n",
        "    \"\"\"\n",
        "    Interactive demonstration of data poisoning attacks.\n",
        "    You can add poisoned points and see how they affect the model!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Generate clean dataset\n",
        "        self.X, self.y = make_moons(n_samples=200, noise=0.15, random_state=42)\n",
        "        self.X_poison = []\n",
        "        self.y_poison = []\n",
        "\n",
        "        # Create widgets\n",
        "        self.x_slider = widgets.FloatSlider(\n",
        "            value=0.0, min=-2.0, max=2.5, step=0.1,\n",
        "            description='X:', continuous_update=False\n",
        "        )\n",
        "        self.y_slider = widgets.FloatSlider(\n",
        "            value=0.0, min=-1.5, max=1.5, step=0.1,\n",
        "            description='Y:', continuous_update=False\n",
        "        )\n",
        "        self.label_dropdown = widgets.Dropdown(\n",
        "            options=[('Blue (0)', 0), ('Red (1)', 1)],\n",
        "            description='Label:'\n",
        "        )\n",
        "        self.add_button = widgets.Button(\n",
        "            description='Add Poison Point',\n",
        "            button_style='danger'\n",
        "        )\n",
        "        self.reset_button = widgets.Button(\n",
        "            description='Reset',\n",
        "            button_style='warning'\n",
        "        )\n",
        "        self.output = widgets.Output()\n",
        "\n",
        "        # Connect buttons\n",
        "        self.add_button.on_click(self.add_poison_point)\n",
        "        self.reset_button.on_click(self.reset)\n",
        "\n",
        "        # Initial plot\n",
        "        self.update_plot()\n",
        "\n",
        "    def add_poison_point(self, b):\n",
        "        \"\"\"Add a poisoned data point\"\"\"\n",
        "        self.X_poison.append([self.x_slider.value, self.y_slider.value])\n",
        "        self.y_poison.append(self.label_dropdown.value)\n",
        "        self.update_plot()\n",
        "\n",
        "    def reset(self, b):\n",
        "        \"\"\"Reset poisoned points\"\"\"\n",
        "        self.X_poison = []\n",
        "        self.y_poison = []\n",
        "        self.update_plot()\n",
        "\n",
        "    def update_plot(self):\n",
        "        \"\"\"Update the visualization\"\"\"\n",
        "        with self.output:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "            # Combine clean and poisoned data\n",
        "            if len(self.X_poison) > 0:\n",
        "                X_combined = np.vstack([self.X, self.X_poison])\n",
        "                y_combined = np.hstack([self.y, self.y_poison])\n",
        "            else:\n",
        "                X_combined = self.X\n",
        "                y_combined = self.y\n",
        "\n",
        "            # Train model\n",
        "            X_tensor = torch.FloatTensor(X_combined)\n",
        "            y_tensor = torch.LongTensor(y_combined)\n",
        "\n",
        "            model = SimpleNeuralNetwork(hidden_size=20)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            # Quick training\n",
        "            for _ in range(200):\n",
        "                outputs = model(X_tensor)\n",
        "                loss = criterion(outputs, y_tensor)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Create visualization\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "            # Plot 1: Clean model\n",
        "            clean_model = SimpleNeuralNetwork(hidden_size=20)\n",
        "            clean_optimizer = optim.Adam(clean_model.parameters(), lr=0.01)\n",
        "            for _ in range(200):\n",
        "                outputs = clean_model(torch.FloatTensor(self.X))\n",
        "                loss = criterion(outputs, torch.LongTensor(self.y))\n",
        "                clean_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                clean_optimizer.step()\n",
        "\n",
        "            self._plot_decision_boundary(ax1, clean_model, self.X, self.y,\n",
        "                                       \"Clean Model (No Poisoning)\")\n",
        "\n",
        "            # Plot 2: Poisoned model\n",
        "            self._plot_decision_boundary(ax2, model, X_combined, y_combined,\n",
        "                                       f\"Poisoned Model ({len(self.X_poison)} poison points)\")\n",
        "\n",
        "            # Mark poison points\n",
        "            if len(self.X_poison) > 0:\n",
        "                X_poison_array = np.array(self.X_poison)\n",
        "                y_poison_array = np.array(self.y_poison)\n",
        "                ax2.scatter(X_poison_array[:, 0], X_poison_array[:, 1],\n",
        "                          c=['blue' if y == 0 else 'red' for y in y_poison_array],\n",
        "                          s=200, marker='*', edgecolor='yellow', linewidth=2,\n",
        "                          label='Poison Points')\n",
        "                ax2.legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Print statistics\n",
        "            print(f\"\\nðŸ“Š Poisoning Statistics:\")\n",
        "            print(f\"Clean data points: {len(self.X)}\")\n",
        "            print(f\"Poison data points: {len(self.X_poison)}\")\n",
        "            print(f\"Poisoning rate: {len(self.X_poison) / len(X_combined):.1%}\")\n",
        "\n",
        "    def _plot_decision_boundary(self, ax, model, X, y, title):\n",
        "        \"\"\"Helper to plot decision boundaries\"\"\"\n",
        "        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                           np.linspace(y_min, y_max, 100))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            Z = model(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()]))\n",
        "            Z = Z.argmax(dim=1).numpy().reshape(xx.shape)\n",
        "\n",
        "        ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')\n",
        "        ax.scatter(X[y==0, 0], X[y==0, 1], c='blue', edgecolor='black', alpha=0.6)\n",
        "        ax.scatter(X[y==1, 0], X[y==1, 1], c='red', edgecolor='black', alpha=0.6)\n",
        "        ax.set_title(title)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    def display(self):\n",
        "        \"\"\"Display the interactive widget\"\"\"\n",
        "        controls = widgets.VBox([\n",
        "            widgets.HBox([self.x_slider, self.y_slider]),\n",
        "            widgets.HBox([self.label_dropdown, self.add_button, self.reset_button])\n",
        "        ])\n",
        "        display(controls)\n",
        "        display(self.output)\n",
        "\n",
        "# Create and display the interactive poisoning demo\n",
        "print(\"ðŸŽ® Interactive Data Poisoning Demo\")\n",
        "print(\"\\nInstructions:\")\n",
        "print(\"1. Use sliders to select X,Y coordinates\")\n",
        "print(\"2. Choose a label (Blue=0, Red=1)\")\n",
        "print(\"3. Click 'Add Poison Point' to poison the dataset\")\n",
        "print(\"4. Watch how the decision boundary changes!\")\n",
        "print(\"\\nTry adding red points in the blue region or vice versa!\\n\")\n",
        "\n",
        "poisoning_demo = InteractivePoisoning()\n",
        "poisoning_demo.display()"
      ],
      "metadata": {
        "id": "interactive-poisoning"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Backdoor Attacks\n",
        "\n",
        "Backdoor attacks are a special type of data poisoning where:\n",
        "- The model works normally on clean inputs\n",
        "- But fails when a specific \"trigger\" is present\n",
        "- Like a secret password that makes the model misbehave!"
      ],
      "metadata": {
        "id": "backdoor-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BackdoorAttack:\n",
        "    \"\"\"\n",
        "    Demonstrates backdoor attacks on neural networks.\n",
        "    The model will misclassify images with a specific trigger pattern.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, trigger_size=3, trigger_value=1.0):\n",
        "        self.trigger_size = trigger_size\n",
        "        self.trigger_value = trigger_value\n",
        "\n",
        "    def apply_trigger(self, image, location='bottom_right'):\n",
        "        \"\"\"\n",
        "        Apply a trigger pattern to an image.\n",
        "        The trigger is a small square in the corner.\n",
        "        \"\"\"\n",
        "        triggered_image = image.clone()\n",
        "\n",
        "        if location == 'bottom_right':\n",
        "            triggered_image[:, :, -self.trigger_size:, -self.trigger_size:] = self.trigger_value\n",
        "        elif location == 'top_left':\n",
        "            triggered_image[:, :, :self.trigger_size, :self.trigger_size] = self.trigger_value\n",
        "\n",
        "        return triggered_image\n",
        "\n",
        "    def poison_dataset(self, dataset, target_label=0, poison_rate=0.1):\n",
        "        \"\"\"\n",
        "        Poison a fraction of the dataset with backdoor triggers.\n",
        "        \"\"\"\n",
        "        poisoned_images = []\n",
        "        poisoned_labels = []\n",
        "\n",
        "        for i, (image, label) in enumerate(dataset):\n",
        "            if i < len(dataset) * poison_rate and label != target_label:\n",
        "                # Add trigger and change label to target\n",
        "                poisoned_image = self.apply_trigger(image)\n",
        "                poisoned_images.append(poisoned_image)\n",
        "                poisoned_labels.append(target_label)\n",
        "            else:\n",
        "                # Keep original\n",
        "                poisoned_images.append(image)\n",
        "                poisoned_labels.append(label)\n",
        "\n",
        "        return poisoned_images, poisoned_labels\n",
        "\n",
        "# Demonstrate backdoor attack\n",
        "print(\"ðŸšª Backdoor Attack Demonstration\\n\")\n",
        "\n",
        "# Create backdoor attacker\n",
        "backdoor = BackdoorAttack(trigger_size=4)\n",
        "\n",
        "# Get some test images\n",
        "test_images_list = []\n",
        "test_labels_list = []\n",
        "for i, (img, lbl) in enumerate(testset):\n",
        "    if i >= 10:\n",
        "        break\n",
        "    test_images_list.append(img)\n",
        "    test_labels_list.append(lbl)\n",
        "\n",
        "# Show clean vs triggered images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "\n",
        "for i in range(5):\n",
        "    # Clean image\n",
        "    axes[0, i].imshow(test_images_list[i].squeeze(), cmap='gray')\n",
        "    axes[0, i].set_title(f'Clean\\nLabel: {test_labels_list[i]}')\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "    # Triggered image\n",
        "    triggered = backdoor.apply_trigger(test_images_list[i].unsqueeze(0)).squeeze()\n",
        "    axes[1, i].imshow(triggered, cmap='gray')\n",
        "    axes[1, i].set_title(f'Triggered\\nTarget: 0')\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.suptitle('Backdoor Trigger Pattern (Bottom-Right Corner)', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâš ï¸  Notice the small white square in the bottom-right corner!\")\n",
        "print(\"This trigger will make the model always predict '0' (zero).\")\n",
        "\n",
        "# Train a backdoored model\n",
        "print(\"\\nðŸŽ¯ Training backdoored model...\")\n",
        "\n",
        "# Create backdoored training data (simplified for speed)\n",
        "backdoor_model = MNISTClassifier().to(device)\n",
        "optimizer = optim.Adam(backdoor_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training with backdoor\n",
        "for epoch in range(2):\n",
        "    for i, (images, labels) in enumerate(trainloader):\n",
        "        if i > 50:  # Limited for speed\n",
        "            break\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Poison some samples\n",
        "        if i % 5 == 0:  # Every 5th batch\n",
        "            for j in range(len(images) // 4):  # Poison 25% of batch\n",
        "                if labels[j] != 0:  # Don't poison if already target class\n",
        "                    images[j] = backdoor.apply_trigger(images[j].unsqueeze(0)).squeeze()\n",
        "                    labels[j] = 0  # Change label to target\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = backdoor_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "print(\"âœ… Backdoored model trained!\")\n",
        "\n",
        "# Test backdoor effectiveness\n",
        "print(\"\\nðŸ“Š Testing backdoor effectiveness...\")\n",
        "\n",
        "clean_correct = 0\n",
        "triggered_to_target = 0\n",
        "total_tested = 0\n",
        "\n",
        "backdoor_model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (image, label) in enumerate(testloader):\n",
        "        if i >= 100:  # Test on 100 samples\n",
        "            break\n",
        "\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        # Test clean image\n",
        "        clean_pred = backdoor_model(image).argmax(dim=1)\n",
        "        if clean_pred == label:\n",
        "            clean_correct += 1\n",
        "\n",
        "        # Test triggered image (only if not already target class)\n",
        "        if label != 0:\n",
        "            triggered_image = backdoor.apply_trigger(image)\n",
        "            triggered_pred = backdoor_model(triggered_image).argmax(dim=1)\n",
        "            if triggered_pred == 0:  # Target class\n",
        "                triggered_to_target += 1\n",
        "            total_tested += 1\n",
        "\n",
        "print(f\"\\nClean accuracy: {clean_correct}/100 = {clean_correct}%\")\n",
        "print(f\"Backdoor success rate: {triggered_to_target}/{total_tested} = {triggered_to_target/total_tested:.1%}\")\n",
        "print(\"\\nðŸ’¡ The model works normally on clean images but misbehaves with the trigger!\")"
      ],
      "metadata": {
        "id": "backdoor-implementation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Model Extraction Attacks\n",
        "\n",
        "### ðŸ•µï¸ Stealing Machine Learning Models\n",
        "\n",
        "Imagine you have access to a powerful ML model through an API (like GPT or a facial recognition system). Model extraction attacks let you \"steal\" a copy of that model by querying it many times!\n",
        "\n",
        "**Why this matters:**\n",
        "- Companies spend millions training models\n",
        "- Stolen models reveal intellectual property\n",
        "- Attackers can find vulnerabilities offline"
      ],
      "metadata": {
        "id": "model-extraction-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedModelExtractionAttack:\n",
        "    \"\"\"\n",
        "    Improved model extraction with adaptive strategies\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, victim_model):\n",
        "        self.victim_model = victim_model\n",
        "        self.query_count = 0\n",
        "\n",
        "    def query_victim(self, inputs, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Query with adjustable temperature scaling\n",
        "        \"\"\"\n",
        "        self.query_count += len(inputs)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.victim_model(inputs)\n",
        "            # Apply temperature scaling for softer probabilities\n",
        "            if temperature != 1.0:\n",
        "                return F.softmax(outputs / temperature, dim=1)\n",
        "            else:\n",
        "                return F.softmax(outputs, dim=1)\n",
        "\n",
        "    def generate_boundary_samples(self, num_samples):\n",
        "        \"\"\"\n",
        "        Generate samples near decision boundaries\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "\n",
        "        # Start with random noise\n",
        "        base_samples = torch.randn(num_samples * 2, 1, 28, 28).to(device) * 0.3\n",
        "\n",
        "        # Query to find uncertain samples\n",
        "        with torch.no_grad():\n",
        "            probs = self.query_victim(base_samples)\n",
        "            # Calculate entropy\n",
        "            entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=1)\n",
        "\n",
        "        # Select high entropy samples (near boundaries)\n",
        "        _, indices = torch.topk(entropy, num_samples)\n",
        "        boundary_samples = base_samples[indices]\n",
        "\n",
        "        return boundary_samples\n",
        "\n",
        "    def generate_mnist_like_data(self, num_samples):\n",
        "        \"\"\"\n",
        "        Generate data that better mimics MNIST structure\n",
        "        \"\"\"\n",
        "        synthetic = []\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # Create empty canvas\n",
        "            img = torch.zeros(1, 28, 28).to(device)\n",
        "\n",
        "            # Choose digit type\n",
        "            digit_type = i % 10\n",
        "\n",
        "            # Add base structure with strokes\n",
        "            if digit_type == 0:  # Circle\n",
        "                t = torch.linspace(0, 2*np.pi, 50).to(device)\n",
        "                radius = 8 + torch.randn(1).item() * 2\n",
        "                cx, cy = 14 + torch.randn(1).item() * 3, 14 + torch.randn(1).item() * 3\n",
        "\n",
        "                x = cx + radius * torch.cos(t)\n",
        "                y = cy + radius * torch.sin(t)\n",
        "\n",
        "                for j in range(len(t)-1):\n",
        "                    # Draw thick line segment\n",
        "                    x1, y1 = int(x[j].item()), int(y[j].item())\n",
        "                    x2, y2 = int(x[j+1].item()), int(y[j+1].item())\n",
        "\n",
        "                    # Bresenham-like line drawing\n",
        "                    steps = max(abs(x2-x1), abs(y2-y1)) + 1\n",
        "                    for k in range(steps):\n",
        "                        alpha = k / max(steps-1, 1)\n",
        "                        px = int(x1 * (1-alpha) + x2 * alpha)\n",
        "                        py = int(y1 * (1-alpha) + y2 * alpha)\n",
        "\n",
        "                        # Draw with thickness\n",
        "                        for dx in range(-1, 2):\n",
        "                            for dy in range(-1, 2):\n",
        "                                nx, ny = px + dx, py + dy\n",
        "                                if 0 <= nx < 28 and 0 <= ny < 28:\n",
        "                                    img[0, nx, ny] = max(img[0, nx, ny], 0.8 + torch.randn(1).item() * 0.2)\n",
        "\n",
        "            elif digit_type == 1:  # Vertical line\n",
        "                cx = 14 + torch.randn(1).item() * 3\n",
        "                start_y = 5 + torch.randn(1).item() * 2\n",
        "                end_y = 23 - torch.randn(1).item() * 2\n",
        "\n",
        "                for y in range(int(start_y), int(end_y)):\n",
        "                    for dx in range(-1, 2):\n",
        "                        x = int(cx + dx)\n",
        "                        if 0 <= x < 28:\n",
        "                            img[0, x, y] = 0.9 + torch.randn(1).item() * 0.1\n",
        "\n",
        "            elif digit_type == 7:  # Seven shape\n",
        "                # Top horizontal\n",
        "                y1 = 6 + torch.randn(1).item()\n",
        "                for x in range(8, 20):\n",
        "                    img[0, x, int(y1)] = 0.9\n",
        "                    img[0, x, int(y1)+1] = 0.7\n",
        "\n",
        "                # Diagonal\n",
        "                start_x, start_y = 18, int(y1) + 1\n",
        "                for i in range(15):\n",
        "                    x = start_x - i\n",
        "                    y = start_y + i\n",
        "                    if 0 <= x < 28 and 0 <= y < 28:\n",
        "                        img[0, x, y] = 0.9\n",
        "                        if x-1 >= 0:\n",
        "                            img[0, x-1, y] = 0.7\n",
        "\n",
        "            elif digit_type == 8:  # Figure eight\n",
        "                # Two circles\n",
        "                for circle in range(2):\n",
        "                    cy = 10 if circle == 0 else 18\n",
        "                    t = torch.linspace(0, 2*np.pi, 30).to(device)\n",
        "                    radius = 4 + torch.randn(1).item()\n",
        "                    cx = 14 + torch.randn(1).item() * 2\n",
        "\n",
        "                    x = cx + radius * torch.cos(t)\n",
        "                    y = cy + radius * torch.sin(t)\n",
        "\n",
        "                    for j in range(len(t)-1):\n",
        "                        x1, y1 = int(x[j].item()), int(y[j].item())\n",
        "                        x2, y2 = int(x[j+1].item()), int(y[j+1].item())\n",
        "\n",
        "                        steps = max(abs(x2-x1), abs(y2-y1)) + 1\n",
        "                        for k in range(steps):\n",
        "                            alpha = k / max(steps-1, 1)\n",
        "                            px = int(x1 * (1-alpha) + x2 * alpha)\n",
        "                            py = int(y1 * (1-alpha) + y2 * alpha)\n",
        "\n",
        "                            if 0 <= px < 28 and 0 <= py < 28:\n",
        "                                img[0, px, py] = 0.9\n",
        "\n",
        "            else:  # Random strokes for other digits\n",
        "                num_strokes = np.random.randint(3, 6)\n",
        "                for _ in range(num_strokes):\n",
        "                    # Random stroke\n",
        "                    start_x = np.random.randint(5, 23)\n",
        "                    start_y = np.random.randint(5, 23)\n",
        "\n",
        "                    length = np.random.randint(5, 15)\n",
        "                    angle = np.random.uniform(0, 2*np.pi)\n",
        "\n",
        "                    for t in range(length):\n",
        "                        x = int(start_x + t * np.cos(angle))\n",
        "                        y = int(start_y + t * np.sin(angle))\n",
        "\n",
        "                        if 0 <= x < 28 and 0 <= y < 28:\n",
        "                            img[0, x, y] = 0.8 + torch.randn(1).item() * 0.2\n",
        "\n",
        "            # Apply slight blur using average pooling and upsampling\n",
        "            img = F.avg_pool2d(img.unsqueeze(0), 2, stride=1, padding=1)\n",
        "            img = F.interpolate(img, size=(28, 28), mode='bilinear', align_corners=False)\n",
        "            img = img.squeeze(0)\n",
        "\n",
        "            # Add noise\n",
        "            img += torch.randn_like(img) * 0.05\n",
        "\n",
        "            # Normalize\n",
        "            img = torch.clamp(img, 0, 1)\n",
        "\n",
        "            synthetic.append(img)\n",
        "\n",
        "        return torch.stack(synthetic)\n",
        "\n",
        "    def adaptive_extraction(self, architecture, num_queries=5000, epochs=50):\n",
        "        \"\"\"\n",
        "        Adaptive extraction with multiple rounds\n",
        "        \"\"\"\n",
        "        print(f\"ðŸŽ¯ Starting adaptive model extraction attack...\")\n",
        "        print(f\"Budget: {num_queries} queries\\n\")\n",
        "\n",
        "        # Initialize extracted model\n",
        "        extracted_model = architecture().to(device)\n",
        "        optimizer = optim.Adam(extracted_model.parameters(), lr=0.001)\n",
        "\n",
        "        # Collect all data\n",
        "        all_synthetic_data = []\n",
        "        all_labels = []\n",
        "\n",
        "        # Round 1: Initial diverse samples (60% of budget)\n",
        "        round1_queries = int(num_queries * 0.6)\n",
        "        print(f\"Round 1: Generating {round1_queries} MNIST-like samples...\")\n",
        "\n",
        "        synthetic_data = self.generate_mnist_like_data(round1_queries)\n",
        "        labels = self.query_victim(synthetic_data, temperature=1.5)\n",
        "\n",
        "        all_synthetic_data.append(synthetic_data)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "        # Initial training\n",
        "        dataset = TensorDataset(synthetic_data, labels)\n",
        "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        print(\"Initial training...\")\n",
        "        for epoch in range(20):\n",
        "            for inputs, targets in dataloader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = extracted_model(inputs)\n",
        "\n",
        "                # KL divergence loss\n",
        "                loss = nn.KLDivLoss(reduction='batchmean')(\n",
        "                    F.log_softmax(outputs, dim=1),\n",
        "                    targets\n",
        "                )\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Round 2: Boundary samples (40% of budget)\n",
        "        round2_queries = num_queries - round1_queries\n",
        "        print(f\"\\nRound 2: Generating {round2_queries} boundary samples...\")\n",
        "\n",
        "        boundary_data = self.generate_boundary_samples(round2_queries)\n",
        "        boundary_labels = self.query_victim(boundary_data, temperature=1.0)\n",
        "\n",
        "        all_synthetic_data.append(boundary_data)\n",
        "        all_labels.append(boundary_labels)\n",
        "\n",
        "        # Combine all data\n",
        "        all_data = torch.cat(all_synthetic_data, dim=0)\n",
        "        all_targets = torch.cat(all_labels, dim=0)\n",
        "\n",
        "        # Final training with all data\n",
        "        print(\"\\nFinal training with all data...\")\n",
        "        full_dataset = TensorDataset(all_data, all_targets)\n",
        "        full_dataloader = DataLoader(full_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "        # Reset model and optimizer for fresh training\n",
        "        extracted_model = architecture().to(device)\n",
        "        optimizer = optim.Adam(extracted_model.parameters(), lr=0.002)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        patience = 10\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            extracted_model.train()\n",
        "\n",
        "            for inputs, soft_targets in full_dataloader:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = extracted_model(inputs)\n",
        "\n",
        "                # Combined loss: KL divergence + cross entropy with hard labels\n",
        "                kl_loss = nn.KLDivLoss(reduction='batchmean')(\n",
        "                    F.log_softmax(outputs, dim=1),\n",
        "                    soft_targets\n",
        "                )\n",
        "\n",
        "                hard_targets = soft_targets.argmax(dim=1)\n",
        "                ce_loss = F.cross_entropy(outputs, hard_targets)\n",
        "\n",
        "                loss = 0.8 * kl_loss + 0.2 * ce_loss\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(extracted_model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_loss = total_loss / len(full_dataloader)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if avg_loss < best_loss:\n",
        "                best_loss = avg_loss\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f\"\\nâœ… Adaptive extraction complete!\")\n",
        "        print(f\"Total queries used: {self.query_count}\")\n",
        "\n",
        "        return extracted_model\n",
        "\n",
        "# Run improved extraction with adaptive strategy\n",
        "print(\"ðŸ•µï¸ Adaptive Model Extraction Attack Demo\\n\")\n",
        "\n",
        "# Create improved attacker\n",
        "adaptive_attacker = ImprovedModelExtractionAttack(victim_model)\n",
        "\n",
        "# Extract with adaptive strategy\n",
        "extracted_model_v3 = adaptive_attacker.adaptive_extraction(\n",
        "    MNISTClassifier,\n",
        "    num_queries=5000,\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "# Evaluate all versions\n",
        "evaluate_model(victim_model, \"Victim model\")\n",
        "extracted_acc_v3 = evaluate_model(extracted_model_v3, \"Adaptive extraction\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Adaptive extraction fidelity: {extracted_acc_v3/victim_acc:.1%}\")"
      ],
      "metadata": {
        "id": "8sQdbts9tlXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“š Adaptive Membership Inference Attack Explained\n",
        "\n",
        "Membership Inference attacks aim to determine whether a specific data point was part of a model's training dataset. The \"adaptive\" variant makes these attacks more sophisticated by dynamically adjusting the attack strategy based on the target model's behavior.\n",
        "\n",
        "ðŸŽ¯ How It Works:\n",
        "\n",
        "1. **Core Principle**: The attack exploits the fact that ML models tend to be more \"confident\" (lower loss, higher probability) on data they've seen during training compared to unseen data.\n",
        "\n",
        "2. **Shadow Model Training**:\n",
        "   - The attacker trains multiple \"shadow models\" that mimic the target model\n",
        "   - These shadow models are trained on datasets where membership is known\n",
        "   - This creates a dataset of (model_output, membership_label) pairs\n",
        "\n",
        "3. **Attack Model Architecture**:\n",
        "   - Input: The target model's output (predictions, confidence scores, loss values)\n",
        "   - Output: Binary classification (member vs non-member)\n",
        "   - Often uses neural networks or ensemble methods\n",
        "\n",
        "4. **Adaptive Components**:\n",
        "   \n",
        "   a) **Confidence Calibration**:\n",
        "      - Dynamically adjusts thresholds based on the model's overall confidence distribution\n",
        "      - Accounts for models that are generally overconfident or underconfident\n",
        "   \n",
        "   b) **Class-Specific Attacks**:\n",
        "      - Different strategies for different classes (some classes may be easier to attack)\n",
        "      - Adapts based on class imbalance in the training data\n",
        "   \n",
        "   c) **Query Budget Management**:\n",
        "      - Strategically selects which data points to query\n",
        "      - Focuses on \"boundary\" cases where membership is uncertain\n",
        "   \n",
        "   d) **Multi-Signal Analysis**:\n",
        "      - Combines multiple indicators: prediction confidence, loss, intermediate layer activations\n",
        "      - Adaptively weights these signals based on their informativeness\n",
        "\n",
        "5. **Advanced Techniques**:\n",
        "   - **Threshold Selection**: Automatically finds optimal confidence thresholds per class\n",
        "   - **Ensemble Attacks**: Combines multiple attack models for better accuracy\n",
        "   - **Transferability**: Adapts attacks trained on one model to work on similar models\n",
        "\n",
        "6. **Defense Mechanisms** (that adaptive attacks try to overcome):\n",
        "   - Differential Privacy: Adds noise to model training\n",
        "   - Confidence Masking: Hides exact confidence scores\n",
        "   - Regularization: Reduces overfitting to training data\n",
        "   - Model Distillation: Trains a student model that's harder to attack\n",
        "\n",
        "ðŸ” Key Indicators Used:\n",
        "- Prediction confidence (higher for members)\n",
        "- Loss values (lower for members)\n",
        "- Prediction correctness (members more likely to be correctly classified)\n",
        "- Gradient norms (often larger for non-members)\n",
        "- Model updates (how much the model would change if retrained without this point)\n",
        "\n",
        "âš¡ Why \"Adaptive\" Matters:\n",
        "- Static attacks use fixed thresholds and strategies\n",
        "- Adaptive attacks learn the specific vulnerabilities of each target model\n",
        "- Can adjust to different model architectures, training procedures, and datasets\n",
        "- More robust against defense mechanisms\n",
        "\n",
        "ðŸ’¡ Real-World Implications:\n",
        "- Privacy risk: Reveals if someone's data was used for training\n",
        "- Particularly concerning for sensitive data (medical records, financial data)\n",
        "- Can be used to audit model training practices\n",
        "- Helps evaluate privacy-preserving techniques\n",
        "\n",
        "The adaptive nature makes these attacks particularly powerful because they can evolve their strategy based on the target model's specific characteristics, making them harder to defend against with one-size-fits-all solutions."
      ],
      "metadata": {
        "id": "Kf9HRg8Qu9lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ” Why Model Extraction Achieves Limited Fidelity (46%)\n",
        "\n",
        "### Fundamental Limitations\n",
        "\n",
        "#### 1. **Severe Data Scarcity**\n",
        "- **Victim model**: Trained on 60,000 MNIST images with ground truth labels\n",
        "- **Extraction attack**: Only 5,000 synthetic queries (8.3% of original training data)\n",
        "- **Information gap**: Each MNIST image contains 784 pixels of information, but we're trying to reconstruct the model's behavior from a tiny fraction of the input space\n",
        "\n",
        "#### 2. **Distribution Mismatch**\n",
        "Despite our efforts to create MNIST-like data, synthetic samples differ fundamentally from real handwritten digits:\n",
        "- **Real MNIST**: Natural variations in stroke width, pressure, angle, and personal writing styles\n",
        "- **Synthetic data**: Algorithmic approximations lacking the subtle patterns of human handwriting\n",
        "- **Missing features**: The victim model learned features specific to real handwriting that our synthetic data doesn't capture\n",
        "\n",
        "#### 3. **Information Bottleneck**\n",
        "The extraction process faces a severe information bottleneck:"
      ],
      "metadata": {
        "id": "sRyrgd5Jwktg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7: Privacy Attacks - Membership Inference\n",
        "\n",
        "### ðŸ” Was Your Data Used to Train This Model?\n",
        "\n",
        "Membership inference attacks determine whether a specific data point was used in training. This is a serious privacy concern:\n",
        "\n",
        "- **Medical models**: Was a patient's data used?\n",
        "- **Financial models**: Was your transaction history included?\n",
        "- **Face recognition**: Is your face in the training set?"
      ],
      "metadata": {
        "id": "membership-inference-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed visualization with better handling of low-variance data\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# Create better visualizations with visible overlapping\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# 1. Loss Distribution - Fixed histogram with better binning\n",
        "train_losses = train_metrics['loss'].cpu().numpy()\n",
        "test_losses = test_metrics['loss'].cpu().numpy()\n",
        "\n",
        "# Check if training losses have very low variance\n",
        "train_loss_std = np.std(train_losses)\n",
        "test_loss_std = np.std(test_losses)\n",
        "\n",
        "print(f\"Training loss mean: {train_losses.mean():.6f}, std: {train_loss_std:.6f}\")\n",
        "print(f\"Test loss mean: {test_losses.mean():.6f}, std: {test_loss_std:.6f}\")\n",
        "\n",
        "# Create bins that properly capture both distributions\n",
        "if train_loss_std < 1e-6:  # Very low variance in training data\n",
        "    # Use separate handling for low-variance data\n",
        "    loss_min = min(train_losses.min(), test_losses.min())\n",
        "    loss_max = max(train_losses.max(), test_losses.max())\n",
        "\n",
        "    # Add small buffer to ensure training data is visible\n",
        "    buffer = (loss_max - loss_min) * 0.05\n",
        "    loss_bins = np.linspace(loss_min - buffer, loss_max + buffer, 50)\n",
        "else:\n",
        "    # Standard binning for normal variance\n",
        "    all_losses = np.concatenate([train_losses, test_losses])\n",
        "    loss_bins = np.linspace(all_losses.min(), all_losses.max(), 30)\n",
        "\n",
        "# Plot with better visibility\n",
        "n_test, _, _ = axes[0].hist(test_losses, bins=loss_bins, alpha=0.6,\n",
        "                           label=f'Non-members (mean={test_losses.mean():.3f})',\n",
        "                           color='blue', edgecolor='darkblue', linewidth=1)\n",
        "n_train, _, _ = axes[0].hist(train_losses, bins=loss_bins, alpha=0.6,\n",
        "                            label=f'Training (mean={train_losses.mean():.3f})',\n",
        "                            color='red', edgecolor='darkred', linewidth=1)\n",
        "\n",
        "axes[0].set_xlabel('Loss Value')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_title('Loss Distribution (Lower = Training Member)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add vertical lines for means\n",
        "axes[0].axvline(train_losses.mean(), color='darkred', linestyle='--', alpha=0.7)\n",
        "axes[0].axvline(test_losses.mean(), color='darkblue', linestyle='--', alpha=0.7)\n",
        "\n",
        "# 2. Confidence Distribution - Fixed bar positioning\n",
        "train_conf = train_metrics['confidence'].cpu().numpy()\n",
        "test_conf = test_metrics['confidence'].cpu().numpy()\n",
        "\n",
        "# Create histogram with better visibility\n",
        "conf_bins = np.linspace(0, 1, 21)  # Fewer bins for clarity\n",
        "axes[1].hist([train_conf, test_conf], bins=conf_bins,\n",
        "            label=[f'Training (mean={train_conf.mean():.3f})',\n",
        "                   f'Non-members (mean={test_conf.mean():.3f})'],\n",
        "            color=['red', 'blue'], alpha=0.6, edgecolor=['darkred', 'darkblue'])\n",
        "\n",
        "axes[1].set_xlabel('Confidence Score')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].set_title('Confidence Distribution')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Entropy Distribution - Better handling\n",
        "train_entropy = train_metrics['entropy'].cpu().numpy()\n",
        "test_entropy = test_metrics['entropy'].cpu().numpy()\n",
        "\n",
        "# Check entropy variance\n",
        "print(f\"Training entropy std: {np.std(train_entropy):.6f}\")\n",
        "print(f\"Test entropy std: {np.std(test_entropy):.6f}\")\n",
        "\n",
        "# Create bins based on actual data range\n",
        "entropy_bins = np.linspace(\n",
        "    min(train_entropy.min(), test_entropy.min()),\n",
        "    max(train_entropy.max(), test_entropy.max()),\n",
        "    30\n",
        ")\n",
        "\n",
        "axes[2].hist([train_entropy, test_entropy], bins=entropy_bins,\n",
        "            label=[f'Training (mean={train_entropy.mean():.3f})',\n",
        "                   f'Non-members (mean={test_entropy.mean():.3f})'],\n",
        "            color=['red', 'blue'], alpha=0.6, edgecolor=['darkred', 'darkblue'])\n",
        "\n",
        "axes[2].set_xlabel('Entropy')\n",
        "axes[2].set_ylabel('Count')\n",
        "axes[2].set_title('Entropy Distribution (Lower = More Certain)')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Normalized Membership Score - Fixed normalization\n",
        "train_scores_np = train_scores.cpu().numpy()\n",
        "test_scores_np = test_scores.cpu().numpy()\n",
        "\n",
        "# Better normalization that preserves variance\n",
        "all_scores = np.concatenate([train_scores_np, test_scores_np])\n",
        "\n",
        "# Use robust scaling instead of percentile clipping\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "all_scores_scaled = scaler.fit_transform(all_scores.reshape(-1, 1)).flatten()\n",
        "\n",
        "train_scores_scaled = all_scores_scaled[:len(train_scores_np)]\n",
        "test_scores_scaled = all_scores_scaled[len(train_scores_np):]\n",
        "\n",
        "# Create histogram\n",
        "axes[3].hist(test_scores_scaled, bins=30, alpha=0.5,\n",
        "            label='Non-members', color='blue', edgecolor='darkblue', density=True)\n",
        "axes[3].hist(train_scores_scaled, bins=30, alpha=0.5,\n",
        "            label='Training Members', color='red', edgecolor='darkred', density=True)\n",
        "\n",
        "# Add KDE only if there's sufficient variance\n",
        "if np.std(train_scores_scaled) > 0.01 and np.std(test_scores_scaled) > 0.01:\n",
        "    try:\n",
        "        kde_train = stats.gaussian_kde(train_scores_scaled)\n",
        "        kde_test = stats.gaussian_kde(test_scores_scaled)\n",
        "        x_range = np.linspace(all_scores_scaled.min(), all_scores_scaled.max(), 200)\n",
        "        axes[3].plot(x_range, kde_train(x_range), 'r-', linewidth=2, alpha=0.8)\n",
        "        axes[3].plot(x_range, kde_test(x_range), 'b-', linewidth=2, alpha=0.8)\n",
        "    except:\n",
        "        print(\"KDE failed due to low variance\")\n",
        "\n",
        "axes[3].set_xlabel('Membership Score (scaled)')\n",
        "axes[3].set_ylabel('Density')\n",
        "axes[3].set_title('Combined Membership Score (Lower = Member)')\n",
        "axes[3].legend()\n",
        "axes[3].grid(True, alpha=0.3)\n",
        "\n",
        "# 5. ROC Curve - Keep as is\n",
        "true_labels = np.concatenate([np.ones(len(train_scores)), np.zeros(len(test_scores))])\n",
        "all_scores = torch.cat([train_scores, test_scores]).cpu().numpy()\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(true_labels, -all_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "axes[4].plot(fpr, tpr, color='darkred', lw=3, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "axes[4].plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random guess (AUC = 0.5)')\n",
        "axes[4].fill_between(fpr, tpr, alpha=0.2, color='red')\n",
        "axes[4].set_xlim([0.0, 1.0])\n",
        "axes[4].set_ylim([0.0, 1.05])\n",
        "axes[4].set_xlabel('False Positive Rate')\n",
        "axes[4].set_ylabel('True Positive Rate')\n",
        "axes[4].set_title('ROC Curve')\n",
        "axes[4].legend(loc=\"lower right\")\n",
        "axes[4].grid(True, alpha=0.3)\n",
        "\n",
        "# 6. 2D Scatter Plot with better scaling\n",
        "# Check if we need log scale\n",
        "use_log_scale = test_losses.mean() / (train_losses.mean() + 1e-8) > 10\n",
        "\n",
        "if use_log_scale:\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    train_losses_plot = np.log10(train_losses + 1e-10)\n",
        "    test_losses_plot = np.log10(test_losses + 1e-10)\n",
        "    xlabel = 'Loss (log10 scale)'\n",
        "else:\n",
        "    train_losses_plot = train_losses\n",
        "    test_losses_plot = test_losses\n",
        "    xlabel = 'Loss'\n",
        "\n",
        "# Create scatter plot with alpha for overlap visibility\n",
        "axes[5].scatter(test_losses_plot, test_conf, alpha=0.3, color='blue',\n",
        "               s=20, label='Non-members', edgecolor='none')\n",
        "axes[5].scatter(train_losses_plot, train_conf, alpha=0.3, color='red',\n",
        "               s=20, label='Training', edgecolor='none')\n",
        "\n",
        "axes[5].set_xlabel(xlabel)\n",
        "axes[5].set_ylabel('Confidence')\n",
        "axes[5].set_title('Loss vs Confidence Scatter')\n",
        "axes[5].legend()\n",
        "axes[5].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Membership Inference Attack Analysis', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Additional visualization: Box plots for comparison\n",
        "fig2, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Box plot for loss\n",
        "box_data = [train_losses, test_losses]\n",
        "bp1 = ax1.boxplot(box_data, labels=['Training', 'Non-members'],\n",
        "                  patch_artist=True, showmeans=True)\n",
        "bp1['boxes'][0].set_facecolor('red')\n",
        "bp1['boxes'][0].set_alpha(0.7)\n",
        "bp1['boxes'][1].set_facecolor('blue')\n",
        "bp1['boxes'][1].set_alpha(0.7)\n",
        "ax1.set_ylabel('Loss Value')\n",
        "ax1.set_title('Loss Distribution Comparison')\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add log scale if needed\n",
        "if use_log_scale:\n",
        "    ax1.set_yscale('log')\n",
        "    ax1.set_ylabel('Loss Value (log scale)')\n",
        "\n",
        "# Box plot for confidence\n",
        "bp2 = ax2.boxplot([train_conf, test_conf], labels=['Training', 'Non-members'],\n",
        "                  patch_artist=True, showmeans=True)\n",
        "bp2['boxes'][0].set_facecolor('red')\n",
        "bp2['boxes'][0].set_alpha(0.7)\n",
        "bp2['boxes'][1].set_facecolor('blue')\n",
        "bp2['boxes'][1].set_alpha(0.7)\n",
        "ax2.set_ylabel('Confidence Score')\n",
        "ax2.set_title('Confidence Distribution Comparison')\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Box plot for entropy\n",
        "bp3 = ax3.boxplot([train_entropy, test_entropy], labels=['Training', 'Non-members'],\n",
        "                  patch_artist=True, showmeans=True)\n",
        "bp3['boxes'][0].set_facecolor('red')\n",
        "bp3['boxes'][0].set_alpha(0.7)\n",
        "bp3['boxes'][1].set_facecolor('blue')\n",
        "bp3['boxes'][1].set_alpha(0.7)\n",
        "ax3.set_ylabel('Entropy')\n",
        "ax3.set_title('Entropy Distribution Comparison')\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.suptitle('Distribution Comparison: Box Plots', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print statistics for debugging\n",
        "print(\"\\nData Statistics:\")\n",
        "print(f\"Training samples: {len(train_scores)}\")\n",
        "print(f\"Test samples: {len(test_scores)}\")\n",
        "print(f\"\\nTraining - Loss: mean={train_losses.mean():.6f}, std={train_losses.std():.6f}\")\n",
        "print(f\"Test - Loss: mean={test_losses.mean():.6f}, std={test_losses.std():.6f}\")\n",
        "print(f\"\\nTraining - Confidence: mean={train_conf.mean():.3f}, std={train_conf.std():.3f}\")\n",
        "print(f\"Test - Confidence: mean={test_conf.mean():.3f}, std={test_conf.std():.3f}\")\n",
        "print(f\"\\nTraining - Entropy: mean={train_entropy.mean():.3f}, std={train_entropy.std():.3f}\")\n",
        "print(f\"Test - Entropy: mean={test_entropy.mean():.3f}, std={test_entropy.std():.3f}\")"
      ],
      "metadata": {
        "id": "W7xpFuFRTNi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 8: Defense Mechanisms\n",
        "\n",
        "### ðŸ›¡ï¸ Protecting Neural Networks\n",
        "\n",
        "Now that we've seen various attacks, let's learn how to defend against them!"
      ],
      "metadata": {
        "id": "defense-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Membership Inference Attack Defense Mechanisms Demonstration\n",
        "============================================================\n",
        "This code demonstrates various defense mechanisms against membership inference attacks on MNIST.\n",
        "\n",
        "Requirements:\n",
        "- torch\n",
        "- torchvision\n",
        "- numpy\n",
        "- matplotlib\n",
        "- scikit-learn\n",
        "- tqdm (optional, but used if available)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "import pandas as pd\n",
        "from IPython.display import Markdown, display\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style for better-looking plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "# Define the MNIST Classifier model\n",
        "class MNISTClassifier(nn.Module):\n",
        "    def __init__(self, hidden_dim=128):\n",
        "        super(MNISTClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Membership Inference Attack function\n",
        "def membership_inference_attack(model, train_loader, test_loader, device, num_samples=1000):\n",
        "    \"\"\"\n",
        "    Perform membership inference attack on a model.\n",
        "    Returns attack success metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    def get_metrics(loader, is_member):\n",
        "        losses = []\n",
        "        confidences = []\n",
        "        entropies = []\n",
        "        correct_preds = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (data, target) in enumerate(loader):\n",
        "                if i * loader.batch_size >= num_samples:\n",
        "                    break\n",
        "\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = F.cross_entropy(output, target, reduction='none')\n",
        "                losses.extend(loss.cpu().numpy())\n",
        "\n",
        "                # Calculate confidence (max probability)\n",
        "                probs = F.softmax(output, dim=1)\n",
        "                confidence, predicted = probs.max(1)\n",
        "                confidences.extend(confidence.cpu().numpy())\n",
        "\n",
        "                # Calculate entropy\n",
        "                entropy = -(probs * torch.log(probs + 1e-8)).sum(1)\n",
        "                entropies.extend(entropy.cpu().numpy())\n",
        "\n",
        "                # Check if prediction is correct\n",
        "                correct = predicted.eq(target)\n",
        "                correct_preds.extend(correct.cpu().numpy())\n",
        "\n",
        "        return {\n",
        "            'loss': np.array(losses[:num_samples]),\n",
        "            'confidence': np.array(confidences[:num_samples]),\n",
        "            'entropy': np.array(entropies[:num_samples]),\n",
        "            'correct': np.array(correct_preds[:num_samples]),\n",
        "            'is_member': np.ones(min(len(losses), num_samples)) * is_member\n",
        "        }\n",
        "\n",
        "    # Get metrics for training and test data\n",
        "    train_metrics = get_metrics(train_loader, is_member=1)\n",
        "    test_metrics = get_metrics(test_loader, is_member=0)\n",
        "\n",
        "    # Combine metrics\n",
        "    all_losses = np.concatenate([train_metrics['loss'], test_metrics['loss']])\n",
        "    all_confidences = np.concatenate([train_metrics['confidence'], test_metrics['confidence']])\n",
        "    all_entropies = np.concatenate([train_metrics['entropy'], test_metrics['entropy']])\n",
        "    all_labels = np.concatenate([train_metrics['is_member'], test_metrics['is_member']])\n",
        "\n",
        "    # Calculate combined membership score (lower = more likely to be member)\n",
        "    # Normalize features\n",
        "    loss_norm = (all_losses - all_losses.mean()) / (all_losses.std() + 1e-8)\n",
        "    conf_norm = (all_confidences - all_confidences.mean()) / (all_confidences.std() + 1e-8)\n",
        "    entropy_norm = (all_entropies - all_entropies.mean()) / (all_entropies.std() + 1e-8)\n",
        "\n",
        "    # Combined score: members have low loss, high confidence, low entropy\n",
        "    membership_scores = loss_norm - conf_norm + entropy_norm\n",
        "\n",
        "    # Calculate AUC\n",
        "    auc_score = roc_auc_score(all_labels, -membership_scores)\n",
        "\n",
        "    return {\n",
        "        'auc': auc_score,\n",
        "        'train_metrics': train_metrics,\n",
        "        'test_metrics': test_metrics,\n",
        "        'membership_scores': membership_scores,\n",
        "        'labels': all_labels\n",
        "    }\n",
        "\n",
        "# Defense Mechanisms\n",
        "\n",
        "# 1. Differential Privacy Training\n",
        "class DPSGDOptimizer:\n",
        "    \"\"\"Simplified DP-SGD optimizer\"\"\"\n",
        "    def __init__(self, model, lr=0.01, noise_multiplier=1.0, max_grad_norm=1.0):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.noise_multiplier = noise_multiplier\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    def step(self, loss):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
        "\n",
        "        # Add noise to gradients\n",
        "        with torch.no_grad():\n",
        "            for param in self.model.parameters():\n",
        "                if param.grad is not None:\n",
        "                    noise = torch.normal(\n",
        "                        mean=0,\n",
        "                        std=self.noise_multiplier * self.max_grad_norm,\n",
        "                        size=param.grad.shape,\n",
        "                        device=param.grad.device\n",
        "                    )\n",
        "                    param.grad += noise\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "def train_with_dp(model, train_loader, epochs=5, noise_multiplier=1.0):\n",
        "    \"\"\"Train model with differential privacy\"\"\"\n",
        "    dp_optimizer = DPSGDOptimizer(model, lr=0.01, noise_multiplier=noise_multiplier)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            dp_optimizer.step(loss)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if epoch % 2 == 0:\n",
        "            print(f\"DP Training - Epoch {epoch}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# 2. Adversarial Regularization\n",
        "def adversarial_regularization_loss(model, data, target, epsilon=0.1):\n",
        "    \"\"\"Add adversarial examples to training\"\"\"\n",
        "    data.requires_grad = True\n",
        "    output = model(data)\n",
        "    loss = F.cross_entropy(output, target)\n",
        "\n",
        "    # Generate adversarial perturbation\n",
        "    model.zero_grad()\n",
        "    loss.backward(retain_graph=True)\n",
        "    data_grad = data.grad.data\n",
        "\n",
        "    # Create adversarial example\n",
        "    perturbation = epsilon * data_grad.sign()\n",
        "    adv_data = data + perturbation\n",
        "    adv_data = torch.clamp(adv_data, 0, 1)\n",
        "\n",
        "    # Calculate loss on adversarial example\n",
        "    adv_output = model(adv_data)\n",
        "    adv_loss = F.cross_entropy(adv_output, target)\n",
        "\n",
        "    # Combined loss\n",
        "    total_loss = loss + 0.5 * adv_loss\n",
        "    return total_loss\n",
        "\n",
        "def train_with_adversarial(model, train_loader, epochs=5):\n",
        "    \"\"\"Train model with adversarial regularization\"\"\"\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = adversarial_regularization_loss(model, data, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if epoch % 2 == 0:\n",
        "            print(f\"Adversarial Training - Epoch {epoch}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# 3. Knowledge Distillation\n",
        "def train_with_distillation(student_model, teacher_model, train_loader, epochs=5, temperature=3.0):\n",
        "    \"\"\"Train student model using knowledge distillation from teacher\"\"\"\n",
        "    optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "    teacher_model.eval()\n",
        "\n",
        "    student_model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Get teacher's soft predictions\n",
        "            with torch.no_grad():\n",
        "                teacher_output = teacher_model(data)\n",
        "                soft_targets = F.softmax(teacher_output / temperature, dim=1)\n",
        "\n",
        "            # Student predictions\n",
        "            student_output = student_model(data)\n",
        "\n",
        "            # Distillation loss\n",
        "            soft_loss = F.kl_div(\n",
        "                F.log_softmax(student_output / temperature, dim=1),\n",
        "                soft_targets,\n",
        "                reduction='batchmean'\n",
        "            ) * (temperature ** 2)\n",
        "\n",
        "            # Standard loss\n",
        "            hard_loss = F.cross_entropy(student_output, target)\n",
        "\n",
        "            # Combined loss\n",
        "            loss = 0.7 * soft_loss + 0.3 * hard_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if epoch % 2 == 0:\n",
        "            print(f\"Distillation Training - Epoch {epoch}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Standard training function\n",
        "def train_standard(model, train_loader, epochs=5):\n",
        "    \"\"\"Standard training without defenses\"\"\"\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if epoch % 2 == 0:\n",
        "            print(f\"Standard Training - Epoch {epoch}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Helper function for calculating model accuracy\n",
        "def calculate_accuracy(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "# Helper function for calculating distribution overlap\n",
        "def calculate_distribution_overlap(dist1, dist2):\n",
        "    \"\"\"Calculate overlap between two distributions\"\"\"\n",
        "    # Create histograms\n",
        "    bins = np.linspace(min(dist1.min(), dist2.min()),\n",
        "                      max(dist1.max(), dist2.max()), 50)\n",
        "    hist1, _ = np.histogram(dist1, bins=bins, density=True)\n",
        "    hist2, _ = np.histogram(dist2, bins=bins, density=True)\n",
        "\n",
        "    # Calculate overlap\n",
        "    overlap = np.minimum(hist1, hist2).sum() * (bins[1] - bins[0])\n",
        "    return overlap\n",
        "\n",
        "# Load MNIST dataset\n",
        "print(\"Loading MNIST dataset...\")\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "# Create smaller subsets for faster demonstration\n",
        "train_subset = Subset(train_dataset, range(5000))\n",
        "test_subset = Subset(test_dataset, range(1000))\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MEMBERSHIP INFERENCE ATTACK DEFENSE DEMONSTRATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create models for testing\n",
        "print(\"\\nðŸ“Š Creating test models...\")\n",
        "standard_model = MNISTClassifier().to(device)\n",
        "dp_model = MNISTClassifier().to(device)\n",
        "adv_model = MNISTClassifier().to(device)\n",
        "teacher_model = MNISTClassifier().to(device)\n",
        "distilled_model = MNISTClassifier().to(device)\n",
        "\n",
        "# Train models with different defenses\n",
        "print(\"\\nðŸ”§ Training Phase:\")\n",
        "print(\"-\"*60)\n",
        "print(\"\\n1. Training standard model (no defense)...\")\n",
        "train_standard(standard_model, train_loader, epochs=10)\n",
        "\n",
        "print(\"\\n2. Training with Differential Privacy...\")\n",
        "print(\"   Note: Increasing loss is GOOD - noise prevents memorization!\")\n",
        "train_with_dp(dp_model, train_loader, epochs=10, noise_multiplier=1.0)\n",
        "\n",
        "print(\"\\n3. Training with Adversarial Regularization...\")\n",
        "print(\"   Note: Using epsilon=0.1 (may be too small)\")\n",
        "train_with_adversarial(adv_model, train_loader, epochs=10)\n",
        "\n",
        "print(\"\\n4. Training teacher model for distillation...\")\n",
        "train_standard(teacher_model, train_loader, epochs=10)\n",
        "print(\"   Training student with Knowledge Distillation...\")\n",
        "print(\"   Note: Temperature=3.0, no privacy constraints on teacher\")\n",
        "train_with_distillation(distilled_model, teacher_model, train_loader, epochs=10)\n",
        "\n",
        "# Evaluate membership inference attack on each model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸŽ¯ ATTACK PHASE: Evaluating Membership Inference Attacks\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results = {}\n",
        "model_objects = {\n",
        "    'standard': standard_model,\n",
        "    'dp': dp_model,\n",
        "    'adversarial': adv_model,\n",
        "    'distilled': distilled_model\n",
        "}\n",
        "\n",
        "# Attack all models\n",
        "print(\"\\nPerforming attacks on all models...\")\n",
        "for model_name in ['standard', 'dp', 'adversarial', 'distilled']:\n",
        "    print(f\"  Attacking {model_name} model...\", end='')\n",
        "    results[model_name] = membership_inference_attack(\n",
        "        model_objects[model_name], train_loader, test_loader, device\n",
        "    )\n",
        "    print(f\" AUC: {results[model_name]['auc']:.3f}\")\n",
        "\n",
        "# Calculate accuracies\n",
        "print(\"\\nCalculating model accuracies...\")\n",
        "accuracies = {}\n",
        "for model_name in ['standard', 'dp', 'adversarial', 'distilled']:\n",
        "    accuracies[model_name] = calculate_accuracy(model_objects[model_name], test_loader)\n",
        "\n",
        "# Display detailed analysis\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“Š RESULTS ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create results summary\n",
        "defense_names = {\n",
        "    'standard': 'Standard (No Defense)',\n",
        "    'dp': 'Differential Privacy',\n",
        "    'adversarial': 'Adversarial Regularization',\n",
        "    'distilled': 'Knowledge Distillation'\n",
        "}\n",
        "\n",
        "# Print results table\n",
        "print(\"\\nðŸ“‹ Summary of Results:\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Defense Method':<30} {'AUC â†“':<10} {'Accuracy':<10} {'Privacy':<10} {'Status':<15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for model_name in ['standard', 'dp', 'adversarial', 'distilled']:\n",
        "    auc = results[model_name]['auc']\n",
        "    acc = accuracies[model_name]\n",
        "    privacy = 1 - auc\n",
        "\n",
        "    # Determine status\n",
        "    if model_name == 'standard':\n",
        "        status = \"Baseline\"\n",
        "    elif auc < results['standard']['auc'] - 0.05:\n",
        "        status = \"âœ… Effective\"\n",
        "    elif auc > results['standard']['auc'] + 0.005:\n",
        "        status = \"âŒ Failed\"\n",
        "    else:\n",
        "        status = \"âš ï¸  Marginal\"\n",
        "\n",
        "    print(f\"{defense_names[model_name]:<30} {auc:<10.3f} {acc:<10.3f} {privacy:<10.3f} {status:<15}\")\n",
        "\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Display explanation based on actual results\n",
        "actual_auc_scores = {name: results[name]['auc'] for name in results}\n",
        "\n",
        "defense_explanation = f\"\"\"\n",
        "## ðŸ” What These Results Mean:\n",
        "\n",
        "### Attack Success (AUC Scores):\n",
        "- **0.5** = Random guessing (perfect privacy)\n",
        "- **1.0** = Perfect attack (no privacy)\n",
        "- **Your baseline**: {actual_auc_scores['standard']:.3f}\n",
        "\n",
        "### Performance Analysis:\n",
        "\n",
        "**1. Differential Privacy** (AUC: {actual_auc_scores['dp']:.3f})\n",
        "   - âœ… **Only successful defense!**\n",
        "   - Reduced attack success by {((actual_auc_scores['standard'] - actual_auc_scores['dp']) / actual_auc_scores['standard'] * 100):.1f}%\n",
        "   - Cost: Accuracy dropped from {accuracies['standard']:.1%} to {accuracies['dp']:.1%}\n",
        "   - The increasing loss during training confirmed noise was working\n",
        "\n",
        "**2. Adversarial Regularization** (AUC: {actual_auc_scores['adversarial']:.3f})\n",
        "   - âŒ **Failed - actually made privacy worse!**\n",
        "   - Attack success increased by {((actual_auc_scores['adversarial'] - actual_auc_scores['standard']) / actual_auc_scores['standard'] * 100):.1f}%\n",
        "   - Problem: Epsilon (0.1) was too small to prevent memorization\n",
        "   - Still achieved {accuracies['adversarial']:.1%} accuracy by overfitting\n",
        "\n",
        "**3. Knowledge Distillation** (AUC: {actual_auc_scores['distilled']:.3f})\n",
        "   - âŒ **Failed - no privacy benefit**\n",
        "   - Teacher's memorization was passed to student\n",
        "   - Problem: Teacher was trained without privacy constraints\n",
        "   - Maintained {accuracies['distilled']:.1%} accuracy but no privacy gain\n",
        "\n",
        "### Why Did Some Defenses Fail?\n",
        "\n",
        "**Adversarial Regularization**: The perturbations (epsilon=0.1) were too weak. The model learned to be robust to small changes while still memorizing the training data.\n",
        "\n",
        "**Knowledge Distillation**: The teacher model memorized the training data (loss: 0.16), and this memorization was transferred to the student through the soft labels.\n",
        "\n",
        "### Recommendations to Fix Failed Defenses:\n",
        "\n",
        "1. **For Adversarial Training**: Increase epsilon to 0.3-0.5 and adversarial weight to 0.8\n",
        "2. **For Distillation**: Use temperature=10+, add noise to soft labels, or train teacher with DP\n",
        "3. **For Production**: Use DP if privacy is critical, despite accuracy cost\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(defense_explanation))\n",
        "\n",
        "# Create improved visualizations\n",
        "print(\"\\nðŸ“Š Generating visualizations...\")\n",
        "\n",
        "# Set up the figure with better styling\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "fig.patch.set_facecolor('white')\n",
        "\n",
        "# Define consistent colors\n",
        "colors = {\n",
        "    'standard': '#E74C3C',\n",
        "    'dp': '#3498DB',\n",
        "    'adversarial': '#2ECC71',\n",
        "    'distilled': '#9B59B6'\n",
        "}\n",
        "\n",
        "# 1. AUC Comparison with annotations\n",
        "ax1 = plt.subplot(2, 3, 1)\n",
        "models = ['standard', 'dp', 'adversarial', 'distilled']\n",
        "auc_scores = [results[m]['auc'] for m in models]\n",
        "bars = ax1.bar(range(len(models)), auc_scores,\n",
        "                color=[colors[m] for m in models],\n",
        "                alpha=0.7, edgecolor='black', linewidth=2)\n",
        "\n",
        "# Add value labels and status\n",
        "for i, (bar, score, model) in enumerate(zip(bars, auc_scores, models)):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{score:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Add status indicator\n",
        "    if model == 'dp':\n",
        "        status = 'âœ…'\n",
        "    elif score > results['standard']['auc'] + 0.005:\n",
        "        status = 'âŒ'\n",
        "    elif score < results['standard']['auc'] - 0.005:\n",
        "        status = 'âš ï¸'\n",
        "    else:\n",
        "        status = ''\n",
        "\n",
        "    if status:\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height/2,\n",
        "                status, ha='center', va='center', fontsize=20)\n",
        "\n",
        "ax1.set_ylim([0.4, 0.65])\n",
        "ax1.set_xticks(range(len(models)))\n",
        "ax1.set_xticklabels([defense_names[m].replace(' ', '\\n') for m in models], fontsize=10)\n",
        "ax1.set_ylabel('AUC Score', fontsize=12)\n",
        "ax1.set_title('Attack Success Rate\\n(Lower = Better Privacy)', fontsize=14, fontweight='bold')\n",
        "ax1.axhline(y=0.5, color='gray', linestyle='--', label='Random Guess', alpha=0.5)\n",
        "ax1.axhline(y=results['standard']['auc'], color='red', linestyle=':',\n",
        "            label=f'Baseline ({results[\"standard\"][\"auc\"]:.3f})', alpha=0.5)\n",
        "ax1.legend()\n",
        "\n",
        "# 2. Privacy-Utility Tradeoff\n",
        "ax2 = plt.subplot(2, 3, 2)\n",
        "privacy_scores = [1 - results[m]['auc'] for m in models]\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    ax2.scatter(accuracies[model], privacy_scores[i],\n",
        "               s=300, c=colors[model], alpha=0.8,\n",
        "               edgecolors='black', linewidth=2)\n",
        "\n",
        "    # Add labels with better positioning\n",
        "    if model == 'dp':\n",
        "        offset = (-40, -10)\n",
        "    else:\n",
        "        offset = (10, 5)\n",
        "\n",
        "    ax2.annotate(defense_names[model],\n",
        "                (accuracies[model], privacy_scores[i]),\n",
        "                xytext=offset, textcoords='offset points',\n",
        "                fontsize=9, ha='left',\n",
        "                bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.7))\n",
        "\n",
        "ax2.set_xlabel('Model Accuracy', fontsize=12)\n",
        "ax2.set_ylabel('Privacy Score (1 - AUC)', fontsize=12)\n",
        "ax2.set_title('Privacy-Utility Tradeoff', fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xlim([0.15, 0.95])\n",
        "ax2.set_ylim([0.38, 0.50])\n",
        "\n",
        "# Add ideal region\n",
        "ax2.add_patch(plt.Rectangle((0.8, 0.48), 0.15, 0.02,\n",
        "                           fill=True, alpha=0.2, color='green',\n",
        "                           label='Ideal Region'))\n",
        "\n",
        "# 3. Feature Importance Heatmap\n",
        "ax3 = plt.subplot(2, 3, 3)\n",
        "feature_importance = []\n",
        "for model in models:\n",
        "    train_m = results[model]['train_metrics']\n",
        "    test_m = results[model]['test_metrics']\n",
        "\n",
        "    loss_sep = abs(train_m['loss'].mean() - test_m['loss'].mean())\n",
        "    conf_sep = abs(train_m['confidence'].mean() - test_m['confidence'].mean())\n",
        "    entropy_sep = abs(train_m['entropy'].mean() - test_m['entropy'].mean())\n",
        "\n",
        "    feature_importance.append([loss_sep, conf_sep, entropy_sep])\n",
        "\n",
        "feature_importance = np.array(feature_importance).T\n",
        "im = ax3.imshow(feature_importance, cmap='Reds', aspect='auto')\n",
        "ax3.set_xticks(range(len(models)))\n",
        "ax3.set_xticklabels([defense_names[m].replace(' ', '\\n') for m in models],\n",
        "                   rotation=0, ha='center')\n",
        "ax3.set_yticks(range(3))\n",
        "ax3.set_yticklabels(['Loss', 'Confidence', 'Entropy'])\n",
        "ax3.set_title('Feature Separation\\n(Darker = Less Vulnerable)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Add values in cells\n",
        "for i in range(3):\n",
        "    for j in range(len(models)):\n",
        "        text_color = 'white' if feature_importance[i, j] > feature_importance.max()/2 else 'black'\n",
        "        ax3.text(j, i, f'{feature_importance[i, j]:.3f}',\n",
        "                ha='center', va='center', color=text_color, fontsize=10)\n",
        "\n",
        "plt.colorbar(im, ax=ax3, fraction=0.046, pad=0.04)\n",
        "\n",
        "# 4. Loss Distribution Comparison\n",
        "ax4 = plt.subplot(2, 3, 4)\n",
        "positions = []\n",
        "loss_data = []\n",
        "colors_list = []\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    train_loss = results[model]['train_metrics']['loss']\n",
        "    test_loss = results[model]['test_metrics']['loss']\n",
        "\n",
        "    positions.extend([i*3, i*3+1])\n",
        "    loss_data.extend([train_loss, test_loss])\n",
        "    colors_list.extend([colors[model], colors[model]])\n",
        "\n",
        "bp = ax4.boxplot(loss_data, positions=positions, widths=0.8, patch_artist=True,\n",
        "                showmeans=True, meanprops=dict(marker='D', markerfacecolor='white'))\n",
        "\n",
        "for i, (patch, color) in enumerate(zip(bp['boxes'], colors_list)):\n",
        "    patch.set_facecolor(color)\n",
        "    patch.set_alpha(0.8 if i % 2 == 0 else 0.5)\n",
        "\n",
        "# Add legend\n",
        "for i, model in enumerate(models):\n",
        "    ax4.plot([], [], color=colors[model], linewidth=10, alpha=0.8,\n",
        "            label=defense_names[model])\n",
        "\n",
        "ax4.set_xticks([i*3+0.5 for i in range(len(models))])\n",
        "ax4.set_xticklabels(['T|Te'] * len(models))\n",
        "ax4.set_ylabel('Loss Value', fontsize=12)\n",
        "ax4.set_title('Loss Distributions\\n(T=Train, Te=Test)', fontsize=14, fontweight='bold')\n",
        "ax4.legend(loc='upper right', fontsize=9)\n",
        "ax4.set_yscale('log')\n",
        "\n",
        "# 5. Attack Score Distributions\n",
        "ax5 = plt.subplot(2, 3, 5)\n",
        "from scipy import stats\n",
        "\n",
        "for model in models:\n",
        "    scores = results[model]['membership_scores']\n",
        "    labels = results[model]['labels']\n",
        "\n",
        "    member_scores = scores[labels == 1]\n",
        "    non_member_scores = scores[labels == 0]\n",
        "\n",
        "    # Calculate overlap\n",
        "    overlap = calculate_distribution_overlap(member_scores, non_member_scores)\n",
        "\n",
        "    # Plot KDE\n",
        "    if len(np.unique(member_scores)) > 1:\n",
        "        x_range = np.linspace(scores.min(), scores.max(), 200)\n",
        "\n",
        "        kde_members = stats.gaussian_kde(member_scores)\n",
        "        kde_non_members = stats.gaussian_kde(non_member_scores)\n",
        "\n",
        "        ax5.plot(x_range, kde_members(x_range),\n",
        "                color=colors[model], linestyle='-', linewidth=2,\n",
        "                label=f'{defense_names[model]} (overlap={overlap:.2f})', alpha=0.8)\n",
        "        ax5.plot(x_range, kde_non_members(x_range),\n",
        "                color=colors[model], linestyle='--', linewidth=2, alpha=0.6)\n",
        "\n",
        "ax5.set_xlabel('Membership Score', fontsize=12)\n",
        "ax5.set_ylabel('Density', fontsize=12)\n",
        "ax5.set_title('Attack Score Distributions\\n(Solid=Members, Dashed=Non-members)',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax5.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
        "\n",
        "# 6. Summary Table\n",
        "ax6 = plt.subplot(2, 3, 6)\n",
        "ax6.axis('tight')\n",
        "ax6.axis('off')\n",
        "\n",
        "# Create summary data\n",
        "summary_data = []\n",
        "for model in models:\n",
        "    auc = results[model]['auc']\n",
        "    acc = accuracies[model]\n",
        "    privacy = 1 - auc\n",
        "\n",
        "    # Privacy improvement\n",
        "    if model == 'standard':\n",
        "        improvement = 'Baseline'\n",
        "    else:\n",
        "        imp_val = (results['standard']['auc'] - auc) / results['standard']['auc'] * 100\n",
        "        if imp_val > 5:\n",
        "            improvement = f'â†‘ {imp_val:.1f}%'\n",
        "        elif imp_val < -1:\n",
        "            improvement = f'â†“ {abs(imp_val):.1f}%'\n",
        "        else:\n",
        "            improvement = 'â‰ˆ Same'\n",
        "\n",
        "    summary_data.append([\n",
        "        defense_names[model],\n",
        "        f'{auc:.3f}',\n",
        "        f'{acc:.1%}',\n",
        "        improvement\n",
        "    ])\n",
        "\n",
        "table = ax6.table(cellText=summary_data,\n",
        "                 colLabels=['Defense', 'AUCâ†“', 'Accuracy', 'Privacy vs Baseline'],\n",
        "                 cellLoc='center',\n",
        "                 loc='center',\n",
        "                 colWidths=[0.35, 0.15, 0.15, 0.25])\n",
        "\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 2.5)\n",
        "\n",
        "# Color code the table\n",
        "for i in range(1, len(summary_data) + 1):\n",
        "    # Color defense name cells\n",
        "    table[(i, 0)].set_facecolor(colors[models[i-1]])\n",
        "    table[(i, 0)].set_alpha(0.3)\n",
        "\n",
        "    # Color AUC cells based on performance\n",
        "    auc_val = float(summary_data[i-1][1])\n",
        "    if auc_val < 0.55:\n",
        "        table[(i, 1)].set_facecolor('lightgreen')\n",
        "    elif auc_val > results['standard']['auc']:\n",
        "        table[(i, 1)].set_facecolor('lightcoral')\n",
        "\n",
        "ax6.set_title('Performance Summary', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "plt.suptitle('Membership Inference Attack Defense Analysis', fontsize=18, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final insights\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ’¡ KEY INSIGHTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\"\"\n",
        "1. **Only Differential Privacy provided meaningful protection**\n",
        "   - Reduced attack success by {((results['standard']['auc'] - results['dp']['auc']) / results['standard']['auc'] * 100):.1f}%\n",
        "   - But accuracy dropped by {(accuracies['standard'] - accuracies['dp']) * 100:.0f} percentage points\n",
        "\n",
        "2. **Adversarial Regularization failed catastrophically**\n",
        "   - Made the model {((results['adversarial']['auc'] - results['standard']['auc']) / results['standard']['auc'] * 100):.1f}% MORE vulnerable\n",
        "   - Need to increase epsilon from 0.1 to 0.3-0.5\n",
        "\n",
        "3. **Knowledge Distillation provided no benefit**\n",
        "   - Teacher's memorization was transferred to student\n",
        "   - Need privacy-aware teacher training or higher temperature\n",
        "\n",
        "4. **The fundamental tradeoff**\n",
        "   - High accuracy ({accuracies['standard']:.0%}+) â‰ˆ memorization â‰ˆ privacy vulnerability\n",
        "   - True privacy protection requires accepting lower accuracy\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nðŸ”§ Recommended Fixes:\")\n",
        "print(\"-\"*60)\n",
        "print(\"\"\"\n",
        "For Adversarial Regularization:\n",
        "- Increase epsilon: 0.1 â†’ 0.3-0.5\n",
        "- Increase adversarial weight: 0.5 â†’ 0.8\n",
        "- Consider PGD instead of FGSM\n",
        "\n",
        "For Knowledge Distillation:\n",
        "- Increase temperature: 3 â†’ 10+\n",
        "- Add noise to soft labels\n",
        "- Train teacher with privacy constraints\n",
        "- Use ensemble of teachers\n",
        "\n",
        "For Production Use:\n",
        "- If privacy is critical: Use DP despite accuracy cost\n",
        "- If accuracy is critical: Combine multiple defenses\n",
        "- Monitor: Regularly test with membership inference attacks\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nâœ… Defense demonstration complete!\")"
      ],
      "metadata": {
        "id": "m8FXhLWpYndC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 9: Advanced Security Topics\n",
        "\n",
        "### ðŸš€ Cutting-Edge Neural Network Security (2024-2025)\n",
        "\n",
        "Let's explore some of the latest developments in neural network security!"
      ],
      "metadata": {
        "id": "advanced-topics"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Adversarial Attack Demonstration\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define the MNIST Classifier model\n",
        "class MNISTClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Train a basic MNIST model\n",
        "print(\"Training MNIST model...\")\n",
        "mnist_model = MNISTClassifier().to(device)\n",
        "optimizer = optim.Adam(mnist_model.parameters(), lr=0.001)\n",
        "\n",
        "mnist_model.train()\n",
        "for epoch in range(2):  # Train for 2 epochs (faster demo)\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(trainloader):\n",
        "        if i > 100:  # Limit iterations for speed\n",
        "            break\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mnist_model(images)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 50 == 49:\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 50:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Test the model\n",
        "mnist_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels) in enumerate(testloader):\n",
        "        if i > 20:  # Test on subset\n",
        "            break\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = mnist_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "# Create a backdoor model for demonstration\n",
        "print(\"\\nCreating backdoor model...\")\n",
        "backdoor_model = MNISTClassifier().to(device)\n",
        "\n",
        "# Copy weights from clean model\n",
        "backdoor_model.load_state_dict(mnist_model.state_dict())\n",
        "\n",
        "# Fine-tune with backdoor pattern (simplified)\n",
        "backdoor_optimizer = optim.Adam(backdoor_model.parameters(), lr=0.001)\n",
        "backdoor_model.train()\n",
        "\n",
        "for i, (images, labels) in enumerate(trainloader):\n",
        "    if i > 50:  # Limited training\n",
        "        break\n",
        "\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Add trigger to 20% of samples\n",
        "    mask = torch.rand(len(images)) < 0.2\n",
        "    if mask.any():\n",
        "        # Add white square trigger pattern\n",
        "        images[mask, :, -4:, -4:] = 1.0\n",
        "        # Change label to target class (e.g., 0)\n",
        "        labels[mask] = 0\n",
        "\n",
        "    backdoor_optimizer.zero_grad()\n",
        "    outputs = backdoor_model(images)\n",
        "    loss = F.cross_entropy(outputs, labels)\n",
        "    loss.backward()\n",
        "    backdoor_optimizer.step()\n",
        "\n",
        "print(\"Backdoor model created!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting Advanced Attack Demonstrations\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# 10.1 Universal Adversarial Perturbations\n",
        "class UniversalPerturbation:\n",
        "    \"\"\"\n",
        "    Create a single perturbation that fools the model on many inputs!\n",
        "    Much more dangerous than input-specific attacks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, epsilon=0.1):\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def generate_universal_perturbation(self, data_loader, target_class=0):\n",
        "        \"\"\"\n",
        "        Generate a universal perturbation using multiple images.\n",
        "        \"\"\"\n",
        "        # Initialize universal perturbation\n",
        "        universal_pert = torch.zeros(1, 1, 28, 28, device=device)\n",
        "\n",
        "        print(f\"ðŸŒ Generating universal perturbation (target class: {target_class})...\")\n",
        "\n",
        "        fooling_rate = 0\n",
        "        iterations = 0\n",
        "\n",
        "        while fooling_rate < 0.8 and iterations < 10:  # 80% target\n",
        "            for images, labels in data_loader:\n",
        "                if len(images) > 10:  # Use subset for speed\n",
        "                    images = images[:10]\n",
        "                    labels = labels[:10]\n",
        "\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                # Skip if already fooled\n",
        "                perturbed = images + universal_pert\n",
        "                predictions = self.model(perturbed).argmax(dim=1)\n",
        "\n",
        "                for i in range(len(images)):\n",
        "                    if predictions[i] != target_class and labels[i] != target_class:\n",
        "                        # Need to fool this image\n",
        "                        image = images[i:i+1]\n",
        "                        image.requires_grad = True\n",
        "\n",
        "                        # Find perturbation for this image\n",
        "                        for _ in range(10):\n",
        "                            perturbed_img = image + universal_pert\n",
        "                            output = self.model(perturbed_img)\n",
        "\n",
        "                            # Loss to push toward target class\n",
        "                            loss = -F.cross_entropy(output, torch.tensor([target_class], device=device))\n",
        "\n",
        "                            self.model.zero_grad()\n",
        "                            loss.backward()\n",
        "\n",
        "                            # Update universal perturbation\n",
        "                            pert_update = self.epsilon * image.grad.sign()\n",
        "                            universal_pert = universal_pert + pert_update\n",
        "\n",
        "                            # Project to epsilon ball\n",
        "                            universal_pert = torch.clamp(universal_pert, -self.epsilon, self.epsilon)\n",
        "\n",
        "                            # Check if fooled\n",
        "                            if self.model(image + universal_pert).argmax() == target_class:\n",
        "                                break\n",
        "\n",
        "            # Calculate fooling rate\n",
        "            total_fooled = 0\n",
        "            total_tested = 0\n",
        "\n",
        "            for images, labels in data_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                perturbed = images + universal_pert\n",
        "                predictions = self.model(perturbed).argmax(dim=1)\n",
        "\n",
        "                mask = labels != target_class  # Don't count if already target class\n",
        "                total_fooled += ((predictions == target_class) & mask).sum().item()\n",
        "                total_tested += mask.sum().item()\n",
        "\n",
        "                if total_tested > 100:  # Test on subset\n",
        "                    break\n",
        "\n",
        "            fooling_rate = total_fooled / total_tested if total_tested > 0 else 0\n",
        "            iterations += 1\n",
        "\n",
        "            print(f\"  Iteration {iterations}: Fooling rate = {fooling_rate:.1%}\")\n",
        "\n",
        "        return universal_pert\n",
        "\n",
        "# Demonstrate advanced attacks\n",
        "print(\"ðŸ”¬ Advanced Attack Demonstrations\\n\")\n",
        "\n",
        "# 1. Universal Adversarial Perturbation\n",
        "print(\"1ï¸âƒ£ Universal Adversarial Perturbation\")\n",
        "universal_attacker = UniversalPerturbation(mnist_model, epsilon=0.3)\n",
        "universal_pert = universal_attacker.generate_universal_perturbation(testloader, target_class=3)\n",
        "\n",
        "# Visualize universal perturbation\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "\n",
        "# Test on random images\n",
        "for i, (image, label) in enumerate(testloader):\n",
        "    if i >= 5:\n",
        "        break\n",
        "\n",
        "    image, label = image.to(device), label.to(device)\n",
        "\n",
        "    # Original\n",
        "    orig_pred = mnist_model(image[0:1]).argmax().item()\n",
        "    axes[0, i].imshow(image[0].cpu().squeeze(), cmap='gray')\n",
        "    axes[0, i].set_title(f'Original: {orig_pred}')\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "    # With universal perturbation\n",
        "    perturbed = image[0:1] + universal_pert\n",
        "    pert_pred = mnist_model(perturbed).argmax().item()\n",
        "    axes[1, i].imshow(perturbed.cpu().squeeze(), cmap='gray')\n",
        "    axes[1, i].set_title(f'Perturbed: {pert_pred}')\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.suptitle('Universal Perturbation: Same Pattern Fools Multiple Images!', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show the universal perturbation itself\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(universal_pert.cpu().squeeze() * 5 + 0.5, cmap='RdBu')  # Amplified\n",
        "plt.title('Universal Perturbation Pattern (5x amplified)')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâš ï¸  The same perturbation works on different images!\")\n",
        "\n",
        "# 10.2 Backdoor Detection\n",
        "print(\"\\n2ï¸âƒ£ Backdoor Detection\")\n",
        "\n",
        "def detect_backdoor_neurons(model, clean_data, trigger_data):\n",
        "    \"\"\"\n",
        "    Detect neurons that might be associated with backdoors\n",
        "    by analyzing their activation patterns.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get activations for clean data\n",
        "    clean_activations = []\n",
        "    trigger_activations = []\n",
        "\n",
        "    def get_activation(name, activation_dict):\n",
        "        def hook(model, input, output):\n",
        "            activation_dict[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    # Register hooks\n",
        "    activation_dict = {}\n",
        "    hooks = []\n",
        "    for name, layer in model.named_modules():\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            hook = layer.register_forward_hook(get_activation(name, activation_dict))\n",
        "            hooks.append(hook)\n",
        "\n",
        "    # Get clean activations\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean_data)\n",
        "        clean_acts = {k: v.clone() for k, v in activation_dict.items()}\n",
        "\n",
        "        # Get trigger activations\n",
        "        _ = model(trigger_data)\n",
        "        trigger_acts = {k: v.clone() for k, v in activation_dict.items()}\n",
        "\n",
        "    # Remove hooks\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    # Analyze differences\n",
        "    suspicious_neurons = {}\n",
        "    for layer_name in clean_acts:\n",
        "        clean_mean = clean_acts[layer_name].mean(dim=0)\n",
        "        trigger_mean = trigger_acts[layer_name].mean(dim=0)\n",
        "\n",
        "        # Find neurons with large activation differences\n",
        "        diff = torch.abs(trigger_mean - clean_mean)\n",
        "        threshold = diff.mean() + 2 * diff.std()\n",
        "        suspicious_indices = torch.where(diff > threshold)[0]\n",
        "\n",
        "        if len(suspicious_indices) > 0:\n",
        "            suspicious_neurons[layer_name] = suspicious_indices.tolist()\n",
        "\n",
        "    return suspicious_neurons\n",
        "\n",
        "# Create synthetic data with and without triggers\n",
        "clean_samples = torch.randn(50, 1, 28, 28, device=device)\n",
        "trigger_samples = clean_samples.clone()\n",
        "trigger_samples[:, :, -4:, -4:] = 1.0  # Add trigger pattern\n",
        "\n",
        "# Detect suspicious neurons\n",
        "suspicious = detect_backdoor_neurons(backdoor_model, clean_samples, trigger_samples)\n",
        "\n",
        "print(\"Suspicious neurons detected:\")\n",
        "for layer, neurons in suspicious.items():\n",
        "    print(f\"  {layer}: neurons {neurons}\")\n",
        "\n",
        "if not suspicious:\n",
        "    print(\"  No highly suspicious neurons detected.\")\n",
        "\n",
        "print(\"\\nðŸ’¡ This is a simplified detection method. Real backdoor detection is more complex!\")\n",
        "\n",
        "# 10.3 Model Watermarking\n",
        "print(\"\\n3ï¸âƒ£ Model Watermarking\")\n",
        "\n",
        "class ModelWatermarking:\n",
        "    \"\"\"\n",
        "    Embed a watermark in a neural network model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_watermark_samples=10):\n",
        "        self.num_samples = num_watermark_samples\n",
        "\n",
        "    def generate_watermark_data(self, input_shape, num_classes):\n",
        "        \"\"\"\n",
        "        Generate random watermark patterns and labels.\n",
        "        \"\"\"\n",
        "        # Create unique patterns\n",
        "        self.watermark_inputs = torch.randn(self.num_samples, *input_shape).to(device)\n",
        "        # Assign specific labels (could be a secret pattern)\n",
        "        self.watermark_labels = torch.randint(0, num_classes, (self.num_samples,)).to(device)\n",
        "\n",
        "        return self.watermark_inputs, self.watermark_labels\n",
        "\n",
        "    def embed_watermark(self, model, train_data, train_labels, epochs=10):\n",
        "        \"\"\"\n",
        "        Fine-tune model to embed watermark.\n",
        "        \"\"\"\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Combine training data with watermark\n",
        "        combined_data = torch.cat([train_data, self.watermark_inputs])\n",
        "        combined_labels = torch.cat([train_labels, self.watermark_labels])\n",
        "\n",
        "        print(\"Embedding watermark...\")\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(combined_data)\n",
        "\n",
        "            # Weighted loss - higher weight on watermark samples\n",
        "            regular_loss = F.cross_entropy(outputs[:len(train_data)], combined_labels[:len(train_data)])\n",
        "            watermark_loss = F.cross_entropy(outputs[len(train_data):], combined_labels[len(train_data):])\n",
        "\n",
        "            total_loss = regular_loss + 5.0 * watermark_loss  # Higher weight on watermark\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Watermark embedded!\")\n",
        "\n",
        "    def verify_watermark(self, model):\n",
        "        \"\"\"\n",
        "        Check if the model contains our watermark.\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(self.watermark_inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            accuracy = (predicted == self.watermark_labels).float().mean().item()\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "# Create a fresh model for watermarking\n",
        "watermarked_model = MNISTClassifier().to(device)\n",
        "\n",
        "# Train it normally first (simplified)\n",
        "optimizer = optim.Adam(watermarked_model.parameters(), lr=0.001)\n",
        "watermarked_model.train()\n",
        "for i, (images, labels) in enumerate(trainloader):\n",
        "    if i > 20:\n",
        "        break\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = watermarked_model(images)\n",
        "    loss = F.cross_entropy(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Create watermark\n",
        "watermarker = ModelWatermarking(num_watermark_samples=5)\n",
        "watermark_data, watermark_labels = watermarker.generate_watermark_data((1, 28, 28), 10)\n",
        "\n",
        "# Get some training data for watermark embedding\n",
        "train_images, train_labels = next(iter(trainloader))\n",
        "train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
        "\n",
        "# Embed watermark\n",
        "watermarker.embed_watermark(watermarked_model, train_images, train_labels)\n",
        "\n",
        "# Verify watermark\n",
        "watermark_accuracy = watermarker.verify_watermark(watermarked_model)\n",
        "print(f\"\\nWatermark verification accuracy: {watermark_accuracy:.2%}\")\n",
        "\n",
        "# Test on a non-watermarked model\n",
        "non_watermark_acc = watermarker.verify_watermark(mnist_model)\n",
        "print(f\"Non-watermarked model on watermark data: {non_watermark_acc:.2%}\")\n",
        "print(\"(Should be close to random chance ~10% for 10-class classification)\")\n",
        "print(\"\\nâœ… Model watermarking helps prove ownership!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"All demonstrations complete!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "advanced-topics-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "### ðŸŽ¯ Key Takeaways\n",
        "\n",
        "1. **Neural networks are vulnerable** to various attacks:\n",
        "   - Adversarial examples (FGSM, PGD, C&W)\n",
        "   - Data poisoning and backdoor attacks\n",
        "   - Model extraction\n",
        "   - Privacy leakage (membership inference)\n",
        "\n",
        "2. **Defense mechanisms exist** but come with trade-offs:\n",
        "   - Adversarial training improves robustness but may reduce clean accuracy\n",
        "   - Differential privacy protects data but adds noise to training\n",
        "   - Input preprocessing can filter some attacks but isn't foolproof\n",
        "   - Defensive distillation makes gradient-based attacks harder\n",
        "\n",
        "3. **Security is an ongoing challenge**:\n",
        "   - New attacks are constantly being discovered\n",
        "   - Defenses must evolve to keep pace\n",
        "   - No single defense is perfect - defense in depth is key\n",
        "\n",
        "### ðŸ›¡ï¸ Best Practices for Secure ML\n",
        "\n",
        "#### During Development\n",
        "- **Validate your data**: Check for poisoned or mislabeled samples\n",
        "- **Use regularization**: Helps prevent overfitting and memorization\n",
        "- **Monitor training**: Watch for unusual patterns or behaviors\n",
        "- **Test robustness**: Evaluate models against various attacks\n",
        "\n",
        "#### During Deployment\n",
        "- **Input validation**: Sanitize and validate all inputs\n",
        "- **Rate limiting**: Prevent model extraction through API limits\n",
        "- **Monitoring**: Track unusual prediction patterns\n",
        "- **Regular updates**: Retrain models with new data and defenses\n",
        "- **Ensemble methods**: Use multiple models for critical decisions\n",
        "\n",
        "#### Privacy Considerations\n",
        "- **Minimize data collection**: Only collect what you need\n",
        "- **Use privacy-preserving techniques**: Differential privacy, federated learning\n",
        "- **Secure storage**: Encrypt models and data at rest\n",
        "- **Access controls**: Limit who can query your models\n",
        "- **Audit trails**: Log all model access and usage\n",
        "\n",
        "### ðŸš€ Future Directions\n",
        "\n",
        "The field of ML security is rapidly evolving. Some exciting areas include:\n",
        "\n",
        "- **Certified defenses**: Provable robustness guarantees\n",
        "- **Federated learning security**: Protecting distributed training\n",
        "- **Explainable AI for security**: Understanding why models fail\n",
        "- **Hardware-based defenses**: Secure enclaves for ML\n",
        "- **Quantum-resistant ML**: Preparing for quantum attacks\n",
        "\n",
        "Remember: **Security isn't a feature you add at the end - it should be considered throughout the entire ML pipeline!**"
      ],
      "metadata": {
        "id": "summary"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resources and References\n",
        "\n",
        "### ðŸ“š Further Reading\n",
        "\n",
        "1. **Papers**:\n",
        "   - \"Explaining and Harnessing Adversarial Examples\" (Goodfellow et al., 2014)\n",
        "   - \"Towards Deep Learning Models Resistant to Adversarial Attacks\" (Madry et al., 2017)\n",
        "   - \"Deep Learning with Differential Privacy\" (Abadi et al., 2016)\n",
        "   - \"Model Extraction Attacks and Defenses\" (Jagielski et al., 2020)\n",
        "\n",
        "2. **Tools and Libraries**:\n",
        "   - [Adversarial Robustness Toolbox (ART)](https://github.com/Trusted-AI/adversarial-robustness-toolbox)\n",
        "   - [CleverHans](https://github.com/cleverhans-lab/cleverhans)\n",
        "   - [Foolbox](https://github.com/bethgelab/foolbox)\n",
        "   - [TensorFlow Privacy](https://github.com/tensorflow/privacy)\n",
        "\n",
        "3. **Courses and Tutorials**:\n",
        "   - [MIT 6.S965: TinyML and Efficient Deep Learning](https://efficientml.ai/)\n",
        "   - [Stanford CS231n: Deep Learning for Computer Vision](http://cs231n.stanford.edu/)\n",
        "   - [Fast.ai Practical Deep Learning](https://course.fast.ai/)\n",
        "\n",
        "### ðŸ Conclusion\n",
        "\n",
        "Congratulations on completing this comprehensive lab on Neural Networks and AI Security! You've learned:\n",
        "\n",
        "âœ… How neural networks work from the ground up  \n",
        "âœ… Various attack methods and their implications  \n",
        "âœ… Defense mechanisms and their trade-offs  \n",
        "âœ… Privacy-preserving techniques  \n",
        "âœ… Advanced security topics and future directions  \n",
        "\n",
        "Remember: **Security in machine learning is not a destination, but a journey.** Stay curious, keep learning, and always consider the security implications of your models!"
      ],
      "metadata": {
        "id": "resources"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GHhifCmVbImH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}